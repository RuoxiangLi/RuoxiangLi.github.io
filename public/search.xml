<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F05%2F30%2Ftest%2F</url>
    <content type="text"><![CDATA[-just for test]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>ubuntu16.04</tag>
        <tag>Hexo</tag>
        <tag>Github</tag>
        <tag>Typora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM基础学习之非线性最小二乘问题求解方法]]></title>
    <url>%2F2019%2F05%2F27%2F%E8%A7%86%E8%A7%89SLAM%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[《Methods for Non-linear Least Squares Problems》学习记录。 Introduction and Definitions最小二乘问题的目标是找到局部极小解，是全局最优化问题（求目标函数/代价函数的全局最小值）的一个特例。由于全局最优化问题很难求解，习惯上将其简化为查找一个局部的最小解，即找到一个自变量向量，使得目标函数在确定区域内具有极小值。 定义 1.1. 最小二成问题$f(\mathbf{x})=(f_1(\mathbf{x}),f_2(\mathbf{x}),…,f_m(\mathbf{x}))$ $f_i(\mathbf{x})=f_i(x_1,x_2,…,x_n)=y_i,\quad\mathcal{R}^n\mapsto\mathcal{R},\mathbf{x}\in\mathcal{R}^n,y_i\in\mathcal{R}$ $f(\mathbf{x})$是向量值函数，其自变量处于$n$维空间，值域属于$m$维向量空间，即$f(\mathbf{x})$的值是向量。 $f_i(\mathbf{x})$是分量函数，其自变量是$n$维向量，值域属于实数集。 2.1 梯度下降法梯度下降法是通过梯度方向和步长，直接求解目标函数的最小值时的参数。 越接近最优值时，步长应该不断减小，否则会在最优值附近来回震荡。 2.2 牛顿法牛顿法是求解函数值为0时的自变量取值的方法。 利用牛顿法求解目标函数的最小值其实是转化为求使目标函数的一阶导数为0的参数值。这一转换的理论依据是，函数的极值点处的一阶导数为0。 其迭代过程是在当前位置$x_0​$求该函数的切线，该切线和x轴的角点为$x_1​$，作为新的$x_0​$，重复这个过程，直到角点和函数的零点重合。此时的参数值就是使得目标函数取得极值的参数值。其迭代过程如下图所示。 迭代公式：$x_{k+1}:=x_k+h_n=x_k-H^{-1}\cdot F’(x)=x_k-\frac{F’(x)}{F’’(x)}$ $H$为海森矩阵，其实就是目标函数对参数$x​$的二阶导数。 总结牛顿法是通过求解目标函数的一阶导数为0时的参数，进而求出目标函数最小值时的参数。 收敛速度快。 海森矩阵的逆在迭代过程中不断减少，可以起到逐步减少步长的效果。 缺点：海森矩阵的逆计算复杂，代价比较大，因此有了拟牛顿法。 2.3 线性搜索该方法确定下降搜索的步长（step length）。 文中的图2.1是代价函数随步长变化而变化的情况，横坐标是步长，其值大于0。 参考资料 推荐中文解析：https://blog.csdn.net/zhangjunhit/article/details/88883459 梯度下降法和牛顿法比较：https://www.cnblogs.com/lyr2015/p/9010532.html 文献原文： http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3215/pdf/imm3215.pdf]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>基础学习</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>最小二乘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读之《SVO Fast Semi-Direct Monocular Visual Odometry》]]></title>
    <url>%2F2019%2F05%2F25%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8ASVO-Fast-Semi-Direct-Monocular-Visual-Odometry%E3%80%8B%2F</url>
    <content type="text"><![CDATA[本文介绍半直接法单目视觉里程计：Semi-direct Visual Odometry，即SVO。 半稠密单目视觉里程计算法SVO (Semi-direct Visual Odometry)：直接使用像素强度值 概率建图方法、异常点剔除策略：Bayesian滤波器用于估计特征点的深度，只有相关的深度滤波器满足收敛条件，三维地图点才被插入地图，因此地图点需要多次测量 可部署至无人机设备，在此之前，应用于无人机的单目视觉里程计都是基于特征法的 结合特征法（追踪特征、并行追踪与建图、关键帧机制）和直接法（低纹理、相机失焦、运动模糊场景更鲁棒） 采用特征匹配关联方法，特征提取只发生在关键帧被用于初始化新的三维地图点时（避免在每帧图像进行特征提取，使得速度得以提升；亚像素特征关联提升了精确性） 基于模型的稀疏图像对齐算法（与之相关的是基于模型的稠密图像对齐算法），用于运动位姿估计 如上图所示，系统采用并行的追踪、建图线程。 运动估计线程实现了半直接法完成相机位姿估计，主要步骤： 基于粗略运动估计完成位姿初始化。初始化位姿使用当前帧和上一帧图像，首先将相同三维点投影至两帧图像，通过最小化投影点的光度误差估计相机初始位姿； 关联特征Patch对齐，细化投影点的像素坐标； 运动估计，即细化位姿和三维结构。该过程采用上述的最小化重投影误差方法。 建图线程基于深度滤波器更新三维点深度信息，主要步骤： 判断新到达的图像帧是否为关键帧；新的关键帧将会提取特征，非关键帧图像则用于更新深度滤波器； 在新的关键帧中提取特征后，为关键帧中的每个2D特征，初始化一个概率深度滤波器，用于估计与其对应的三维点。初始的深度滤波器具有很大的深度不确定性，随着新的图像帧的加入，逐步更新2D特征点对应的三维地图点的深度值，直到深度不确定性足够小，深度滤波收敛，插入新的地图点，地图点用于追踪线程的运动估计过程。 Motion EstimationSparse Model-based Image Alignment将当前关键帧与上一相邻图像帧（注意不是关键帧）对齐，即最小化光度误差Photometric Error（特征点局部Patch的光度误差，Patch的大小为4×4），优化两帧之间的相对位姿（初始化为上一帧的位姿或单位矩阵），即图像帧对齐。 $\mathbf{T}{k,k-1}=\mathop{\arg\min}\limits{\mathbf{T}{k,k-1}}\frac{1}{2}\sum\limits{i\in\mathcal{\overline{R}}}{\parallel\delta \mathbf{I}(\mathbf{T}_{k,k-1},\mathbf{u}_i)\parallel^2}$ 其中，$\delta\mathbf{I}(\mathbf{T},\mathbf{u})=Ik(\pi(\mathbf{T}\cdot\pi^{-1}(\mathbf{u},d{\mathbf{u}}))-I_{k-1}(\mathbf{u}),\quad\forall\mathbf{u}\in\mathcal{\overline{R}}$，表示上一帧中的特征点反投影至其相机坐标系下，再变换到当前帧相机坐标系下，重投影得到像素坐标值。 具体过程： 准备工作。通过之前多帧之间的特征检测和深度估计，已确定第$I{k-1}$帧中特征点位置和深度值，即图中$u_1,u_2,u_3$的坐标即深度值；假设相邻帧$I{k-1}$和$I{k}$之间位姿$T{k,k-1}$已知，一般初始化为上一相邻时刻的位姿或者假设为单位矩阵； 重投影。将第$I{k-1}$帧中特征点$u_i$投影到三维空间，得到该帧相机坐标系下的三维点$p^{k-1}_i$；再使用$T{k,k-1}$将$p^{k-1}_i$变换至当前帧相机坐标系下，得到$p^{k}_i$；再使用相机内参重投影至当前帧图像，得到特征点的像素坐标$u’_i$； 迭代优化更新位姿。相邻两帧的变化比较小，相同特征点的亮度值变化不大，但由于相对位姿$T{k,k-1}$是假设值，投影点$u’_i$的位置不准确，导致投影前后的亮度值不相等，有一个差值。因此，通过迭代优化相对位姿，减小这个差值，得到优化后的位姿$T{k,k-1}$。 Relaxation Through Feature Alignment由于上一步估计的相对位姿是不准确的，导致重投影预测的特征点位置$u’_i$并不是真正的特征点位置。下图$I_k$中灰色特征块为假设的真实位置，目前是未知的；蓝色特征块为预测的特征点位置，利用它们之间的偏差构建残差目标函数，和上述直接法类似，即最小化光度误差（同样是针对特征点的局部Patch）。但这一步加入了放射变换（因为关键帧与关键帧之间的距离可能比较远，特征点局部Patch也扩大为8×8），而且优化变量不再是相机位姿，而是特征点预测位置$u’_i$，通过迭代对特征块的预测位置进行优化，即特征对齐。 $\mathbf{u’}i=\mathop{\arg\min}\limits{\mathbf{u’}i}\frac{1}{2}{\parallel \mathbf{I}_k(\mathbf{u’}_i)-\mathbf{A}{i}\cdot\mathbf{I}_r(\mathbf{u}_i)\parallel^2},\quad\forall i$ Pose and Structure Refinement上一步优化的特征点预测位置与第一步预测的特征点位置（即地图点通过第一步估计的位姿重投影的特征点位置）之间有偏差，利用该偏差构造新的优化目标函数，即最小化重投影特征点位置误差（不是像素值的差异），优化当前关键帧相机位姿以及地图点位置。 这一步其实是Bundler Adjustment，包括Motion-only Bundler Adjustment和Structure-only Bundler Adjustment，前者是优化当前帧位姿（如下优化目标函数），后者是优化当前帧关联的地图点位置（优化目标函数与下式相似，只不过优化变量为三维地图点$_{w}\mathbf{p}_i$位置）。 $\mathbf{T}{k,w}=\mathop{\arg\min}\limits{\mathbf{T}{k,w}}\frac{1}{2}\sum\limits{i}{\parallel\mathbf{u}i-\mathbf{\pi}(\mathbf{T}{k,w},_{w}\mathbf{p}_i)\parallel^2}$ Mapping对于已知位姿的当前帧，Mapping线程估计图像中2D特征（它们关联的三维地图点还未知）的深度值。深度值估计过程采用概率分布模型，每一组观测${Ik,\mathbf{T}{k,w}}$，都会被用于更新Bayesian框架分布。当分布的变化收敛到足够小时，估计的深度将用于生成三维地图点${k}\mathbf{p}=\pi^{-1}(\mathbf{u},d{\mathbf{u}})​$，新生成的地图点被加入到地图中，并立即用于运动估计。 每个关键帧与一个深度滤波器关联。 参考资料 https://github.com/Ewenwan/MVision/tree/master/vSLAM/svo_slam]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>SVO</tag>
        <tag>半直接法SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读之《DTAM Dense Tracking and Mapping in Real-Time》]]></title>
    <url>%2F2019%2F05%2F24%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8ADTAM-Dense-Tracking-and-Mapping-in-Real-Time%E3%80%8B%2F</url>
    <content type="text"><![CDATA[终于有时间读论文了，DTAM——视觉SLAM直接法鼻祖。 DTAM基于稠密的像素匹配方法，而不是特征提取方法，能够进行实时相机追踪和建图。 静态场景下移动RGB相机，系统估计详细的纹理深度地图。 基于关键帧的机制 基于非凸优化框架，最小化全局空间正则化能量函数 基于针对整个稠密模型的帧速率全图像对齐，精确追踪相机的6自由度位姿 使用GPU DTAM (Dense tracking andmapping) 的目标函数中包含了多种数据关联的误差。包括图像空间的匹配误差(2D-2D)和3D 空间的位置误差(3D-3D)。当帧间运动较小，成功匹配的3D点较多时，使用3D-3D匹配估计位姿矩阵；当帧间运动较大，匹配2D点较多时，使用2d-2d匹配估计基础矩阵。 DTAM的direct method在默认环境亮度不变（brightness consistancy assumption）的前提下， 对每一个像素的深度数据进行inverse depth的提取和不断优化来建立稠密地图并实现稳定的位置跟踪。 Mapping 构建稠密的3D表面模型 本模块的分析详见参考资料2。 构建代价体素（Cost Volume）。 Tracking 基于3D模型进行稠密的相机位姿追踪，即全局的图像配准 参考资料 https://github.com/Ewenwan/MVision/tree/master/vSLAM/DTAM 墙裂推荐：https://zhuanlan.zhihu.com/p/42137963 slides：https://wenku.baidu.com/view/3774d70c326c1eb91a37f111f18583d049640f04.html 关于帧间相机旋转估计：http://campar.in.tum.de/twiki/pub/ISMAR08IAR/WebHome/pres.pdf]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>PTAM</tag>
        <tag>直接法SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读之主流VSLAM算法初始化总结]]></title>
    <url>%2F2019%2F05%2F23%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E4%B8%BB%E6%B5%81VSLAM%E7%AE%97%E6%B3%95%E5%88%9D%E5%A7%8B%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文记录主流VSLAM系统初始化过程整理。 MonoSLAM：初始化过程需要将相机放置在一个距离已知的平面场景下。 PTAM：初始化过程，假设一个平面场景，计算单应矩阵，分解出旋转平移矩阵，作为相机初始位姿。 DTAM： SVO：系统启动，获取两帧关键帧的位姿；使用最开始的两帧关键帧，采用三角化方法初始化地图；与PTAM相同，初始化时假设一个平面场景，通过估计单应矩阵估算初始位姿。]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读之《Parallel Tracking and Mapping for Small AR Workspaces》]]></title>
    <url>%2F2019%2F05%2F20%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8AParallel%20Tracking%20and%20Mapping%20for%20Small%20AR%20Workspaces%E3%80%8B%2F</url>
    <content type="text"><![CDATA[本篇文章记录自己在阅读PTAM论文时整理的一些要点。 PTAM，是最早提出将Track和Map并行处理的一种SLAM算法，是一种基于关键帧和非线性优化的单目视觉SLAM算法。PTAM的贡献是具有重大意义的： 首次实现跟踪与建图的并行化； 首次使用非线性优化，替代传统滤波器后端方案； 引入关键帧机制。 缺点：适用场景小，跟踪容易丢失。 Tracker线程 鲁棒估计相机运动。 主要任务 估计相机位姿，使用Map完成追踪（初始化时使用2D-2D方法估算初始位姿，后续过程使用3D-2D方法追踪） 与Map线程通信，选取添加关键帧到缓存队列，进而发送到Map 构建关键帧，关键帧的选择 地图初始化 重定位 Tracking线程也用到关键帧，每一个新的图像帧都会在追踪过程进行之前构建一个关键帧。但，大部分关键帧都被丢弃，未被丢弃的关键帧加入到Map，这些是真正意义上的关键帧。 重要函数 MakeKeyFrame_Lite：使用当前帧构建初始关键帧 TrackForInitialMap：地图初始化。由于刚开始没有地图，需要在双目图像帧（使用的是第一帧图像以及有足够平移的另一帧图像，保证双目模型的基线满足）中进行简单的Patch搜索。Patch搜索是在TrailTracking_Advance进行的。具体过程如下： 第一帧图像构建初始关键帧（该过程检测FAST角点），调用TrailTracking_Start函数，完善关键帧结构，包括对金字塔层级所有图像中的FAST角点非极大值抑制，并使用Shi-Tomasi方法计算角点得分，选取高得分角点（作为候选关键点）； 根据Shi-Tomasi得分，对上述候选关键点排序，将这些候选关键点全部加入待追踪/匹配集合，并提取它们的局部Patch； 正向搜索。第二帧（合适的、距离第一帧有一定距离）图像构建初始关键帧，调用TrailTracking_Advance函数，对于上一帧关键帧中的每一个$Corner_i$的$Patch_i$，在当前关键帧的FAST角点中基于SSD匹配（是块匹配），搜索最佳的角点$Corner_j$，提取$Corner_j$的局部块$Patch_j$； 反向搜索（Cross Check Test？）。在上一帧关键帧中的FAST角点中搜索与$Pathc_j$匹配的最佳的角点$Corner_k$； 判断$i-k&lt;=2$，则$Corner_j$是$Corner_i$的匹配关键点；如果没找到$Corner_i$的匹配点，则将$Corner_i$删除，是从待追踪/匹配集合中删除，而不是从关键帧候选关键点集合中删除（后面三角化完成之后还会根据候选关键点集合，采用极线搜索添加三维地图点，所以这里不会删除候选关键点）； 如果追踪/匹配到的角点对数量&gt;=10，则用这些匹配到的点对，调用InitFromStereo函数，求取两个关键帧的初始位姿和初始地图： 使用单应矩阵求位姿，作为相机初始位姿； 设置尺度，缩小为实际距离的0.1倍； 使用匹配点对三角化求三维地图点（三角化参考博客），Triangulate函数得到第一视图坐标系下的坐标； 全局BA优化三维地图点和相机位姿； 极线搜索添加三维地图点。将地图点对应的二维坐标周围10*10区域内的候选关键点删除，使用剩下的候选关键点进行极线搜索，并三角化添加三维地图点，极线搜索是在最新的关键帧与其最近的关键帧之间进行的； 统一世界坐标系。 至此，TrackForInitialMap函数完成地图初始化。 PredictPoseWithMotionModel：减速运动模型（匀速运动模型使用比较多）预测相机位姿 计算上一帧图像高斯模糊小图的雅可比矩阵 采用ESM（Efficient Second-order Minimization）跟踪算法跟踪当前帧，计算当前帧图像相对于上一帧图像的旋转矩阵 TrackMap：使用估算的位姿，将所有地图点投影到当前帧图像，不在当前帧图像中的地图点丢弃 根据预测的相机位姿，将当前所有世界点根据小孔成像原理进行投影，投影后的像素点记为pi，并计算出对应的金字塔层级 根据金字塔高层优先原则，选取一定数量世界点（通常，粗搜索选取30～60个，细搜索选取1000个左右） Mapping 从之前观测到的视频帧中产生三维地图点特征。 主要任务： 从缓存队列中提取关键帧到地图 Global BA Local BA：只调整一部分关键帧的位姿，以及这些关键帧对应的所有地图点的位置。PTAM中调整地图中最新的5个关键帧的位姿。 极线搜索添加新的地图点到地图 数据关联的细化 优先级：关键帧插入&gt;BA&gt;数据关联细化操作。 地图 Map 3D Point Features Keyframes 地图点 每个地图点MapPoint保存第一次观测到该地图点的关键帧索引，保存该地图点对应的特征点所在金字塔层级，以及在该层级图像中的像素坐标 每个地图点对应一个观测结构Measurement，该结构中保存地图点在关键帧图像中的金字塔层级、像素坐标以及观测来源，即该地图点是如何观测到的 关键帧 Keyframes每个关键帧包括四层金字塔图像，下采样（四个像素的平均）得到的低分辨率的上层图像，每一层为一个数据结构Level，金字塔每层图像（每层Level）保存该层图像上的所有的FAST角点，它们经过Shi-Tomasi得分筛选之后，成为关键点候选Candidate，即可能会对应一个三维地图点，这些候选能够生成三维地图点的方式： Patch匹配三角化得到三维地图点 极线搜索三角化得到三维地图点 …. 构造金字塔的目的：加快匹配；提高地图点相对于相机远近变化的鲁棒性。 每个关键帧中保存所有与其关联的（该关键帧观测范围内的）地图点 每个关键帧中的金字塔图像中保存候选的关键点，会在后续过程中三角化生成地图点 关键帧有关的操作 关键帧构建（MakeKeyFrame_Lite函数）过程/初始化，在关键帧四层金字塔，即四层图像中，进行FAST角点的检测，没有非极大值抑制（其实是在Tracking线程中进行）； 关键帧初始化过程使用Tracking线程估计的相机位姿和特征点的观测数据； Tracking线程只测量图像帧中潜在视觉特征的一部分，Mapping线程将这些特征反投影、测量其他的特征。即与关键帧关联的地图点可能是Tracking线程得到的，可能是Mapping线程得到的； Mapping线程（MakeKeyFrame_Rest函数）对FAST角点进行极大值抑制，使用Shi-Tomasi分数，确定生成地图点的候选特征点；候选特征点对应的地图点如果距离已有的观测地图点比较近，这些候选特征点将会被剔除。 关键帧加入地图的条件 关键帧追踪的质量达到标准； 距离上一次加入的关键帧已经超过20个图像帧； 相机必须与地图中已有的最近关键点保持最小距离，需要保证三角化的基线不能过大、过小。最小距离取决于观测到的特征的平均深度值，即观测到的特征距离相机越远，关键帧之间的距离就越大。 Map初始化？？？？地图初始化过程需要前两帧图像，前两帧图像都设定为关键帧。随着相机的运动，新的关键帧和地图点被加入到地图中。 首先在第一帧关键帧中检测1000个FAST角点，构成2D图像块； 在第二帧图像中追踪上述图像块； 采用五点法对极约束，求取基础矩阵，提取两帧图像之间的旋转和平移； 同时使用RANSAC方法剔除异常点； 使用匹配到的点对，三角化生成三维地图点，建立初始地图。 特征点三角化基于关键帧图像，三角化获取特征点的三维坐标信息，该过程一个关键帧是无法完成的，需要两个关键帧。PTAM选择地图中已有的最近的两个关键帧进行三角化操作。使用对极搜索获取两个关键帧中的关联特征，关联过程为： 对于第一个关键帧图像中的某个特征，提取其像素坐标周围的局部像素块P； 在第二帧关键帧图像中的极线上搜索与P最接近的像素块，确定关联特征的位置； 两个像素块的比较使用zero-mean SSD，并且只在相同的金字塔层级内进行； 一旦匹配到一对特征点，则采用三角化方法求取新的地图点，并将新的地图点插入地图。 Bundle Adjustment全局的BA能够调整所有关键帧的位姿以及所有地图点的位置。 全局BA利用了SFM问题固有的稀疏性，将三次代价矩阵分解的复杂性从$O((N+M)^3)$降低为$O(N^3)$。 有新的关键帧插入Maping线程时，会中断BA操作，保证新的关键帧可以及时插入地图中。 局部BA的复杂性也是和地图的大小有关的。 数据关联细化BA收敛并且不再需要新的关键帧时，即相机已经处于一个建图比较理想的环境中，此时Mapping处于空闲状态，可以进行地图的优化，即数据关联的细化。 数据关联的细化主要是通过在旧的关键帧中进行新的测量来完成的，可以是测量新加入的地图点在旧的关键帧中的位置，例如，通过对极搜索新加入的特征点，对其进行的初始测量只和两个关键帧有关联，即进行对极约束的两个关键帧。但是其他的关键帧也可能会观测到该特征点；也可以是再次进行异常点测量。 该过程优先级低于BA。 参考资料 墙裂推荐：https://github.com/Ewenwan/MVision/tree/master/vSLAM/PTAM https://blog.csdn.net/ilotuo/article/category/6297333 https://blog.csdn.net/u013925378/article/details/77455555 https://blog.csdn.net/u011178262/article/details/79315782 https://blog.csdn.net/u011178262/article/details/86729887]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>单目SLAM</tag>
        <tag>SLAM</tag>
        <tag>PTAM</tag>
        <tag>特征法SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VSCode调试C++代码]]></title>
    <url>%2F2019%2F04%2F29%2FVSCode%E8%B0%83%E8%AF%95C-%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[简单记录下使用VSCode调试代码收集的一些内容。 对于一般的C++程序，可以参考这里或这里配置相关文件，并调试代码。 其中比较重要的一步是在launch.json文件中添加可执行文件的路径； 如果不想用外部控制台进行调试，只在vscode内部显示相关信息，设置参数“externalConsole”: false即可。 GDB Quick Guide 对于采用CMake编译的情况，除了步骤1的配置外，还需注意： CMakeList.txt文件set命令中需要添加-g，表示允许调试，否则即使设置了断点，也不会在断点处暂停。例如：set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++11 -O3 -march=native -g&quot; )。参考自链接1、链接2。 对于ROS程序，除了上述步骤1、2的配置外，还需注意： 项目launch.json文件中添加的可执行文件路径为：/home/username/ros_work_space/devel/lib/ros_package_name/file_name。 关于ROS下如何调试程序，可参见这里。 ROS程序，如果使用roslaunch命令启动节点 需要在launch文件中节点定义一行添加：launch-prefix=&quot;xterm -e gdb --args&quot;，例如：&lt;node name=&quot;x&quot; pkg=&quot;xx&quot; type=&quot;xxx&quot; output=&quot;screen&quot; launch-prefix=&quot;xterm -e gdb --args&quot; &gt;。 Roslaunch Nodes in valgrind or GDBTutorials(2f)Roslaunch(20)Nodes(20)in(20)Valgrind(20)or(20)GDB.html)]]></content>
      <categories>
        <category>工具</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>ubuntu</tag>
        <tag>VSCode</tag>
        <tag>Debug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读之《Past, Present, and Future of SLAM》]]></title>
    <url>%2F2018%2F10%2F17%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8APast%2C%20Present%2C%20and%20Future%20of%20SLAM%E3%80%8B%2F</url>
    <content type="text"><![CDATA[本篇文章记录SLAM综述性文章《Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age》，只对重点部分作详细翻译，其余为本人根据自己的理解对内容概括性的总结。 概述一、介绍SLAM现行制定的标准 二、总结相关工作（包括一系列关于长时间建图中的鲁棒性和可扩展性、用于建图的度量和语义表示、理论上的性能保证、主动SLAM和探索、以及其他的前沿技术） 三、介绍开放的挑战和新的研究问题 四、作者对两个问题的看法，即机器人需要SLAM吗？SLAM问题解决了嘛？ IntroducitonSLAM包括同时的机器人状态估计和环境模型（地图）重建，机器人配置片上传感器，重建的环境是由机器人传感器感知到的。简单的情况下，机器人的状态使用其位姿（位置和方向/姿态）表示，当然机器人的状态还会包含其他的量，如机器人速度、传感器偏差、校准参数。地图是用于描述机器人所处环境的我们感兴趣部分（如路标点位置、障碍物）的表示。 需要构建环境地图的原因包括两方面。首先，地图经常被用于支撑其他任务；例如，一个地图可以通知路径规划或者提供可供用户操作的直观的可视化。其次地图能够限制机器人状态估计中积累的误差。没有地图时轨迹估计很快就会发生漂移；如果由地图，机器人就可以通过再次访问已访问的区域对位置错误进行重置，即所谓的回环检测。SLAM适用于所有没有先验经验需要创建地图的场景应用。 一些机器人的应用中环境地图是作为已知的先验经验。 SLAM问题的普及源于移动机器人的室内应用的出现。室内的操作就要排除GPS的使用，GPS是可以用于限制位置偏差的。SLAM为用户构建的地图提供了一个吸引人的替代方案，显示出在没有专门的本地化基础架构的情况下，机器人操作是可能的。 古典时代（1986-2004）（the classical age）：对SLAM的主要概率公式进行了介绍，包括基于扩展卡尔曼滤波器的方法、RaoBlackwellised粒子滤波器和最大似然估计；而且还描绘了与效率和强大数据关联相关的基本挑战。[14、94、299、298] 算法分析时代（2004-2015）（the algorithmic-analysis age）：研究了SLAM的基本性质，包括可观测性、收敛性和一致性。该时期对稀疏性对于高效SLAM求解器的关键作用有了理解，并发展出主要的开源SLAM库。[89] 底层SLAM（前端）涵盖了其他的研究领域，如机器视觉和信号处理； 在高层的后端，SLAM是几何学、图论、优化和概率的组合。而且，对于一个SLAM专家必须处理实际的从传感器建模到系统集成等各个方面的问题。 Do autonomous robots really need SLAM? 本文对SLAM的现状进行了广泛的概述，并提供了部分社区对SLAM研究的开放性问题和未来发展方向的看法。其他关于SLAM的综述性文章： Year Topic Reference 2006 Probabilistic approaches and data association Durrant-Whyte and Bailey [14, 94] 2008 Filtering approaches Aulinas et al. [12] 2008 Visual SLAM Neira et al. (special issue) [220] 2011 SLAM back-end Grisetti et al. [129] 2011 Observability, consistency and convergence Dissanayake et al. [89] 2012 Visual odometry Scaramuzza and Fraundofer [115, 274] 2016 Multi robot SLAM Saeedi et al. [271] 2016 Visual place recognition Lowry et al. [198] Do autonomous robots really need SLAM?回答这个问题需要理解SLAM特殊在哪。SLAM旨在利用自身运动的测量和回环检测建立场景的全局一致的表示。关键点在于回环检测：如果忽略了回环检测，SLAM退化为里程计。在早期的应用中，里程计是通过整合在车轮上的编码器获取到的，通过车轮里程计获取到的位姿检测很快就会发生漂移，使得估计在运动数米后就不可用。这也是推动SLAM发展的主要原因之一：外界场景路标点的观测对于消除轨迹漂移是有帮助的，能够尽可能的矫正轨迹。然而，目前更多的里程计算法都是基于视觉和惯性信息，具有很少的漂移（&lt;轨迹长度的0.5%）。这种情况下是否需要SLAM呢？分三方面回答。 过去十年中完成的SLAM研究工作是产生目前代表现有技术状态的视觉-惯性里程计算法；在这个意义上，视觉惯性导航（VIN）就是SLAM：VIN可以看作是忽略掉回环检测或者位置识别模块的简化SLAM系统。更一般意义上，SLAM导致在比其他知识领域（例如，航天工程中的惯性导航）更早地考虑到的更具挑战性配置（即，没有GPS，低质量传感器）下研究传感器融合。 关于回环检测。忽略回环检测执行里程计的情况下，机器人会把世界解释成一个没有尽头的走廊，机器人在里面一直无限地对新的区域进行探测。回环检测事件能够告诉机器人该走廊是保持相交的。这样回环检测的好处就清楚了：通过回环检测，机器人能够理解环境真实的拓扑结构，并且能够找到位置间的快捷/最短路径。因此，如果得到正确的环境拓扑结构是SLAM的优点之一，为什么不简单地忽略度量信息，只是做位置识别/回环检测呢？答案是：度量信息使得位置识别更简单、更健壮；度量重建使得机器人有闭环检测的机会，并允许丢弃虚假闭环。因此，SLAM在原则上是多余的（一个oracle位置识别模块就足以进行拓扑映射），SLAM为错误的数据关联和感知别名提供了一个自然的防御，与环境中的不同位置相对应的相似的场景会欺骗位置识别模块。SLAM地图提供了一种预测并验证未来的测量的机制：该机制对于健壮性测量是关键的。 由于许多应用隐式或显式地需要全局一致的地图，因此需要使用SLAM。例如在很多军事和民用应用中，机器人的目标是探测环境，并提供一个可供用户操作的地图。另一个例子是，机器人必须执行结构检测（例如建筑、桥梁等）；还有需要进行全局一致的3D重建时。 因此，机器人研究者需要设计一个SLAM系统时，他将面对多种设计选择。例如，一个拓扑地图能够用于分析给定地点的可达性，但是并不适合于路径规划和底层的控制；一个局部一致的度量地图非常适合用于避障以及局部与环境的交互作用，但牺牲了精确性；一个全局一致的度量地图允许机器人执行全局的路径规划，但是计算和维护地图又是高计算复杂度性的。一个更通用的选择更合适的SLAM系统的方法是将SLAM看作是一种机制，完成足够的统计计算来总结机器人的所有已观测到的数据，在这种情况下，在这个压缩表示中保留哪些信息是非常依赖于任务本身的。 Is SLAM solved?这是机器人社区经常被问到的问题。回答该问题的难度在于问题本身：SLAM是一个非常开放的话题，这个问题只适用于给定的机器人/环境/性能组合。尤其是，一旦下面几方面内容明确了，我们就可以评估SLAM问题的成熟度： 机器人(robot)：运动的类型（例如动态、最大速度）、可用传感器（例如分辨率、采样率）、可用的计算资源； 场景/环境（environment）：平面或三维空间、自然或人造路标点的存在、动态元素的数量、对称性的量和感知混叠的风险。请注意，这些中的许多方面实际上取决于传感器-环境对：例如，对于2D激光扫描仪，两个房间可能看起来相同（感知别名），而相机可能会根据外观线索辨别； 性能要求（performance requirements）：机器人状态估计中的精确性要求、环境表示的精确性和类型（例如基于路标点的或稠密的）、成功率（满足精度范围的测试的百分比）、估计延迟、最大可操作时间、建图区域的最大尺寸范围。 例如，在保证足够的精确性（&lt;10cm）以及足够的健壮性（即低错误率）的前提下，使用一个装有车轮编码器和激光扫描器的机器人构建2D室内场景的地图，可以认为是基本可以解决的（一个工业系统执行SLAM的例子是the Kuka Navigation Solution）。同样地，基于缓慢移动的机器人的视觉SLAM和视觉-惯性测量法都可以认为是成熟的研究领域。另一方面，其他的机器人/环境/性能组合仍需要大量的基础性研究。在机器人移动或者环境极具挑战性时（例如快速机器人动态、高度动态的环境），目前的SLAM算法很容易就会失败。同时，SLAM算法经常无法应对严格的性能要求，例如，快速闭环控制的高速率估计。本文将提供这些公开问题的综合概述，等等。 我们在这一节最后对SLAM的未来进行更广泛的考虑。我们认为，SLAM正在进入第三个时代，即鲁棒感知时代（the robust-perception age），其特点是具有以下关键要求： 性能强劲（robust performance）：SLAM系统能够在广泛的环境中以较低的故障率长时间运行；系统具备故障安全机制和自我调整功能，这样能够做出适应不同场景的系统参数的选择； 高层次的理解力（high-level understanding）：SLAM系统超越基础的几何重建，能够获取到对环境的一个更高层次的理解，例如语义、可用性、高级几何、物理； 资源意识（resource awareness）：SLAM系统可以针对可用的传感和计算资源量身定制，并可以提供根据可用资源调整计算负载的方法； 任务驱动推理（task-driven inference）：SLAM系统产生自适应地图表示，其复杂性可以根据机器人必须执行的任务而改变。 论文组织结构SectionII：SLAM标准的制定和体系结构 Section III：解决终身（长时间）SLAM中的鲁棒性 Section IV：处理可扩展性 Section V：讨论如何表示环境的几何形状 Section VI：将环境表征的问题扩展到语义信息的建模。 Section VII：提供了关于SLAM的理论方面的当前成就的概述 Section VIII：扩大了讨论范围，并回顾了使用决策来改善SLAM结果质量的活动SLAM问题 Section IX：概述了SLAM的最新趋势，包括非常规传感器的使用 Section X：提供最后的评论 尽管SLAM有其独特的特性，但它是与计算机视觉、计算几何、控制理论中有关问题相关的，这些领域的交叉使用是实现快速发展的必要条件。 III. LONG-TERM AUTONOMY I: ROBUSTNESS影响SLAM系统鲁棒性因素：算法层面、硬件层面、软件层面。 算法层面现有SLAM算法的限制，如无法处理动态环境、极端场景。 数据关联（data association）与感知混淆涉及的一个重要原因就是数据关联错误。如基于特征的vSLAM需要关联每一个视觉特征点到确定的路标点，感知混淆（Perceptual aliasing）（即对于不同的传感器输入引起相同的传感器信号）会使得问题解决变得困难，导致产生错误测量状态匹配（外点、错误匹配等），进一步影响后端优化；另外，数据关联如果错误地将测量数据判断为错误测量，就只能以估算的精度为代价，使用较少的测量值进行估算了。 short-term数据关联传感器采样速率相对于机器人运动变化足够大，直观上可以认为传感器视角不会发生剧烈的变化，因此可以完成诸如特征匹配或光流法，从而避免错误的数据关联。 long-term数据关联更具挑战性，涉及到闭合回环检测及验证。蛮力的方法处理回环检测是不切实际的。基于视觉特征的Bag-of-words方法[282]通过引入分层的词汇树、量化特征空间，使得搜索更加高效。但是不能处理严重的照明变化。因此提出了一些新的方式，如匹配序列[213]，将不同的视觉外观聚集成统一的表示[69]，或使用空间和外观信息[140]，关于此类的综述见[198]。闭合回环验证使用几何验证的方法确定闭合回环的质量。基于视觉的应用中，RANSAC[274]用于几何验证和异常值剔除。基于激光的方法可以通过检测当前scan与已有的地图的匹配度验证闭合回环，例如残差。当然，错误的闭合回环也是无法避免的。 动态场景挑战一：SLAM系统必须要检测、剔除、追踪环境的变化，主流的方法就是剔除场景的动态变化部分，一些方法将动态元素一同建模。挑战二：SLAM系统必须对永久性或半永久性变化进行建模，并理解如何、何时更新地图。 硬件层面传感器本身、执行器退化引起。如何检测降级的传感器操作？ 如何调整传感器噪声统计数据（协方差，偏见）？ 软件层面集成和测试是SLAM和机器人技术的关键方面，它们同样会引起误差。 V. REPRESENTATION I: METRIC REASONING介绍SLAM中的模型几何，即度量表达方法。2D情况有基于路标的地图表达和占用网格地图表达。 基于路标的稀疏表达方式。可以是环境中可分辨特征相关联的3D路标点，例如线、角。前提是路标是可区分的，例如描述子。目前大部分的研究关注点特征的估计，也扩展到线、线段、圆弧[200]。 低层次的原始稠密表达。可以提供3D物体的高分辨率模型，适用于机器人研究中的避障和路径规划或者可视化、渲染。原始表达使用点云或多边形、面元地图（surfels maps）描述3D几何物体。这种方式比较笨重，因为需要存储大量的数据信息。 边界和空间划分密集表示。明确地表示surfaces (or boundaries) and volumes。空间划分表示方法最具代表性的是spatial-occupancy enumeration，该方法将三维空间划分为相同的立方体（体素）并排列在规则的3D网格中。其他方法还包括octree（应用于3D）、Polygonal Map octree和Binary Space-Partitioning tree等。 vSLAM中关于稀疏（基于特征的）表达方式与稠密表达方式、直接法的比较。 基于特征的方法取决于特征的类型。 稠密、直接表达方式更适用于低纹理、散焦和运动模糊的场景，但需要高计算能力和实时性能。 半稠密方法克服了稠密法高计算需求的缺点。 半直接法一定程度上同时使用了稀疏特征和直接法（被证明更高效，SVO[113]），允许结构和运动同时估计。 高层次的基于物体的表达。SLAM++。 VI. REPRESENTATION II: SEMANTIC REASONING 基于视觉的拓扑地图SLAM系统综述[198]。 SLAM help Semantics比较早的关于分割度量地图的方法[215]，offline的方法，使用2D激光扫描器构建几何地图。online语义地图构建系统[257、258]，将传感、分类、位置识别组合，使用激光和相机构建环境的语义地图。[41]使用了运动优化，将粗略的语义分割与不同的对象检测器互。[249]使用单目SLAM系统提高视频中对象识别任务的性能。 Semantics helps SLAM既然可以在SLAM构建的地图中识别物体，那也可以使用关于物体的几何先验知识去优化地图的估计。这方面的研究：[63、71]基于稀疏特征的单目SLAM系统，应用于小尺度环境中。[84]稠密的地图表达。[272]使用RGB-D传感器提出一个SLAM系统，该系统是基于对环境中的已知物体的检测。 Joint SLAM and Semantics inference将单目SLAM和地图分割联合。 online系统[106]构建模型，使用曼哈顿世界假设，针对室内环境分割主要平面部分的地图。 [16]提出使用场景的几何和语义属性估计相机参数和场景点、物体标签，offline。 [136、184、275]offline、[310]online。 研究点思考首先，对于SLAM问题，我们需要考虑的四个方面： 机器人 环境 传感器-环境组合 性能需求 因此可以考虑不同的机器人/环境/性能组合进行基础性研究的可能性。同时，在机器人移动或者环境极具挑战性时（例如快速机器人动态、高度动态的环境），目前的SLAM算法很容易失败。此外，SLAM算法经常无法应对严格的性能要求，例如，快速闭环控制的高速率估计。目前，针对SLAM系统的研究可以从下面几个方面中开展： 鲁棒性性能研究：故障安全机制和自我调整功能，这样能够做出适应不同场景的系统参数的选择 高层次的提升研究：超越基础的几何重建，能够获取到对环境的一个更高层次的理解，例如语义、可用性、高级几何、物理； 资源利用研究：针对可用的传感和计算资源量身定制，并可以提供根据可用资源调整计算负载的方法； 任务驱动方向研究：自适应地图表示，其复杂性可以根据机器人必须执行的任务而改变。 具体的研究点思考如下。 鲁棒性研究动态环境下的处理，如何检测、提出、追踪变化，可以将变化剔除[221]、考虑到模型[266, 312, 313]如何应对白天和晚上剧烈的光照变化、季节的变化、环境结构的变化（新建筑物-&gt;旧建筑物）白天、夜晚回环检测的应对方法[69, 213、223]对于数据异常情况（外点的出现）的处理，目前SLAM系统不具备提前感知即将面临的失败的能力、无法提供恢复机制应对已经发生的失败情况硬件环境、传感器异常情况的处理？如何检测？如何调整？极端环境下的研究，例如水下[20, 100, 102, 166]非刚性物体的应对方法，非刚性地图的研究[245, 246] 、[36] and [304]、[4, 5, 128]、[225] 可扩展性研究因子图优化复杂性降低方法地图的存储[201、172]多机器人[271]多机器人回环检测的研究，机器人之间可能无法共享同一个参考图像帧，机器人传感器探测视角也有所不同[151]地图中哪些信息需要丢弃、更新、保存，该以怎样的频率更新地图信息？？？在面对严格的带宽限制和通信中断时，保证多机器人团队的可靠运行？？？ [70] 度量表达研究基于路标的地图占用网格地图，地图标准[147]高层次的表达方式，机器人不能识别出自己所处的环境类型，例如room vs. corridor，当然可以使用更复杂的模型（如parameterized primitive instancing）解决这种问题。更紧凑的表达方法可以降低大尺度环境的地图大小。最佳的（可选择性的）表示，简单的室内环境使用参数化的原语、负责的室外环境选择mesh模型，如何比较不同的表达方式、如何选择最有的表达方式？？？[262、285]自动的自适应的表达方式，希望机器人可以根据人物和环境的复杂性自主地选择更复杂或更简单的表达方式。这将对长时间的导航带来很大的帮助。 语义表达不仅仅是分类。 引用文献14299298899412220129115274271198282]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2系统Rviz可视化方案]]></title>
    <url>%2F2018%2F10%2F14%2FORB-SLAM2%E7%B3%BB%E7%BB%9FRviz%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[本篇记录ORB_SLAM2系统嵌入Rviz可视化模块的方案实现。 概述ORB-SLAM的地图和定位可视化是通过Rviz进行展示的，而在ORB-SLAM2中，为了不依赖于ROS，ORB-SLAM2的可视化采用了pangolin库。而我的硕士课题有个需求，就是要使ORB-SLAM2的结果和Cartographer的结果同时显示在同一个可视化工具中，而Cartographer是采用Rviz显示的，还定制了专用的Rviz插件。思考再三，决定为ORB-SLAM2重新添加基于Rviz的可视化模块。 ORB-SLAM2的Rviz可视化ORB-SLAM中关于Rviz的可视化： ORB-SLAM的Rviz可视化使用单独的一个类来完成可视化信息的发布：MapPublisher类 所有的可视化信息都是Rviz的Mark类型，根据发布的地图点、关键帧、Covisibility Graph、Spanning Tree和相机轨迹，使用了不同的Mark类型。 所有的可视化信息，包括地图、轨迹等都是从ORB-SLAM中的Map类中获取的。 每次获得一帧图像，进行Track后，利用MapPublisher类发布可视化信息。 在配置相应的Rviz，使其可以接收可视化信息。 明白了这几点之后，在ORB-SLAM2中添加Rviz可视化模块就很简单了，主要对源代码做以下改动： 添加MapPublisher类和配置Rviz，可以直接复用ORB-SLAM中的MapPublisher类和Rviz文件；并在每次Track之后（执行完mpSLAM-&gt;TrackStereo()）利用MapPublisher类发布可视化信息。 为Map类添加返回相关信息的接口。 特别要注意ORB-SLAM2的坐标系下，z轴是朝前的，而Rviz的坐标系下，z轴是朝上的，因此要做相应的转换。 以上改动可以基于我的这篇文章完成。 ORB_SLAM2中的坐标系ORB_SLAM2中的世界坐标系z轴是朝前的，Rviz坐标系z轴是朝上的，如下图所示。要把ORB_SLAM2中得到的地图（地图点、轨迹、相机、关键帧等）和相机位姿正确显示在Rviz中，需要将数据进行坐标转换。 关键帧或相机的显示是以相机坐标系原点为参照，如下图所示，构造了一个四角锥体，以原点为顶点，底面的四个顶点z值大于0。在获取到位姿，将相机或关键帧转换到ORB_SLAM2世界坐标系统之后，再转换到Rviz世界坐标系。 通过以上几个修改，就能在Rviz中显示ORB-SLAM2的地图构建结果和相机位姿了。 运行结果基于之前的内容，修改启动文件如下： 12345678910&lt;launch&gt; &lt;node name="stereo_left_kitti" pkg="my_image_transport" type="stereo_left_kitti"&gt; &lt;/node&gt; &lt;node name="stereo_right_kitti" pkg="my_image_transport" type="stereo_right_kitti"&gt; &lt;/node&gt; &lt;node name="Stereo_eric" pkg="ORB_SLAM2" type="Stereo_eric" output="screen"&gt; &lt;/node&gt; &lt;node pkg="rviz" type="rviz" name="rviz" output="log" args="-d $(find ORB_SLAM2)/config/rviz.rviz" &gt; &lt;/node&gt;&lt;/launch&gt; 其中rviz.rviz文件使用的ORB_SLAM中的，只需要将line 50修改为： 1Fixed Frame: ORB_SLAM/World 执行命令：roslaunch my_image_transport stereo_image_transport.launch 运行结果： 使用PCL库显示地图点目前我的课题考虑基于ORB_SLAM2生成的点云数据生成二维栅格地图再结合Cartographer的地图，调研后发现可以有两种方案。 方案一：使用pointcloud_to_laserscan包将点云数据转成模拟激光扫描数据，进而可以作为gmapping的输入数据生成二维栅格地图，然后与Cartographer的地图融合。 方案二：使用文章[1]中提出的方法直接将点云数据生成二维栅格地图，融合Cartographer的地图。 方案一实施注意：ORB_SLAM2系统Map类中的地图点是世界坐标系中的所有地图点，而不是每个关键 ROSpointcloud_to_laserscan包pointcloud_to_laserscan_node节点将点云PointCloud转成2D激光扫描 订阅节点：cloud_in(sensor_msgs/PointCloud2) 发布节点：scan(sensor_msg/LaserScan) 参考here1和here2配置好ROS程序包，在MapPulisher类中稍作修改，参照如下内容： 1234567891011121314151617181920212223242526272829303132333435#include&lt;ros/ros.h&gt;#include&lt;pcl/point_cloud.h&gt;#include&lt;pcl_conversions/pcl_conversions.h&gt;#include&lt;sensor_msgs/PointCloud2.h&gt;main (int argc, char **argv)&#123; ros::init (argc, argv, "pcl_create"); ros::NodeHandle nh; ros::Publisher pcl_pub = nh.advertise&lt;sensor_msgs::PointCloud2&gt; ("pcl_output", 1); pcl::PointCloud&lt;pcl::PointXYZ&gt; cloud; sensor_msgs::PointCloud2 output; // Fill in the cloud data cloud.width = 100; cloud.height = 1; cloud.points.resize(cloud.width * cloud.height); for (size_t i = 0; i &lt; cloud.points.size(); ++i) &#123; cloud.points[i].x = 1024 * rand () / (RAND_MAX + 1.0f); cloud.points[i].y = 1024 * rand () / (RAND_MAX + 1.0f); cloud.points[i].z = 1024 * rand () / (RAND_MAX + 1.0f); &#125; //Convert the cloud to ROS message pcl::toROSMsg(cloud, output); output.header.frame_id = "odom";//this has been done in order to be able to visualize our PointCloud2 message on the RViz visualizer ros::Rate loop_rate(1); while (ros::ok()) &#123; pcl_pub.publish(output); ros::spinOnce(); loop_rate.sleep(); &#125; return 0;&#125; 展示效果： 参考资料[1] 2D Grid Mapping and Navigation with ORB SLAM. Abhineet Kumar Singh, Ali Jahani Amiri.]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
        <tag>Rviz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记八-图像表示与存储]]></title>
    <url>%2F2018%2F10%2F12%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E5%85%AB-%E5%9B%BE%E5%83%8F%E8%A1%A8%E7%A4%BA%E4%B8%8E%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[这篇文章记录十四讲中代码学习引出的一些内容，主要是计算机中图像的表示和存储相关的。 引出在阅读十四讲第5讲的源码时，注意到下面这部分代码： 12345678910111213for ( int v=0; v&lt;color.rows; v++ ) for ( int u=0; u&lt;color.cols; u++ ) &#123; //... PointT p ; p.x = pointWorld[0]; p.y = pointWorld[1]; p.z = pointWorld[2]; p.b = color.data[ v*color.step+u*color.channels() ]; p.g = color.data[ v*color.step+u*color.channels()+1 ]; p.r = color.data[ v*color.step+u*color.channels()+2 ]; pointCloud-&gt;points.push_back( p ); &#125; 代码中color是彩色图像，9\10\11行从内存中读取像素[u,v]的三个通道值。如下图所示，图像在OpenCV中的表示和存储： 其中step是行数据长度，即内存中保存图像的一行数据的空间长度。图像矩阵的像素数据按行存储在内存中，通常情况内存足够大的话图像的每一行是连续存放的，也就是在内存上图像的所有数据存放成一行，这种情况在访问时可以提供很大方便，例如上述代码。 图像的表示 对于一个位于$x,y$处的像素，它在程序中的访问方式是： 1unsigned char pixel = image[y][x]; 它对应着灰度值$I(x,y)$的读数。注意$x$和$y$的顺序，如果顺序错误的话，编译器无法提供任何信息，会引起程序运行中的越界错误。 参考资料 视觉slam十四讲第5讲 【OpenCV】访问Mat图像中每个像素的值]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当前开源SLAM方案一览]]></title>
    <url>%2F2018%2F09%2F26%2F%E5%BD%93%E5%89%8D%E5%BC%80%E6%BA%90SLAM%E6%96%B9%E6%A1%88%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[这篇文章记录下目前流行的开源SLAM系统的方案。最近课题进展不顺利，目标不明确、思路不清晰、没有创新点，迷茫、不知所措。还是暂时停一下，花时间查阅一些资料，充实一下自己。 时间 开源方案 传感器形式 VO 稀疏\稠密 论文 地址链接 2007 MonoSLAM 单目 [1] Github 2007 PTAM 单目 [2] Source Code 2015 ORB-SLAM 单目为主 特征点法 稀疏 [3] 链接 Github 2017 ORB-SLAM2 单目、双目、RGB-D 特征点法 稀疏 [4] Github 2014 LSD-SLAM 单目（为主）、双目 直接法 半稠密 [5] home Github 2014 SVO 单目 半直接法 [6] Github 2014 RTAB-MAP RGB-D/双目 [7] Github 2015 OKVIS 多目+IMU [8] Github 2015 ROVIO 单目+IMU [9] Github 2011 DTAM RGB-D 直接法 稠密 [10] Github 2013 DVO RGB-D [11] Github 2016 DSO 单目 [12] Github 2014 RGBD-SLAM2 RGB-D [13] Github 2015 Elastic Fusion RGB-D 稠密 [14] Github 2011 Hector SLAM 激光 [15] wiki 2007 GMapping 激光 [16] wiki 2015 OKVIS 多目+IMU [17] Github 2015 ROVIO 单目+IMU [18] Github Paper 2011 Kinetic Fusion RGB-D 稠密 [19] Kintinuous [20] DynamicFusion 稠密 [21] InfiniTAM 稠密 [22] 论文[2] Klein G, Murray D. Parallel tracking and mapping for small AR workspaces[C]//Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality. IEEE Computer Society, 2007: 1-10. [pdf] [slides] [3] Raúl Mur-Artal, J. M. M. Montiel and Juan D. Tardós. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, October 2015. [pdf] Raúl Mur-Artal and Juan D. Tardós. Probabilistic Semi-Dense Mapping from Highly Accurate Feature-Based Monocular SLAM. Robotics: Science and Systems. Rome, Italy, July 2015. [pdf] [poster] [5] LSD-SLAM: Large-Scale Direct Monocular SLAM (J. Engel, T. Schöps, D. Cremers), In European Conference on Computer Vision (ECCV), 2014. [bib] [pdf] [video] [10] R. A. Newcombe, S. Lovegrove, and A. J. Davison. Dtam: Dense tracking and mapping in real-time. In IEEE International Conference on Computer Vision (ICCV), pages 2320–2327, 2011. 1, 2, 3 [11] Dense Visual SLAM for RGB-D Cameras (C. Kerl, J. Sturm, D. Cremers), In Proc. of the Int. Conf. on Intelligent Robot Systems (IROS), 2013. Robust Odometry Estimation for RGB-D Cameras (C. Kerl, J. Sturm, D. Cremers), In Proc. of the IEEE Int. Conf. on Robotics and Automation (ICRA), 2013 Real-Time Visual Odometry from Dense RGB-D Images (F. Steinbruecker, J. Sturm, D. Cremers), In Workshop on Live Dense Reconstruction with Moving Cameras at the Intl. Conf. on Computer Vision (ICCV), 2011. [12] Fast Semi-Direct Monocular Visual Odometry (ICRA 2014) [13] “3D Mapping with an RGB-D Camera”, F. Endres, J. Hess, J. Sturm, D. Cremers, W. Burgard, IEEE Transactions on Robotics, 2014. [14] ElasticFusion: Real-Time Dense SLAM and Light Source Estimation, T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison and S. Leutenegger, IJRR ‘16 ElasticFusion: Dense SLAM Without A Pose Graph, T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker and A. J. Davison, RSS ‘15 [15] A Flexible and Scalable SLAM System with Full 3D Motion Estimation [16] Improved Techniques for Grid Mapping with Rao-Blackwellized Particle Filters [17] Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland Siegwart and Paul Timothy Furgale. Keyframe-based visual–inertial odometry using nonlinear optimization. The International Journal of Robotics Research, 2015. [20] Real-time Large Scale Dense RGB-D SLAM with Volumetric Fusion, T. Whelan, M. Kaess, H. Johannsson, M.F. Fallon, J. J. Leonard and J.B. McDonald, IJRR ‘14 Kintinuous: Spatially Extended KinectFusion, T. Whelan, M. Kaess, M.F. Fallon, H. Johannsson, J. J. Leonard and J.B. McDonald, RSS RGB-D Workshop ‘12 [23] Reconstructing Street-Scenes in Real-Time From a Driving Car (V. Usenko, J. Engel, J. Stueckler, D. Cremers), In Proc. of the Int. Conference on 3D Vision (3DV), 2015. bib [pdf] Large-Scale Direct SLAM for Omnidirectional Cameras (D. Caruso, J. Engel, D. Cremers),In International Conference on Intelligent Robots and Systems (IROS), 2015. [bib] [pdf] [video] Large-Scale Direct SLAM with Stereo Cameras (J. Engel, J. Stueckler, D. Cremers), In International Conference on Intelligent Robots and Systems (IROS), 2015. [bib] [pdf] [video] Semi-Dense Visual Odometry for AR on a Smartphone (T. Schöps, J. Engel, D. Cremers), In International Symposium on Mixed and Augmented Reality, 2014. [bib] [pdf] [video] Semi-Dense Visual Odometry for a Monocular Camera (J. Engel, J. Sturm, D. Cremers), In IEEE International Conference on Computer Vision (ICCV), 2013. [bib] [pdf] [video] 参考资料 当前的开源SLAM方案 【干货】15种SLAM方案详解 SLAM 综述]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>其他</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cartographer学习三源码分析之ROS应用]]></title>
    <url>%2F2018%2F09%2F19%2FCartographer%E5%AD%A6%E4%B9%A0%E4%B8%89%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BROS%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本篇文章是记录cartographer_ros源码学习有关的内容，使用的cartographer版本是20170320。 概述cartographer_ros是基于cartographer的上层应用，它使用ROS作为通信机制，订阅各传感器数据节点发布到消息发布器的数据，然后打包封装成统一的数据格式，通过接口传入cartographer系统核心；通过接口获取系统核心处理后的数据，发布到相应的消息发布器，由rviz展示。下面是cartographer_ros系统运行时节点图： ROS应用框架下图是一个更简洁的框架图： 其中，cartographer_node是ROS应用的主要节点，它订阅了很多话题，其中/scan是激光雷达的数据，/imu是IMU的数据，又发布了很多数据用于展示。 RVIZ配置文件如果是无参数启动rviz，会默认使用配置~/.rviz/default.rviz。如果是正常退出，这一次的配置就会被保存到~/.rviz/default.rviz，需要另外保存成配置文件的话，可以选择File-&gt;Save Config As。 如果使用自己的配置文件，需要在.launch文件中添加内容： 1234&lt;launch&gt;&lt;node pkg="rviz" type="rviz" name="rviz" args="-d $(find cartographer_ros)/configuration_files/demo_3d.rviz"/&gt;&lt;/launch&gt; 运行bag文件如果需要使用roslaunch使用rosbag运行bag文件，在launch文件中添加内容： 1234&lt;launch&gt; &lt;node pkg="rosbag" type="play" name="playe" output="screen" args="--clock /home/path/to/bagfile/2017-10-18-21-30-41.bag"/&gt; &lt;!-- 注意bag文件的路径必须为绝对路径--&gt;&lt;/launch&gt; bag文件的路径可以通过启动参数确定，使用$(arg bag_filename)替换上面的绝对路径。参数值可以通过命令行赋值，命令格式： 1roslaunch cartographer_ros demo_backpack_3d.launch bag_filename:=$&#123;HOME&#125;/slam/example_bags/b3-2016-04-05-14-14-00.bag]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>Cartographer</category>
      </categories>
      <tags>
        <tag>Lidar SLAM</tag>
        <tag>Cartographer</tag>
        <tag>rviz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu系统提示磁盘空间不足问题解决]]></title>
    <url>%2F2018%2F09%2F19%2Fubuntu%E7%B3%BB%E7%BB%9F%E6%8F%90%E7%A4%BA%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[系统经常会遇到提示磁盘空间不足的问题，记录下一种解决方案。 如果系统有以下提示信息： 可以点击分析，查看使用空间比较多的文件夹，如下图所示： 可以看到是var\log目录占用的空间比较多，所以进入该目录，如果是以.开头的文件，需要在文件管理器中勾选“显示隐藏文件”。可以使用df -h命令查看文件系统空间占用情况，如下： var\log目录下都是些系统的日志文件，可以选择删除比较大的某些日志文件，使用如下命令： sudo rm -rf /var/log/xxx 删除文件后，查看文件系统空间占用情况，如下： 成功解决。]]></content>
      <categories>
        <category>系统</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cartographer学习二源码分析之核心代码]]></title>
    <url>%2F2018%2F09%2F16%2FCartographer%E5%AD%A6%E4%B9%A0%E4%BA%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[本篇文章是记录激光开源SLAM系统Cartographer源码学习有关的内容。现在做的项目是基于师兄的毕业设计，使用的cartographer版本是20170320，和最新的系统已经有了挺大的差别，但大部分网上资料介绍的系统源码版本都比较旧，所以还是先学习旧版本的系统，搞明白核心内容后再了解更新的功能。 概述Google开源的代码包含两个部分：cartographer[1]和cartographer_ros[2]。cartographer主要负责处理来自雷达、IMU和里程计的数据并基于这些数据进行地图的构建，是cartographer理论的底层实现。cartographer_ros则基于ros的通信机制获取传感器的数据并将它们转换成cartographer中定义的格式传递给cartographer处理，与此同时也将cartographer的处理结果发布用于显示或保存，是基于cartographer的上层应用。ROS应用放在下篇文章分析。 先借用知乎问题中mfxox的图： Cartographer代码框架如下图所示，是google官方说明文档中的框架图： Cartographer系统可以分为两个互相有联系的独立系统：local SLAM（frontend）和global SLAM（backend）。 前端检测：local SLAM负责构建局部一致的submaps集合并进行融合，这个过程会产生漂移误差。 后端闭环优化：global SLAM运行在后端线程，负责在scans和submaps之间使用scan-matching找到回环约束，同时融合其他传感器数据（包括检测重力的方向），完成全局一致性任务。 关于GraphSLAM原理，可以参考文章，有例子介绍很清晰。 总得来说，前端负责生成高质量的submaps，后端则负责完成地图的全局一致性任务。 输入的传感器数据有四个： Range Data（激光雷达，摄像头等） Odometry Pose(里程计数据) IMU Data FixedFramePose（是指确定的位置？？） 里程计数据与IMU数据共同进入PoseExtraPolator，做航迹推算。给定一个里程计与IMU得到的位置估计，用两次以上里程计计算平均速度，用两次以上IMU数据计算平均角速度，然后推算出下一时刻的位置姿态，给到Scan Matching中作为扫描匹配的初值。 Range Data数据经过体素滤波（一种滤波方法）和自适应体素滤波，进入scanMatching，作为观测值。scanMatching用论文中介绍的基于ceres优化的scanMatching方法获得观测最优位置估计，经过Motion Filter滤波，作为位置最优估计构建submap。 Cartographer代码结构各模块功能 common：定义基本数据结构以及一些工具的使用接口，包括对数据的处理、时间转换、采样器、脉冲率计算、直方图类、数据计算类、对线程和线程锁的封装等 io：文件流写、点云batch类、 sensor：定义雷达数据及点云等相关的数据结构 transform：定义位姿的数据结构及其相关的转换 mapping：定义上层应用的调用接口以及局部submap构建和基于闭环检测的位姿优化等的接口 kalman_filter：（基于UKF的多传感器数据融合）主要通过kalman滤波器完成对IMU、里程计及基于雷达数据的估计位姿的融合，进而估计新进的laser scan的位姿 mapping：定义上层应用的调用接口以及局部submap构建和基于闭环检测的位姿优化等的接口 mapping_2d和mapping_3d：对mapping接口的不同实现（scan match策略对应的文件在这俩目录下的scan_matching目录中） mapping_2dGlobalTrajectoryBuilder实现接收处理上层应用传递的传感器数据的主要接口。有几个重要的成员函数： AddRangefinderData：用于接收处理上层应用传递的雷达数据 AddImuData：用于接收处理上层应用传递的IMU数据 AddOdometerPose：用于接收处理上层应用传递的里程计数据 LocalTrajectoryBuilderlocal SLAM，主要完成完成UKF（扩展卡尔曼滤波器）、scan matching、局部submap的构建，提供了接收处理传感器数据的public函数： AddImuData：用于处理IMU数据 AddOdometerPose：用于处理里程计数据 AddHorizontalLaserFan：用于处理雷达数据 SparsePoseGraph主要完成基于闭环检测的全局位姿优化。 参考资料 cartographer文档 cartographer ROS文档 cartographer源码阅读（1）——算法整体结构 【SLAM】（一）Google Cartographer的初步尝试 Cartographer理论及实现浅析 Cartographer 代码阅读分析 Cartographer 代码阅读分析-2 Cartographer源码分析58系列]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>Cartographer</category>
      </categories>
      <tags>
        <tag>Lidar SLAM</tag>
        <tag>Cartographer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cartographer学习一论文阅读]]></title>
    <url>%2F2018%2F09%2F12%2FCartographer%E5%AD%A6%E4%B9%A0%E4%B8%80%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[本篇文章记录阅读Google开源Cartographer SLAM系统论文过程中的学习内容。 概况Real-time loop closure in 2D LIDAR SLAM是Google发表在ICRA2016上的一篇论文，开源的系统是大名鼎鼎的Cartographer，目前该系统已经有大神改到Cartographer-ROS版本。本文在阅读论文的基础上，参考其他网络博客资料，学习并记录论文的一些要点，通过这个过程希望能够理解论文的核心内容和系统的实现。 主要论文 Real-Time Loop Closure in 2D LIDAR SLAM , ICRA 2016 Efficient Sparse Pose Adjustment for 2D Mapping (SPA) Real-Time Correlative Scan Matching (BBS) 文章重点 第四部分：local 2d slam，主要是将局部地图的scan matching作为一个最小二乘优化问题，由ceres slover解决。 第五部分： closing loop， 采用了 SPA（Sparse Pose Adjustment）进行后端loop closure。 这个过程中有一个很重要的过程是的scan和 submap的匹配，这里采用了BBS（Branch-and-bound scan matching）, 它可大幅提高精度和速度。 文章贡献文章的重点不是关注SLAM本身，而是提出了一种基于激光的5cm分辨率实时建图和回环检测方法，减少了计算量，满足实时的大场景地图构建以及大规模的实时优化的性能需求。 为了达到实时闭环检测，文章使用了分支上界法来计算scan-to-submap的匹配作为约束。 Scan Matching方法介绍涉及到的相关文献在文后列出，以便以后学习。 scan-to-scan matching：基于激光的SLAM中最常用来估计相关位姿的方法。但是非常容易造成累积误差。[1,2,3,4] scan-to-map matching：可以较少累计误差。其中一种方法是使用Gauss-Newton法在线性插值地图上找到局部最优值，前提是获取到了最优的初始位姿估计，这就需要使用有足够高数据获取速率的激光雷达，以保证局部优化的scan-to-map匹配是高效并且鲁棒的。在不稳定的平台上，需要使用惯性测量单元（IMU）将激光扫描投影到水平面上以估计重力方向。[5] pixel-accurate scan matching：可以进一步减少局部误差，但是计算量比较大。这个方法同样可以用来检测回环。[1] 从laser scans中提取特征做匹配，从而减少计算量[4]。 histogram-based matching用于回环检测[6]。 用机器学习做laser scan data的特征检测[7]。 累积误差处理方式 基于粒子滤波（Particle Filter）的优化。粒子滤波在处理大场景地图时，由于粒子数的极具增长造成资源密集。[8,9,10] 基于位姿图的优化（Graph-based SLAM）。与视觉SLAM的位姿图优化大同小异，主要是在观测方程上的区别。[2,11,12,13] 系统概述 Cartographer是实时的室内建图系统，系统生成5cm分辨率的2D栅格地图地图。 laser scans数据被插入到submap中的最佳估计位置，并假定最佳估计位置在短时间内足够准确。 scan matching针对最近的submap发生，因此它只取决于最近的scans和全局帧位姿估计中累计的误差。 系统使用pose optimization处理误差累计。 submap一旦构建完成，就不会再插入新的scans。submap会用于回环检测过程的scan matching，其实回环检测会将所有已经构建完成的submaps和scans考虑在内。 如果scan和submap在当前位姿估计下足够接近的话，scan matcher会尝试在submap中寻找回环scan。 当一个新的laser scan加入到地图中时，如果该laser scan的估计位姿与地图中某个submap的某个laser scan的位姿比较接近的话，那么通过某种 scan match策略就会找到该闭环。 为了减少计算量，Cartographer设计了特殊的策略来找到回环scan。这个策略就是在当前的位姿估计附近设置搜索窗口，在这个搜索窗口内执行branch-and-bound方法来寻找回环scan，如果在搜索窗口中找到了一个足够好的匹配，则会将该匹配作为回环检测约束条件添加到优化问题中。 系统通过使用branch-and-bound方法，并对每个生成的submap预计算出几个栅格地图，保证回环优化的快速完成，快到可以在加入新的scans之前就完成优化，从而保证了一种软实时约束。 cartographer的整体架构是典型的 前端建图 （局部地图）+后端优化。 符号说明 位姿表示：$\xi=(\xix,\xi_y,\xi{\theta})$ 扫描scan：$H={hk}{k=1,…,K},h_k \in \mathbb{R^2}​$ scan-to-submap变换矩阵：$T_\xi​$ scan-to-submap变换：$T{\xi }h_k=\underbrace{\left(\begin{matrix}cos\xi\theta&amp;-sin\xi\theta\sin\xi\theta&amp;cos\xi\theta\end{matrix} \right)}{R\xi}h_k+\underbrace{\left(\begin{matrix}\xi_x\\xi_y\end{matrix}\right)}{t_\xi}​$ 概率栅格地图概率值：$M:r\mathbb{Z}\times r\mathbb{Z} \to [p{min},p{max}]$ submap世界坐标系下的位姿（m代表map）：$\Xi^m={\xi^mi}{i=1,…,m}​$ scan世界坐标系下的位姿（s代表scan）：$\Xi^s={\xi^sj}{j=1,…,n}​$ scan $i​$在匹配到的submap $j​$坐标系下的位姿：$\xi_{ij}​$ 与scan $i​$和submap $j​$相对应的协方差矩阵：$\sum_{ij}​$ Local 2D SLAM系统将局部和全局方法结合到2D SLAM中，两种方法都对LIDAR观测到的位姿进行了优化。这一部分介绍局部地图的scan matching，该问题被构造成最小二乘问题，使用ceres solver解决。 位姿表示为$\xi=(\xix,\xi_y,\xi{\theta})$，这个位姿表示包括$(x,y)$坐标变换（注意这里是二维坐标），以及角度的旋转$\xi_\theta$，对观测位姿的优化实际上就是对scans的优化。平台采用IMU测量重力方向，将水平安装的LIDAR观测到的scans映射到2D平面。 scan matching局部方法中，将每个连续的scan点集和整个地图的一部分进行匹配，就是和submap $M$进行匹配。这个过程中使用了一种非线性的优化方法将submap和scan点集对齐，这也是scan matching的过程。局部方法积累的误差在全局方法消除，即回环检测。 A. Scanssubmap的构建是一个不断将scan和submap坐标系对齐的迭代过程。 一个scan包含一个起点和很多个终点，起点称为scan origin，终点称为scan points，将scan表示为$H={hk}{k=1,…,K},h_k \in \mathbb{R^2}$。在scan坐标系下，origin就是坐标原点，scan points就是在scan坐标系下的坐标。 当把一个scans插入到一个submap中时，假设scan坐标系到submap坐标系的坐标转换表示为$T\xi$（即scan坐标系在submap坐标系下的位姿$\xi$表示为变换矩阵$T\xi$），即激光传感器在submap坐标系下的位姿。 每个$hk$在submap坐标系下的坐标为：$T{\xi }hk=\underbrace{\left(\begin{matrix}cos\xi\theta&amp;-sin\xi\theta\sin\xi\theta&amp;cos\xi\theta\end{matrix} \right)}{R\xi}h_k+\underbrace{\left(\begin{matrix}\xi_x\\xi_y\end{matrix}\right)}{t_\xi}$ B. Submaps一些连续的scans组成submap。采用概率栅格地图的形式表示这些submaps，$M:r\mathbb{Z}\times r\mathbb{Z} \to [p{min},p{max}]$，以给定的分辨率（例如5cm）将离散栅格地图点映射到值，这些值可以记作栅格地图点被占用的概率。对于每个栅格地图点，都定义一个相应的pixel，这个piexl是针对于分辨率来说的，对于5cm的分辨率来说，一个pixel相当于一个5*5的方格，那么对应于scan中应该有很多个point，即论文中定义的：pixel包含了所有靠近这个栅格地图点的points。 疑问 这里栅格地图点和像素的定义不明白？ 答：栅格地图点就是上图中的叉号处，像素定义为叉号周围所有的点的集合，即小方框。 submaps是扫描点的集合？？概率栅格地图表示submaps怎么理解？？ 看源码理解。 对于每个要插入submap的scan，都会产生一组称为hits的grid point和一组称为misses的grid point。如下图所示。 其中阴影带叉的是hit，加入hits集合；阴影不带叉的是miss（scan origin和scan points连线经过的grid points，排除在hits中的），加入misses集合。每个hits中的grid point被赋予初始值$M=p{hit}$，每个misses中的grid point被赋予初始值$M=p{miss}$。如果grid point已经有$p$值，则用下述公式更新： $odds(p)=\frac{p}{1−p}$ $M{new(x)}=clamp(odds^{−1}(odds(M{old}(x))⋅odds(p_{hit})))$ Clamp函数可以将随机变化的数值限制在一个给定的区间[min, max]内，小于min的数值返回min，大于max的数值返回max。 miss集合的更新也是类似的。 C. Ceres scan matching将scan插入submap之前，需要通过scan matching对scan的位姿$ \xi $进行优化（优化过程参照当前局部submap），优化过程使用基于Ceres库的scan matcher。scan matcher的任务就是找到一个scan的位姿，能够满足scan points在submap中有最大概率值。这里涉及到的优化问题为非线性最小二乘问题，通过Ceres库进行求解。使得scan在栅格地图中的概率值最大，那么就需要使得（cs）最小，非线性最小二乘问题目标函数构造形式如下： $\mathop{\arg\min} \limits{\xi}\sum \limits{k=1}^K(1−M{smooth}(T{\xi}h_k))^2 \qquad\qquad(CS)​$ 平滑函数$M_{smooth}$完成了局部submap中概率值从2D到1D的平滑，将值限制在$[0; 1]$范围内，使用双三次插值法（bicubic interpolation）。通常情况下，这种平滑函数的数学优化比栅格地图的分辨率能够提供更好的精度。 对于局部优化问题，一个相对精确的初始估计非常重要。因此如果通过IMU提供角度信息（角速度），来估计scan matching中位姿的旋转分量$\theta$，可以提高优化的准确性。在缺少IMU的情况下，高频率的scan matching或者pixel-accurate scan matching也可以提高准确性，但会增加时间复杂度。 Closing Loops closing loop采用了 SPA（Sparse Pose Adjustment）进行后端loop closure。 这个过程中有一个很重要的过程是的scan和 submap的匹配，采用的算法是BBS（Branch-and-bound scan matching）, 它可大幅提高精度和速度。 SPA由于scans只和一个包含最近的几个scans的submap进行匹配，上面所讲的scan matcher会产生比较小的累计误差。 系统通过创建一个个小的submaps实现大的场景地图构建，并使用Sparse Pose Adjustment方法[2]优化所有scans和submaps的位姿，提高精准度。 BBS将scans插入处的相关位姿保存在内存中，以便在回环检测优化时使用。此外，所有其他的包含一个scan和一个submap组合，一旦submap不再变化，都会被用于回环检测。一个scan matcher会在后台一直不断的运行，当一个好的scan match被找到，该匹配的约束也会被加入到优化问题（是指回环优化问题）中。 上面这一段内容理解的还不是特别清楚？ 其实是下文要提到的$\xi_{ij}$的求解过程。 A. 优化问题回环的优化问题与scan matching的优化问题类似，都是通过构造非线性最小二乘的方式进行的，允许方便地添加残差以考虑附加的数据。每隔几秒，就使用Ceres计算下式的解： $\mathop{\arg\min} \limits_{{\Xi^m},{\Xi^s}} \frac{1}{2}\sum \limits_{ij}\rho(E^2(\xi^m_i,\xi^s_j;\sum \limits_{ij},\xi_{ij})) \qquad(SPA)$ [15] $\rho$是一个损失函数，比如Huber loss等。使用损失函数的目的是减少加入到优化问题中的离群点对于系统的影响，这种情况在局部对称的环境，如办公室走廊容易发生，scan matching会将错误的约束加入到优化问题。 $\Xi^m={\xi^mi}{i=1,…,m}$是submap的位姿，$\Xi^s={\xi^sj}{j=1,…,n}$是scan的位姿，submap位姿和scan位姿都是世界坐标系下的，并且它们之间存在约束条件（这个约束条件是指什么？）用于完成优化。 对于submap $i$和scan $j$，$\xi{ij}$表示scan在匹配到的submap坐标系下的位姿（$j$在$i$坐标系下的位姿，求解方法在下一节介绍），$\sum{ij}$是相应的协方差矩阵，这个协方差矩阵可以通过[14]的方式获得，也可以通过(CS)公式获得。残差$E$的计算： $E^2(\xi^mi,\xi^s_j;\sum{ij},\xi{ij})=e(\xi^m_i,\xi^s_j;\xi{ij})^T\sum{ij}^{-1}e(\xi^m_i,\xi^s_j;\xi{ij})$ $e(\xi^mi,\xi^s_j;\xi{ij})=\xi{ij}-\left(\begin{matrix}R^{-1}{\xi^mi}(t{\xi^mi}-t{\xi^sj}) \ \xi^m{i;\theta}-\xi^s_{j;\theta}\end{matrix}\right)$ 协方差统计学中，标准差、方差一般是用来描述一维数据的。对于多维数据，一般使用协方差度量两个随机变量关系。 方差定义：$var(X)=\frac{\sum \limits_{i=1}^{n}(X_i-\overline X)(X_i-\overline X)}{n-1}$ 协方差定义：$cov(X,Y)=\frac{\sum \limits (X_i-\overline X)(Y_i-\overline Y)}{n-1}$ 使用协方差来度量各个维度偏离其均值的程度。如果协方差为正，两个变量是正相关；为负，负相关；为0，无关，即相互独立。 协方差矩阵协方差只能处理二维问题，对于多维数据，就需要计算多个协方差。一般使用协方差矩阵组织。 协方差矩阵定义：$C{n\times n}=(c{i,j},c_{i,j}=cov(Dim_i,Dim_j))$ 如三维数据的协方差矩阵为：$C=\left(\begin{matrix}cov(x,x)&amp;cov(x,y)&amp;cov(x,z)\cov(y,x)&amp;cov(y,y)&amp;cov(y,z)\cov(z,x)&amp;cov(z,y)&amp;cov(z,z)\end{matrix}\right)$ 协方差矩阵是一个对称的矩阵，而且对角线是各个维度的方差。 B. Branch-and-bound scan matching之前提到的回环约束关系$ξij$（scan $j$在submap $i$坐标系中的位姿）就是通过这里的方法得到的，也是整篇论文最核心的地方。首先看一下pixel-accurate match的匹配过程： $\xi^*=\mathop{\arg\max} \limits{\xi \in W}\sum\limits{k=1}^{K}M{nearest}(T\xi h_k)\qquad\ \ (BBS)$ [14] 其中$W$是搜索空间（搜索窗口），$M_{nearest}$就是该pixel对应的grid point的M值。之后可以通过(CS)公式进一步提高$\xi$匹配的准确度。 搜索空间和搜索步长的选择是决定pixel-accurate match是否高效的关键。搜索步长的计算方式： $d{max}=\mathop{max}\limits{k=1,…,K} \ \ | \mathrm{h} _k|$ $\delta\theta=arccos(1-\frac{r^2}{2d^2{max}})$ $wx=\lceil \frac{W_x}{r} \rceil$, $w_y=\lceil \frac{W_y}{r} \rceil$, $w\theta=\lceil \frac{W\theta}{\delta\theta} \rceil$ 其中$d{max}$是所有scan点集中跨度最大的那个值，$Wx=Wy=7m$，$Wθ=30^\circ$，因此搜索空间就可以确定了。此时搜索空间的大小是7m*7m。 $\overline{W}={-wx,…,w_x}\times{-w_y,…,w_y}\times{-w\theta,…,w_\theta}$ $W={\xi0+(rj_x,rj_y,\delta\theta j\theta):(j_x,j_y,j\theta)\in\overline{W}}​$ 有了搜索空间和搜索步长，就可以得到最原始的暴力搜索方式。算法1如下图所示： 为了进一步提高搜索效率，Cartograoher采用了branch and bound approach的方式。branch and boundapproach是一种在问题的解空间树上搜索问题的解的方法，被Google套用在最优位姿的搜索中，从而将无法实时化暴力解优化到可以满足实时化。分支上界法就是每个分支代表一种可能，使用DFS找到最佳位置即可，和算法1解决的是相同的问题。只不过这个节点的score是可能的最大值而已。 分支上界法分为以下几步：节点选择、分支、计算上限。对于这三步，论文中有具体讲解。节点选择采用的是DFS，分支算法，采用的是算法2。算法3是将节点选择和分支结合到一起之后的算法。 总结论文阐述了一个2D的SLAM系统，系统采用闭环检测的scan-to-submap matching，同时还有图优化（graphoptimization）。一个submap的创建使用的是局部的、基于栅格地图的（grid-based）SLAM方法。在后台，所有的点集与相近的submap的匹配使用的是pixel-accurate scan matching的方法，然后建立闭环检测的约束。这个约束图（基于submap和scan pose的）都会周期性的被后台更改。这个操作是采用GPU加速将已完成的submap和当前的submap进行结合。系统图引用的博客中的。 Scan是激光扫描的单帧数据，通过累加Scan来构建局部地图（Submap），采用的是grid-based 的SLAM方法。生成约束关系的scan和submap的匹配算法采用的是pixel-accurate scan matching的方法。Cartographer的实现并没有采用滤波方法，而是采用了类似图优化的模型进行Pose estimation，具体的实现是用了Ceres scan matching(Ceres是Google自家的库) 。 这样构造出来的很多很多submap是会产生累计误差的，最后通过Loop closing来消除这些误差，完成闭环。 参考资料 Real-time loop closure in 2D LIDAR SLAM google cartographer的论文《real-time loop closure in 2D LIDAR SLAM》翻译 cartographer对应论文的琢磨(2) Real-Time Loop Closure in 2D LIDAR SLAM 论文笔记 cartographer算法简析 【SLAM】（一）Google Cartographer的初步尝试 [转]浅谈协方差矩阵 相关文献[1] E. Olson, “M3RSM: Many-to-many multi-resolution scan matching,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), June 2015.[2] K. Konolige, G. Grisetti, R. Kümmerle, W. Burgard, B. Limketkai, and R. Vincent, “Sparse pose adjustment for 2D mapping,” in IROS, Taipei, Taiwan, 10/2010 2010.[3] F. Lu and E. Milios, “Globally consistent range scan alignment for environment mapping,” Autonomous robots, vol. 4, no. 4, pp. 333–349, 1997.[4] F. Martı́n, R. Triebel, L. Moreno, and R. Siegwart, “Two different tools for three-dimensional mapping: DE-based scan matching and feature-based loop detection,” Robotica, vol. 32, no. 01, pp. 19–41,2014.[5] S. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf, “A flexible and scalable SLAM system with full 3D motionestimation,” in Proc. IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR). IEEE, November 2011.[6] M. Himstedt, J. Frost, S. Hellbach, H.-J. Böhme, and E. Maehle, “Large scale place recognition in 2D LIDAR scans using geometrical landmark relations,” in Intelligent Robots and Systems (IROS 2014),2014 IEEE/RSJ InternationalConference on. IEEE, 2014, pp. 5030–5035.[7] K. Granström, T. B. Schön, J. I. Nieto, and F. T. Ramos, “Learning to close loops from range data,” The International Journal of Robotics Research, vol. 30, no. 14, pp. 1728–1754, 2011.[8] G. Grisetti, C. Stachniss, and W. Burgard, “Improving grid-based SLAM with Rao-Blackwellized particle filters by adaptive proposals and selective resampling,” in Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on. IEEE, 2005, pp. 2432–2437.[9] G. D. Tipaldi, M. Braun, and K. O. Arras, “FLIRT: Interest regions for 2D range data with applications to robot navigation,” in Experimental Robotics. Springer, 2014, pp. 695–710.[10] J. Strom and E. Olson, “Occupancy grid rasterization in large environments for teams of robots,” in Intelligent Robots and Systems (IROS),2011 IEEE/RSJ International Conference on. IEEE, 2011, pp. 4271–4276.[11] R. Kümmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard,“g2o: A general framework for graph optimization,” in Robotics and Automation (ICRA), 2011 IEEE International Conference on. IEEE,2011, pp. 3607–3613.[12] L. Carlone, R. Aragues, J. A. Castellanos, and B. Bona, “A fast and accurate approximation for planar pose graph optimization,” The International Journal of Robotics Research, pp. 965–987, 2014.[13] M. Bosse and R. Zlot, “Map matching and data association for large-scale two-dimensional laser scan-based SLAM,” The International Journal of Robotics Research, vol. 27, no. 6, pp. 667–691, 2008.[14] E. B. Olson, “Real-time correlative scan matching,” in Robotics and Automation, 2009. ICRA’09. IEEE International Conference on. IEEE, 2009, pp. 4387–4393. [15]Efficient Sparse Pose Adjustment for 2D Mapping]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>Cartographer</category>
      </categories>
      <tags>
        <tag>Lidar SLAM</tag>
        <tag>Cartographer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe的GPU模式安装]]></title>
    <url>%2F2018%2F09%2F08%2FCaffe%E7%9A%84GPU%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[这篇文章是有关Caffe GPU模式安装的内容。 暑假终于入手了一台属于自己的可以称之为高配置的笔记本，实验室的台式机显卡太渣，不想耗费时间和精力再配置台式机，自己的笔记本随时随地方便使用，鼓捣好自己的笔记本环境配置才是王道。 笔记本配置 CPU-Intel七代i7 内存-16G 显存6G 双显卡-GeForce GTX 1060 + Intel 双系统Win10+Ubuntu16.04 相关链接 查看自己的GPU是否支持CUDA 下载匹配自己显卡的驱动(apt方法安装则不需要) CUDA下载 选择合适的版本，选择下载runfile文件 下载cuDNN library 需要注册帐号 NVIDIA CUDA Installation Guide for Linux Caffe1官网 Caffe源码 具体过程 安装Nvidia显卡驱动 安装CUDA9 安装cuDNN 安装Caffe 安装Nvidia显卡驱动 从相关链接2查询到自己系统对应的版本 安装驱动 12345sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-390 #此处要根据上面查询到的版本适当更改sudo apt-get install mesa-common-dev sudo apt-get install freeglut3-dev 之后重启系统让GTX1060显卡驱动生效。 测试。终端输入nvidia-smi，会显示显卡相关的信息，说明安装驱动成功。 可以打开Nvidia x server setting切换双显卡。 可能遇到的问题执行sudo apt-get install nvidia-390命令时可能会产生依赖包冲突的问题。 解决方法： 使用aptitude安装，首先安装apitudesudo apt-get install aptitude，使用apitude进行安装的命令sudo aptitude install xxxx。根据提示选Y/N/Q，通常选N直到出现对版本做降级处理，点Y即可解决。 安装CUDA9.0 从相关链接3下载CUDA9.0，选择funfile（下载好的文件名：cuda_9.0.176_384.81_linux.run） 注意：CUDA要和显卡驱动对应，如下图。参考链接。 执行命令sudo ./cuda_9.0.176_384.81_linux.run启动安装程序，一直按空格到最后，输入accept接受条款。 输入n不安装nvidia图像驱动 输入y安装cuda 9.0工具 回车确认cuda默认安装路径：/usr/local/cuda-9.0 输入y或者n安装或者不安装指向/usr/local/cuda的符号链接 输入y安装CUDA 9.0 Samples，以便后面测试 回车确认CUDA 9.0 Samples默认安装路径：/home/eric（eric是我的用户名），该安装路径测试完可以删除 安装完显示如下图 如果需要卸载CUDA： To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-9.0/bin 添加环境变量 执行命令sudo gedit /etc/profile编辑文件，在最后添加： 12export PATH=/usr/local/cuda-9.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:$LD_LIBRARY_PATH 重启系统！ 测试CUDA Toolkit安装是否正确：nvcc --version，输出以下信息说明安装正确： 编译CUDA Samples，默认路径为~/NVIDIA_CUDA-9.0_Samples 1make 生成可执行文件在~/NVIDIA_CUDA-9.0_Samples/bin/x84_64/linux/release 1./deviceQuery 会有如下输出： 如果在第一步下载的CUDA和显卡驱动不对应的话，会提示： 12&gt; CUDA driver version is insufficient for CUDA runtime version&gt; 安装cuDNNcuDNN的全称为NVIDIA CUDA® Deep Neural Network library，是NVIDIA专门针对深度神经网络（Deep Neural Networks）中的基础操作而设计基于GPU的加速库。cuDNN为深度神经网络中的标准流程提供了高度优化的实现方式，例如convolution、pooling、normalization以及activation layers的前向以及后向过程。 从相关链接4下载合适版本的cuDNN（下载好的文件名：cuda_9.0.176_384.81_linux.run） 解压cuda_9.0.176_384.81_linux.run 1234tar -xvzf cuda_9.0.176_384.81_linux.runsudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 更新软链接（应该不需要这一步） 1234cd /usr/local/cuda/lib64/sudo rm -rf libcudnn.so libcudnn.so.5sudo ln –s libcudnn.so.5.1.10 libcudnn.so.5sudo ln –s libcudnn.so.5 libcudnn.so 安装Caffe 安装依赖包 12345sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libatlas-base-devsudo apt-get install build-essentialsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev 安装python的pip和easy_install，方便安装软件包 12345cdwget --no-check-certificate https://bootstrap.pypa.io/ez_setup.pysudo python ez_setup.py --insecurewget https://bootstrap.pypa.io/get-pip.pysudo python get-pip.py 安装科学计算和python所需的部分库 1sudo apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran python-numpy 安装python依赖 1sudo apt-get install python-pip #安装pip 编译Caffe 终端输入123cd /home/eric/caffecp Makefile.config.example Makefile.configgedit Makefile.config 将USE_CUDNN := 1取消注释 INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include后面打上一个空格 然后添加/usr/include/hdf5/serial如果没有这一句可能会报一个找不到hdf5.h的错误 终端输入1make make过程中出现找不到lhdf5_hl和lhdf5的错误解决方案：在计算机中搜索libhdf5_serial.so.10.1.0，找到后右键点击打开项目位置。该目录下空白处右键点击在终端打开，打开新终端输入 sudo ln libhdf5_serial.so.10.1.0 libhdf5.so sudo ln libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so 。最后在终端输入sudo ldconfig使链接生效，原终端中输入make clean清除第一次编译结果，再重新编译。 出现nvcc fatal : Unsupported gpu architecture &#39;compute_20&#39;的错误将Makefile.config文件中CUDA_ARCH :=包含compute_20的两项删除即可。 终端输入：1234make test -j4make runtest -j4make pycaffe -j4make distribute 生成发布安装包 测试python，终端输入:123cd /home/erci/install/caffe/pythonpythonimport caffe 如果不报错就说明编译成功。 提示如果执行import caffe，出现错误ImportError: No module named skimage.io，可以进行如下操作： sudo apt-get install python-skimage sudo apt-get install python-numpy python-scipy python-matplotlib python-sklearn python-skimage python-h5py python-protobuf python-leveldb python-networkx python-nose python-pandas python-glags ipython sudo apt-get update caffe目录下：make pycaffe 参考资料 [专业亲测]Ubuntu16.04安装Nvidia显卡驱动（cuda）—解决你的所有困惑 ubuntu16.04+gtx1060+cuda8.0+caffe安装、测试经历 Ubuntu16.04+GTX1050+CUDA8.0配置深度学习环境 ubuntu16.04下软件依赖冲突的解决方案 cuDNN安装官网教程]]></content>
      <categories>
        <category>深度学习</category>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lightweight_mapping学习之MapDrawer]]></title>
    <url>%2F2018%2F09%2F06%2Flightweight_mapping%E5%AD%A6%E4%B9%A0%E4%B9%8BMapDrawer%2F</url>
    <content type="text"><![CDATA[这篇文章是有关lightwight_mapping项目源码分析的内容，主要记录MapDrawer模块新增的内容。 概述ORB_SLAM2可视化的功能采用的Pangolin库，MapDrawer类是对地图可视化的实现，在可视化线程中被调用，类中各功能的实现都是基于Pangolin的，彼此之间过程类似，比较简单。lightwight_mapping项目在该类中新添了四个功能函数： void DrawVertexes(bool flag); void DrawTetrahedra(bool flag, double y_base ); void DrawSpace(bool flag); void DrawMesh(bool flag); 通过这四个功能函数访问LocalMeshing线程生成的三维空间顶点、三角形、边，并对实现这些信息的可视化，对应的文件为MapDrawer.h和MapDrawer.cpp。 函数功能 该函数通过接口获取LocalMeshing线程生成的三维顶点，使用GL_POINTS类型，通过glVertex3f()函数将顶点可视化。 该函数通过接口获取LocalMeshing线程生成的三维空间中的三角形（其实就是三个三维点组成的结构），使用GL_TRIANGLES类型，通过glVertex3f()函数将三角形可视化（这里很有意思，和可视化顶点用的是一个函数～），三角形的各顶点使用的不同颜色。 该函数通过接口获取LocalMeshing线程生成的三维空间中的边（其实就是两个三维点组成的结构），使用GL_LINES类型，通过glVertex3f()函数将三维边可视化，两个顶点使用的相同颜色。 该函数通过接口获取LocalMeshing线程生成的三维空间中的三角形，使用GL_LINES类型，通过glVertex3f()函数将三角形可视化，这里三角形的三个顶点使用的同一种颜色。 参考资料 无]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>navigation</category>
      </categories>
      <tags>
        <tag>lightweight_mapping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跨平台可视化库Pangolin]]></title>
    <url>%2F2018%2F09%2F05%2F%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BA%93Pangolin%2F</url>
    <content type="text"><![CDATA[这篇文章记录SLAM中常用的一种跨平台可视化库的简单使用，包括点、线、面的绘制等。 概述Pangolin是SLAM系统中常用于可视化的跨平台库，它是对OpenGL进行封装的轻量级的OpenGL输入/输出和视频显示的库。可以用于3D视觉和3D导航的视觉图，可以输入各种类型的视频、并且可以保留视频和输入数据用于debug。其他的可视化库还有MRPT、OpenCV等，都是跨平台的库。Pangolin库中的各种数据类型人如其名，很容易理解。下面学习一些平时常用的内容，实例程序可以参考参考资料1。 点在Pangolin中，点是一切的基础。OpenGL提供了一系列函数指定一个点，它们都以glVertex开头，后面跟一个数字和1~2个字母。例如：glVertex2d、glVertex2f、glVertex3f、glVertex3fv等等。其中数字表示参数的个数，字母表示参数的类型： s表示16位整数（OpenGL中将这个类型定义为GLshort） i表示32位整数（OpenGL中将这个类型定义为GLint和GLsizei） f表示32位浮点数（OpenGL中将这个类型定义为GLfloat和GLclampf） d表示64位浮点数（OpenGL中将这个类型定义为GLdouble和GLclampd） v表示传递的几个参数将使用指针的方式 这些函数除了参数的类型和个数不同以外，功能是相同的。OpenGL的很多函数都是采用类似的形式。 OpenGL中描述一个面（线、点），采用glBegin+glEnd命令组的形式： 12345glBegin(形状); glVertex(顶点1); glVertex(顶点2); //…… glEnd(); 形状可以设为： GL_POINTS：点 GL_LINES：线 GL_LINE_STRIP：折线 GL_LINE_LOOP：封闭折线 GL_TRIANGLES：三角形 GL_POLYGON：多边形 void glPointSize(GLfloat size);，该函数用于设定点的大小，size必须大于0.0f，默认值为1.0f，单位为“像素”。 注意：对于具体的OpenGL实现，点的大小都有个限度的，如果设置的size超过最大值，则设置可能会有问题。 线void glLineWidth(GLfloat width);，该函数用于设定直线的宽度，其用法跟glPointSize类似。画线的形式和画点函数十分类似，不同在于glBegin()中的符号常量。使用图元常量GL_LINES可连接每一对相邻顶点而得到一组直线段。 三角形画三角形以不同顶点的连接有三种方式，但都是内部填充的方式 。 GL_TRIANGLES：如同GL_LINES一样，第一个三角形的点是V0,V1,V2，第二个则是V3,V4,V5，即是一个3的倍数。不然最后的一个或两个点不显示。 GL_TRIANGLE_STRIP：填充方式犹如放弃前一个顶点，如第一个三角形V0,V1,V2，第二个则是V1,V2,V3(舍弃V0)。 GL_TRIANGLE_FAN：填充方式将永远以V0为起始点，如第一个三角形为V0,V1,V2，第二个则是V0,V2,V3 。 其他函数glColor3f颜色设置函数，有三个float类型的参数，参数值的范围是[0.0, 1.0]。具体的有： 12345678glColor3f(0.0, 0.0, 0.0); //--&gt; 黑色 glColor3f(1.0, 0.0, 0.0); //--&gt; 红色 glColor3f(0.0, 1.0, 0.0); //--&gt; 绿色 glColor3f(0.0, 0.0, 1.0); //--&gt; 蓝色 glColor3f(1.0, 1.0, 0.0); //--&gt; 黄色 glColor3f(1.0, 0.0, 1.0); //--&gt; 品红色 glColor3f(0.0, 1.0, 1.0); //--&gt; 青色 glColor3f(1.0, 1.0, 1.0); //--&gt; 白色 需要注意的是，如果在glBegin()与glEnd()函数之间多次连续调用颜色函数，那么只会显示出最后一次调用对应的颜色。例如： 12345glBegin(GL_POINTS) glColor3f(0.0, 1.0, 0.0); //绿色 glColor3f(1.0, 1.0, 0.0); //黄色 glVertex(0.25, 0.75, 0.0); glEnd(); 画出来的线是黄色的。 glBegin()glBegin()和glEnd()之间可调用的函数： glVertex()设置顶点坐标 glColor()设置当前颜色 glIndex()设置当前颜色表 glNormal()设置法向坐标 glEvalCoord()产生坐标 glCallList(),glCallLists()执行显示列表 glTexCoord()设置纹理坐标 glEdgeFlag()控制边界绘制 glMaterial()设置材质 参考资料 OpenGL（三）之基础绘制篇 OpenGL之glColor3f函数 Pangolin学习]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>可视化库</category>
      </categories>
      <tags>
        <tag>Pangolin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lightweight_mapping学习之LocalMeshing]]></title>
    <url>%2F2018%2F09%2F04%2Flightweight_mapping%E5%AD%A6%E4%B9%A0%E4%B9%8BLocalMeshing%2F</url>
    <content type="text"><![CDATA[这篇文章是有关lightwight_mapping项目源码分析的内容，主要记录LocalMesshing模块，该模块涉及CGAL库Delauary三角剖分的知识。 概述LocalMeshing线程主要任务是使用SLAM优化后的稀疏特征（优化的稀疏三维点云）转化为稠密体积表示。具体来说，SLAM模块提供优化后的稀疏三维点云，LocalMeshing线程使用Delaunay三角剖分算法将三维空间进行细分，使用可视化约束刻画空间。LocalMeshing线程的输入是来自LoopClosing线程传送的关键帧（其实这些关键帧都是在Tracking线程创建的），输出是一系列的三维空间中的点、三角形、边结构，这些信息由可视化线程展示在窗口。 疑问 关键帧约束和非关键帧约束区别何在？为何要区分？顶点细化的最后为何要添加这两种约束？？ 顶点删除过程，会对新产生的空洞重新三角剖分？？ 关于Delaunay三角剖分Triangulation三角剖分是代数拓扑学最基本的研究方法。以曲面三角剖分为例，三角剖分需要满足一些条件： （1）每一块碎片都是曲边三角形（曲边三角形就是以等边三角形的三个顶点为圆心，边长为半径画出的图形，曲面的宽度是等长的） （2）曲面上任何两个这样的曲边三角形，不能同时相交两条或两条以上的边，只能不相交或者是相切于一条公共边 （3）曲面中所有的都是三角面，且所有三角面的合集是所有点集合的凸包 Delaunay Triangulation假设边的集合E中的一条边e（两个端点为a，b）,e如果满足下列条件，则称之为Delaunay边： 存在一个圆经过a,b两点，圆内不包含点集中的任何点，这一特性称为空圆特性。 如果一个曲面的点集的一个三角剖分只包含Delaunay边，那么这样的三角剖分就称为Delaunay三角剖分。 关于Delaunay三角剖分更为直观的定义是：三角剖分中的每个三角形的外接圆的内部都不包含点集中的任何点。 Delaunay三角剖分的算法有翻边算法、逐点插入算法、分割合并算法以及Bowyer-Watson算法等。 下图是Delaunay三角剖分的一个直观示意图： LocalMeshing.h新增文件： 1234delaunay文件夹：FreespaceDelaunayAlgorithm相关的文件dataStructure.hFColorMap.h/FColorMap.cppLocalMeshing.h/LocalMeshing.cpp 增添内容文件： 1MapDrawer.h MapDrawer.cpp 重要的数据结构 Class Delaunay3CellInfo：Delaunay三维剖分四面体信息 Class pointInfo：顶点信息 Class constraintInfo：约束信息类，成员包括约束类型、关联的Cell集合 约束约束定义为相机中心与地图点（顶点）之间的线段。每个约束有一个ID，并且保存定义该约束的顶点的ID。 三种类型： CON_KF：关键帧约束 CON_NONKF：非关键帧约束 CON_INFINITY：deleted 约束（无穷约束？？），程序中并没有用到。 关于添加约束的函数：（分别在什么情况下使用？在后面约束插入函数的介绍中说明） 123addSetOfConstraints( int vertexID, float pose_x, float pose_y, float pose_z, float time )：直接与点关联的约束addConstraintKF(int vertexID, KeyFrame *kF)：与关键帧关联的约束addConstraintNonKF() LocalMeshing::buildMesh2该函数是LocalMeshing线程实现功能的主要函数，构建三角剖分过程在该函数中完成。函数主要过程如下： 检查关键帧队列是否为空，不为空则弹出队列头的关键帧，并设置当前关键帧禁止被设为bad； 检查当前关键帧的数据可用性、是否为bad； 获取当前关键帧的位姿的逆，即Twc，由此得到旋转矩阵、平移向量； 获取当前关键帧观测到的地图点vpMapPoints； 关键帧图像转成BGR彩色图； 删除外点 从outlierVertexList集合获取LocalMapping线程截至目前剔除的外点，保存在currentOutlierVertexList，并将outlierVertexList（它是LocalMapping线程地图点剔除过程保存外点的接口）外点清空，重新收集； 遍历获取到的外点（也没进行什么有实质内容的操作啊，有什么作用？会在全局顶点集合中查找每一个外点）； 调用顶点集合删除算法删除外点，并得到与外点关联的约束集合。 根据4获取到的vpMapPoints收集地图点和约束； 更新地图点和约束 调用顶点插入函数插入顶点，并得到与顶点相关联的约束集合； 处理上一步得到的约束集合； 插入关键帧约束。 检查关键帧变化 获取当前关键帧的临近关键帧集合，对于每一个关键帧进行如下操作： 获取当前关键帧匹配到的地图点； 检查地图点，并收集所有地图点到集合； 检查收集到的地图点变化，如果变化大于阈值则需要更新，将地图点放入待更新集合； 对于待更新集合中的地图点，调用顶点细化函数更新地图点； 对于临近关键帧集合中的每一个关键帧进行如下操作： 如果关键帧是bad，则将该关键帧关联的所有约束删除，处理下一个关键帧，否则，继续后面的操作； 获取当前关键帧的位姿； 对于当前关键帧关联的所有约束，进行如下操作： 获取当前约束关联的顶点，得到其坐标信息； 检查位姿和三维点的变化确定是否需要更新约束，如需要更新则删除当前约束，重新添加关键帧约束。 发布网格地图tetsToTris_naive2，创建三维空间点、三角形、边集合，主要过程如下： 初始化三维空间点、三角形、边集合为空； 创建顶点句柄列表，关联到边界顶点（边界顶点与无线远处连接）； 填充顶点列表，创建有限的非边界顶点句柄列表，并创建有用的句柄与顶点的关联映射； 遍历有限面集合，并在网格上添加三角形，创建三角形、边。 顶点插入算法该算法输入是一个顶点的信息，输出是由该顶点引起的冲突Cell关联的约束的集合，由于顶点的插入可能会产生新的四面体，所以还需要重新检查该集合中的约束，将约束关联到四面体（快速搜索与新的约束相交的四面体，并标记为空）。 对应函数1addVertex(int vertexID, float x, float y, float z, set&lt;int&gt;&amp; setUnionedConstraints ) 函数参数 vertexID：顶点ID x,y,z：顶点三维坐标 setUnionedConstraints：顶点关联的约束集合 算法过程 使用传入的参数创建待处理的顶点； 定位顶点的位置； 找到由此顶点引起的冲突Cell集合； 对于每一个冲突Cell，将其从关联的约束的关联Cell列表中删除，同时得到所有冲突Cell关联的约束集合（作为算法输出）； 删除冲突的Cell、插入新顶点并重新三角化空洞产生新的Cell； 将顶点保存到全局顶点集合。 约束插入算法插入约束的函数有三种，分别适用于不同的情况。 addConstraintNonKF()：适用于没有和关键帧关联的顶点，函数直接输入顶点坐标信息； addConstraintKF()：适用于和关键帧关联的顶点，通过关键帧的位姿获取顶点的坐标信息，约束的插入过程与第一种类似，只是增加了和关键帧相关的一些操作，例如该约束分类为CON_KF、当前关键帧的约束列表会添加该约束； addSetOfConstraints：适用于插入已有的约束，函数输入约束集合，使用约束关联的顶点获取顶点坐标信息。 顶点插入的过程大体类似，只介绍第一种插入函数。 对应函数1addConstraintNonKF( int vertexID, float pose_x, float pose_y, float pose_z, float time ) 函数参数 vertexID：顶点ID pose_x,pose_y,pose_z：顶点三维坐标 time：时间戳 算法过程 通过顶点ID获取顶点信息、顶点操作手柄； 使用传入的参数创建待处理的约束，设为CON_NONKF类型； 将约束保存到全局约束集合； 将约束关联到当前顶点； 将约束关联到四面体（快速搜索与新的约束相交的四面体，并标记为空）。 约束删除算法对应函数1removeConstraint( int constraintID ) 函数参数 constraintID：函数输入，要删除的约束ID 算法过程 获取与该约束关联的所有Cell，将该约束与所有Cell消除关联； 获取与该约束关联的顶点，将该约束从顶点的约束列表中删除； 获取该约束关联的关键帧（如果是CON_KF类型的约束），将该约束从关键帧的约束列表中删除； 从全局约束集合中删除该约束。 顶点删除算法包括单个顶点的删除和顶点集合的删除两个函数。顶点删除的过程大致可以分为四个步骤： 删除与该外点关联的约束 收集外点附带的四面体关联的约束集合； 删除外点及其附带的四面体，并对产生的空洞重新三角剖分； 检查约束集合，约束可能会关联到新产生的四面体。 删除顶点对应函数1removeVertex_origin( int vertexID ) 函数参数 vertexID：函数输入，要删除的外点ID 算法过程 获取该点相关的约束集合（约束ID），调用约束删除算法剔除每一个约束； 获取与该外点相关的所有四面体的Cell_hande，对于每一个执行如下操作： 获取与该Cell关联的约束集合； 对于每一个约束，将当前Cell从约束的Cell列表中删除，并收集该约束到集合。 剔除该外点（同时会对新产生的空洞重新三角剖分）； 处理前面收集的约束集合，将约束关联到四面体（快速搜索与新的约束相交的四面体，并标记为空）。 删除顶点集合对应函数1removeVertexSet(list&lt;int&gt;&amp; currentOutlierVertexList, std::set&lt;int&gt;&amp; setUnionedConstraints) 函数参数 currentOutlierVertexList：函数输入，要删除的外点集合 setUnionedConstraints：函数输出，与删除外点相关联的约束集合，同顶点插入函数一样，也需要检查约束集合。 算法过程与顶点删除过程类似，只是多了访问待删除外点集合的循环。 顶点细化算法对应函数1moveVertexSet( const list&lt;int&gt;&amp; moveVertexList ) 函数参数 moveVertexList：函数输入，待细化（更新坐标）的顶点集合 算法过程 对于待细化顶点集合中的每一个顶点，进行如下操作： 获取该顶点的ID、坐标信息（新值），构造tmp顶点（这些顶点具备足够的信息，在后续过程进行更新）； 获取该顶点的约束集合，对于集合中的每一个约束进行如下操作： 如果该约束类型为CON_KF，则在tmp顶点中添加该顶点关联的关键帧位姿信息； 如果该约束类新为CON_NONKF，则用该顶点的坐标值（旧值）为tmp顶点赋值。 将tmp顶点加入moveList集合； 调用顶点集合删除函数，将待细化顶点集合中的顶点全部删除，并得到与删除顶点关联的约束集合； 调用顶点添加函数，添加moveList集合中的顶点（这一步和上一步会产生新的四面体）； 调用addSetOfConstraints函数，处理2中得到的约束集合，与新生成的四面体关联； 添加与moveList集合中的顶点关联的关键帧约束； 添加与moveList集合中的顶点关联的非关键帧约束。 参考资料 论文：Building maps for autonomous navigation using sparse visual SLAM features CGAL手册 书籍：Computational Geometry Algorithms and Applications(third edition)]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>navigation</category>
      </categories>
      <tags>
        <tag>lightweight_mapping</tag>
        <tag>Delaunay三角剖分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记七-双目相机模型]]></title>
    <url>%2F2018%2F09%2F01%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%83-%E5%8F%8C%E7%9B%AE%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这篇文章是视觉SLAM十四讲第5讲阅读过程中总结和记录的学习内容，主要记录双目相机模型，这是视觉SLAM投影、坐标变换等内容的基础，一定要一次性全搞透。 双目相机一般由左右两个相机水平放置组成，可以把两个相机都看作是针孔相机。水平放置的两个相机，意味着它们的光圈中心都位于x轴上。两者之间的距离成为双目相机的基线（Baseline，记做$b$）。双目相机的成像模型如下所示： $O_L,O_r$为左右光圈中心，方框为成像平面，$f$为焦距。$u_L,u_R$为成像平面的坐标。 考虑空间点$P$，它在左右相机各成一像，记做$P_L,P_R$。由于相机基线的存在，它们的成像位置是不同的。记左右侧坐标分别为$u_L,u_R$，根据三角形相似关系，有： $\frac{z-f}z=\frac{b-u_L+u_R}b$ 整理得： $z=\frac{fb}d,\quad d=u_L-u_R$ 其中，$d$为左右图横坐标之差，称为视差。根据视差，可以估计一个像素与相机之间的距离。视差与距离成反比：视差越大，距离越近。 参考资料 视觉SLAM十四讲第5讲]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
        <tag>双目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记六-针孔相机模型]]></title>
    <url>%2F2018%2F09%2F01%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E5%85%AD-%E9%92%88%E5%AD%94%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这篇文章是视觉SLAM十四讲第5讲阅读过程中总结和记录的学习内容，主要记录针孔相机模型，这是视觉SLAM投影、坐标变换等内容的基础，一定要一次性全搞透。 概述“机器人如何表示自身位姿”属于SLAM经典模型的运动方程部分。“机器人如何观测外部世界”属于观测方程部分，以相机为主的视觉SLAM中，观测主要是指相机成像的过程，这里就涉及到相机的成像原理和成像模型。相机模型中常常涉及到四个坐标系：图像像素坐标系、成像平面坐标系、相机坐标系和世界坐标系。 针孔相机模型 针孔相机模型如上图所示，其中，$O-x-y-x$是相机坐标系，$O$是相机的光心，$f$为相机焦距，$z$轴指向相机的正前方；$O’-x’-y’$是物理成像平面，三维世界中的点$P[X_w,Y_w,Z_w]$，相机坐标系的坐标为$[X_c,Y_c,Z_c]$，经过相机光心投影到物理成像平面上，形成成像点$P’[x,y]$。根据相似三角形原理，有： $\frac{Z_c}{f}=\frac{X_c}{x} =\frac{Y_c}{y} \qquad (1)$ 整理得： $x=f\frac{X_c}{Z_c} ,y=f\frac{Y_c}{Z_c} \qquad (2)$ 像素坐标系图像像素坐标系通常简称为图像坐标系或者像素坐标系。如下图所示： 像素坐标系的平面为相机的成像平面，原点在图像的左上方，$u$轴向右与$x$轴平行，$v$轴向下与$y$轴平行。像素坐标系的单位是像素(pixel)，也就是我们常说的分辨率。 物理成像平面坐标系物理成像平面在距相机光心一倍焦距的平面上，和像素坐标系处于同一平面，原点是相机光轴与成像平面的交点，即成像平面的中点或者叫principal point。单位为物理单位，比如毫米。因此成像平面坐标系和像素坐标系只是原点和度量单位不同，两个坐标系之间相差了一个缩放比例和一个原点的平移。 假设某个像素点$P’$坐标为$($u,v$)$，$P’$对应的成像平面坐标为$($x,y$)$，设像素坐标在$u$轴上缩放了$\alpha$倍，在$v$轴上缩放了$\beta$倍（$\alpha,\beta$单位都为像素/米）。成像平面的原点在像素坐标系中的坐标为($c_x$,$c_y$)，则像素坐标系与成像平面坐标系之间有如下转换公式，用到齐次坐标： $\left{ \begin{array}{ll} u=\alpha x+c_x\v=\beta y+c_y \end{array} \right. \Rightarrow \left[\begin{matrix}u\v\1\end{matrix}\right]=\left[\begin{matrix}\alpha&amp;0&amp;c_x\0&amp;\beta&amp;c_y\0&amp;0&amp;1\end{matrix}\right]\left[\begin{matrix}x\y\1\end{matrix}\right] \qquad (3)$ 相机坐标系相机坐标系的原点是相机光心，$X_c$和$Y_c$轴与像素坐标系$u$轴和$v$轴平行，$Z_c$轴为相机的光轴。光心到像素平面的距离为焦距$f$。 由图可以看出相机坐标系上的点和成像平面坐标系上的点存在透视投影关系。根据相似三角形关系，成像平面坐标系与相机坐标系之间有如下转换关系： $\left{ \begin{array}{ll} x=f\frac{X_c}{Z_c}\y=f\frac{Y_c}{Z_c} \end{array} \right. \Rightarrow Z_c\left[\begin{matrix}x\y\1\end{matrix}\right]=\left[\begin{matrix}f&amp;0&amp;0&amp;0\0&amp;f&amp;0&amp;0\0&amp;0&amp;1&amp;0\end{matrix}\right]\left[\begin{matrix}X_c\Y_c\Z_c\1\end{matrix}\right] \qquad (2)$ 根据(2)(3)两式，将$\alpha f$合并成$f_x$，$\beta f$合并成$f_y$，其中$f$称为物理焦距，单位为米，$f_x,f_y$称为像素焦距，单位为像素，得到： $\left{ \begin{array}{ll} u=f_x \frac{X_c}{Z_c}+c_x\v=f_y \frac{Y_c}{Z_c}+c_y \end{array} \right. \qquad (4)$ 写成矩阵的形式，用到齐次坐标： $\left[\begin{matrix}u\v\1\end{matrix}\right]=\frac1{Z_c}\left[\begin{matrix}\alpha&amp;0&amp;c_x\0&amp;\beta&amp;c_y\0&amp;0&amp;1\end{matrix}\right] \left[\begin{matrix}f&amp;0&amp;0&amp;0\0&amp;f&amp;0&amp;0\0&amp;0&amp;1&amp;0\end{matrix}\right]\left[\begin{matrix}X_c\Y_c\Z_c\1\end{matrix}\right]=\frac1{Z_c}\left[\begin{matrix}f_x&amp;0&amp;c_x&amp;0\0&amp;f_y&amp;c_y&amp;0\0&amp;0&amp;1&amp;0\end{matrix}\right]\left[\begin{matrix}X_c\Y_c\Z_c\1\end{matrix}\right]\triangleq\frac1{Z_c} KP_c \qquad (5)$ 传统习惯上把$Z_c$挪到左侧： $Z_c\left[\begin{matrix}u\v\1\end{matrix}\right]=\left[\begin{matrix}f_x&amp;0&amp;c_x&amp;0\0&amp;f_y&amp;c_y&amp;0\0&amp;0&amp;1&amp;0\end{matrix}\right]\left[\begin{matrix}X_c\Y_c\Z_c\1\end{matrix}\right]\triangleq KP_c \qquad(6)$ 世界坐标系在环境中选择一个参考坐标系来描述相机和物体的位置，该坐标系称为世界坐标系，单位为m。相机坐标系和世界坐标系之间的关系可以用旋转矩阵$R$和平移向量$t$来描述。假设$P$在世界坐标系下的坐标为$(X_w,Y_w,Z_w)$，则相机坐标系与世界坐标系之间有如下转换关系： $\left[\begin{matrix}X_c\Y_c\Z_c\1\end{matrix}\right]=\left[\begin{matrix}R&amp;t\0&amp;1\end{matrix}\right]\left[\begin{matrix}X_w\Y_w\Z_w\1\end{matrix}\right] \qquad (7)$ 坐标系转换 坐标系 单位 备注 世界坐标系 m 描述相机位置 相机坐标系（可化为归一化相机坐标） m 原点为相机光心 成像平面坐标系 mm 原点为图像中点 像素坐标系 pixel 原点为图像左上角 通过上述的四个坐标系可以实现从世界坐标系与像素坐标系之间的转换，如下所示： $Z_c\left[\begin{matrix}u\v\1\end{matrix}\right]=\left[\begin{matrix}\alpha&amp;0&amp;c_x\0&amp;\beta&amp;c_y\0&amp;0&amp;1\end{matrix}\right]\left[\begin{matrix}f&amp;0&amp;0&amp;0\0&amp;f&amp;0&amp;0\0&amp;0&amp;1&amp;0\end{matrix}\right]\left[\begin{matrix}R&amp;t\0&amp;1\end{matrix}\right]\left[\begin{matrix}X_w\Y_w\Z_w\1\end{matrix}\right]=\left[\begin{matrix}f_x&amp;0&amp;c_x&amp;0\0&amp;f_y&amp;c_y&amp;0\0&amp;0&amp;1&amp;0\end{matrix}\right]\left[\begin{matrix}R&amp;t\0&amp;1\end{matrix}\right]\left[\begin{matrix}X_w\Y_w\Z_w\1\end{matrix}\right] \qquad (8)$ 其中$\left[\begin{matrix}f_x&amp;0&amp;c_x&amp;0\0&amp;f_y&amp;c_y&amp;0\0&amp;0&amp;1&amp;0\end{matrix}\right]$称为内参数矩阵$K$，$\left[\begin{matrix}R&amp;t\0&amp;1\end{matrix}\right]$称为外参数矩阵$T$。 上式简写成： $Z_c\left[\begin{matrix}u\v\1\end{matrix}\right]=KT\left[\begin{matrix}X_w\Y_w\Z_w\1\end{matrix}\right] \qquad (9)$ 到此，针孔相机成像模型就搞清楚了。 相机的内参数矩阵往往是已知的并且是固定的，而外参数矩阵在SLAM问题中往往是需要求解的，用于相机的位姿定位。从世界坐标系到像素坐标系之间的转换关系可知，已知世界坐标系下的三维点坐标，只要已知内外参矩阵，就可以求得像素坐标。而如果已知像素坐标，即使已知内外参矩阵，其世界坐标下的三维点也不是唯一确定的，而是空间的一条直线。即单目相机只能测平面信息，而不能获取深度信息。 归一化在坐标变换过程中通常需要进行归一化处理，得到点在相机归一化平面上的投影，归一化平面是假想的。 三维点在相机坐标系下表示形式为： $P_c=\left[\begin{matrix}x_c\y_c\z_c\end{matrix}\right]$ 所以有： $\left[ \begin{matrix}z_cu\z_cv\z_c\end{matrix}\right] = K\left[ \begin{matrix}x_c\y_c\z_c\end{matrix}\right]=KPc$ 由上式可知，三维点直接经过内参得到的坐标相当于$u,v$坐标在各自的方向上都放大了$zc$倍。归一化平面是指位于相机前方z=1处的平面上，该平面称为归一化平面。归一化坐标就相当于在$z$的方向上当z=1时用一个平面截断，这时光心与3D点的连线在该面上的点即为该三维点的归一化点，记做$P{c1}$： $P_{c1}=\left[\begin{matrix}\frac{x_c}{z_c}\\frac{y_c}{z_c}\1\end{matrix}\right]$ 由归一化坐标经过内参矩阵后就得到了像素坐标： $\left[\begin{matrix}u\v\1\end{matrix}\right]=K\left[\begin{matrix}\frac{xc}{z_c}\\frac{y_c}{z_c}\1\end{matrix}\right]=KP{c1}$ 因此可以把像素坐标看成对归一化平面上的点进行量化测量的结果。 畸变由于相机透镜的使用，会引入径向畸变，主要分为桶型畸变和枕型畸变 ；由于相机透镜组装过程的误差，导致透镜与成像平面不能严格平行，会引入切向畸变。严格意义上，还需要对这些畸变进行畸变矫正。 参考资料 视觉SLAM十四讲第5讲 针孔相机模型 计算机视觉：相机成像原理：世界坐标系、相机坐标系、图像坐标系、像素坐标系之间的转换 相机成像模型]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
        <tag>单目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记五-图优化与g2o基础]]></title>
    <url>%2F2018%2F08%2F31%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BA%94-%E5%9B%BE%E4%BC%98%E5%8C%96%E4%B8%8Eg2o%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[这篇文章是阅读视觉SLAM十四讲第6讲非线性优化部分和高翔关于图优化与g2o库的博客过程中总结和记录的内容，属于SLAM系统优化的基础理论内容。 概述SLAM问题的处理方法主要分为滤波和图优化两类。滤波的方法中常见的是扩展卡尔曼滤波、粒子滤波、信息滤波等，这种方法是递增的、实时的处理数据并矫正机器人位姿。比如基于粒子滤波的SLAM的处理思路是假设机器人知道当前时刻的位姿，利用编码器或者IMU之类的惯性导航又能够计算下一时刻的位姿，然而这类传感器有累计误差，所以再将每个粒子的激光传感器数据或者图像特征对比当前建立好的地图中的特征，挑选和地图特征匹配最好的粒子的位姿当做当前位姿，如此往复。当然在gmapping、hector_slam这类算法中，不会如此轻易的使用激光数据，激光测距这么准，当然不能只用来计算粒子权重，而是将激光数据与地图环境进行匹配（scan match）估计机器人位姿，比用编码器之流精度高出很多。 目前SLAM主流研究热点几乎都是基于图优化的，对于处理视觉SLAM，如果用EKF，随着时间推移地图扩大，内存消耗，计算量都很大；而使用图优化计算在高建图精度的前提下效率更高。 关于图优化 阅读参考资料1，通过例子更直观地理解图优化的原理。 图优化是目前视觉SLAM里主流的优化方式。其思想是把一个优化问题表达成图（Graph），以便我们理解、观察。一个图中有很多顶点，以及连接各顶点的边。当它们表示一个优化问题时，顶点是待优化的变量，而边是指误差项。我们把各个边的误差加到一起，就得到了整个优化问题的误差函数。 图优化SLAM问题能够分解成两个任务： 构建图，机器人位姿当做顶点，位姿间关系当做边，这一步常常被成为前端（front-end），往往是传感器信息的堆积。 优化图，调整机器人位姿顶点尽量满足边的约束，这一步称为后端（back-end）。 图优化过程：先堆积数据，机器人位姿为构建的顶点，边是位姿之间的关系，可以是编码器数据计算的位姿，也可以是通过ICP匹配计算出来的位姿，还可以是闭环检测的位姿关系。首先是构建图，得到原始未经优化的地图，构建完成地图后，调整顶点满足边的约束，最后得到优化后的地图。 图的表达下面这些图都是不同的表达形式，前三个是ORB-SLAM2中用到图。（关于图优化和常用的优化库g2o还需要再深入学习） Covisibility Graph共视图，是一个无向有权图（Graph），这个概念最早来自2010的文章[Closing Loops Without Places]。该图中每个顶点就是关键帧，如果两个关键帧有相同的地图点（即它们有共视点），就把这两个顶点连接起来，连接边的权重就是两个关键帧共享的3D点的个数。 Essential Graph为了在优化阶段减小计算量，ORB-SLAM2作者提出了Essential Graph的概念，主要用它来进行全局优化。它是共视图的子集，即Covisibity Graph的最小生成树（MST）。该图中顶点是关键帧，但只连接某个关键帧和与之拥有最多共享的地图点的关键帧。这样能够连接所有的顶点，但是边会减少很多。 Spanning Graph生成树 Pose Graph如果我们对特征点的空间位置并不关心，就可以构建只带有关键帧结点，以及它们之间的边这样的图。由于一个照片中常常有上千个特征点，这样做可以节省许多计算量。 参考资料 graph slam tutorial : 从推导到应用1 graph slam tutorial ：从推导到应用2 graph slam tutorial ：从推导到应用3 【SLAM】（二）Cartographer的原理探究——GraphSLAM理论基础 深入理解图优化与g2o：图优化篇 知乎问题]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
        <tag>图优化</tag>
        <tag>g2o</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析九-ORB特征匹配]]></title>
    <url>%2F2018%2F08%2F30%2FORB-SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%9D-ORB%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录ORB特征匹配有关的内容。 概述ORB_SLAM2系统中，负责特征匹配的类是ORBmatcher，定义在ORBmatcher.h文件中。该类负责特征点与特征点之间，地图点与特征点之间通过投影关系、词袋模型或者Sim3位姿匹配，用来辅助完成单目初始化、三角化恢复新的地图点、Tracking、Relocalization以及Loop Closing等任务，因此比较重要。类中提供了多个接口，这些接口分别负责完成不同的匹配任务，下面进行一一介绍。 主要接口重载的SearchByProjection函数用于追踪局部地图（Tracking）对应函数： 1int SearchByProjection(Frame &amp;F, const std::vector&lt;MapPoint*&gt; &amp;vpMapPoints, const float th=3); 该函数负责在当前帧特征点和投影的地图点之间搜索匹配，具体来说，就是将投影的地图点vpMapPoints（这些地图点是上一帧的局部关键帧对应的所有局部地图点）投影到当前图像帧并搜索匹配，返回匹配到的点对数。具体匹配过程： 对于vpMapPoints中的每个地图点，搜索其在当前图像帧一个窗口内的特征点； 计算该地图点的描述子与窗口范围内的每一个特征点描述子的汉明距离； 找到汉明距离值最小bestDist和次小bestDist2的两个最佳匹配特征点； 如果两个特征点所属的金字塔层不同并且最佳匹配点的汉明距离值bestDist大于mfNNratio与次最佳匹配点的距离值bestDist2的乘积，则认为最佳匹配点就是汉明距离最小的这个特征点；将该地图点设为当前特征点对应的地图点，匹配成功，匹配数+1，继续循环匹配； 否则，继续循环匹配； 继续循环匹配，直到vpMapPoints中的每个地图点都匹配完毕（或匹配到或匹配不到）。 用于从上一图像帧追踪（Tracking）对应函数： 1int SearchByProjection(Frame &amp;CurrentFrame, const Frame &amp;LastFrame, const float th, const bool bMono); 该函数负责将从上一图像帧追踪到的地图点投影到当前帧，并搜索匹配，返回匹配到的点对数量，用于TrackWithMotionModel。具体过程： 建立旋转直方图，用于检测旋转一致性； 分别计算当前帧和前一帧的旋转矩阵和平移矩阵； 通过相机模型，将前一帧的每一个地图点，投影得到在当前帧中的像素坐标； 在一个窗口内搜寻特征点GetFeaturesInArea，窗口尺寸根据尺度（特征点所处金字塔层）进行变化。因为前面ORB是进行了高斯金字塔分层，金字塔越上层的尺寸越小，搜索窗口相应就小； 找到地图点描述子与在窗口中的特征点描述子之间的距离的最佳值（最小值），如果距离小于设定值TH_HIGH，则认为是匹配点对，匹配数+1； 在直方图中记录特征点角度变化（做旋转一致性检测有关的记录，上一帧关键点角度和当前帧关键点角度相减）；直方图形式如下所示，旋转角度除以直方图长度，四舍五入得到横坐标（横坐标范围是0-HISTO_LENGTH），每个横坐标值对应一个vector集合，记录所有对应具有当前角度的匹配点的序号，纵坐标是集合的大小。 循环3，继续匹配； 应用旋转一致性再一次筛选匹配点对（根据角度变化剔除不好的匹配），具体做法是从直方图中选出纵坐标值最大的前三个集合，剔除这三个集合以外的其他匹配点。 用于重定位（Tracking）对应函数： 1int SearchByProjection(Frame &amp;CurrentFrame, KeyFrame* pKF, const std::set&lt;MapPoint*&gt; &amp;sAlreadyFound, const float th, const int ORBdist); 该函数负责将从关键帧中观测到的地图点投影到当前帧，并搜索匹配（投影匹配当前帧的特征点），返回匹配到的点对数量，用于Relocalization。具体过程与上一个函数类似。 用于回环检测（LoopClosing）对应函数： 1int SearchByProjection(KeyFrame* pKF, cv::Mat Scw, const std::vector&lt;MapPoint*&gt; &amp;vpPoints, std::vector&lt;MapPoint*&gt; &amp;vpMatched, int th); 该函数负责使用相似变换投影地图点到当前关键帧，并搜索匹配，返回匹配到的点对数量，用于Loop Closing。 参数 pKF：检测闭环时的当前关键帧 Scw：闭环帧（从候选帧选出的）与当前帧之间的相似变换Scm vpPoints：闭环帧及其相连关键帧对应的地图点 vpMatched：已经找到的地图点（当前关键帧与所有候选关键帧SearchByBoW匹配到的地图点） 具体过程 获取相机标定参数，为之后的投影做准备； 分解相似变换矩阵：先计算得到尺度因子s，然后计算世界坐标系到相机坐标系pKF的旋转矩阵和平移向量（都是去掉尺度因子的）； 在已经找到的匹配点中，去除没有对应匹配点的地图点； 遍历每个地图点，不包括坏的地图点和已经找到的地图点 首先获取到地图点的3D世界坐标系，并通过第2步计算得到的矩阵转换到相机坐标系下，且在Pc下的深度必须为正； 再投影到像素坐标系下，坐标值必须在image范围内； 计算世界坐标系下，该地图点的深度，该深度必须在该点的尺度方差区域内，且观察视角必须小于60°（通过向量内积来判断观察角度是否小于60°）； 根据地图点的深度和关键帧预测一个高斯金字塔层，根据该层计算一个半径，获得该半径范围内的特征点； 找出半径范围内所有特征点的描述子与地图点描述子汉明距离最小的特征点，如果满足最小距离bestDist&lt;=TH_LOW，则认定匹配成功。 重载的SearchByBow函数用于重定位（Tracking）对应函数： 1int SearchByBoW(KeyFrame *pKF, Frame &amp;F, std::vector&lt;MapPoint*&gt; &amp;vpMapPointMatches); 该函数通过BoW对关键帧和当前图像帧的特征点进行匹配，用于TrackReferenceKeyFrame和Relocalization。 用于回环检测（LoopClosing）对应函数： 1int ORBmatcher::SearchByBoW(KeyFrame *pKF1, KeyFrame *pKF2, vector&lt;MapPoint *&gt; &amp;vpMatches12) 该函数通过BoW对两个关键帧（当前关键帧pKF1和候选关键帧之一pKF2）的特征点进行匹配，用于Loop Closing时对两个关键帧间的特征点匹配，返回匹配到的点对数量，将匹配到的特征点对应的地图点记录在vpMatches12中作为输出。 参数 pKF1：当前关键帧 pKF2：候选关键帧中的一个 vpMatches12：匹配到的地图点 具体实现 对四叉树中属于相同节点的特征点才考虑匹配，首先对KF1中某个节点下的特征点，遍历KF2中对应节点的所有特征点，计算描述子距离并记录信息； 根据距离和阈值关系添加匹配，记录角度变化； 根据角度变化剔除误匹配。 重载的Fuse函数主要用于地图点的融合。如果地图点能匹配上当前关键帧的地图点，也就是地图点重合了，选择观测数多的地图点替换；地图点能匹配上当前帧的特征点，但是该特征点还没有生成地图点，则生成新的地图点。重载的第一个函数是为了减小尺度漂移的影响，需要知道当前关键帧的sim3位姿。 使用相似变换的投影对应函数： 1int Fuse(KeyFrame* pKF, cv::Mat Scw, const std::vector&lt;MapPoint*&gt; &amp;vpPoints, float th, vector&lt;MapPoint *&gt; &amp;vpReplacePoint); 该函数用于回环检测线程，负责使用一个给定的相似变换Scw将地图点投影到关键帧，搜索重叠的地图点。具体实现： 分解相似变换矩阵，得到旋转矩阵和平移向量等数据； 对每一个地图点投影到相机系和图像系，检查深度、视角等信息； 由距离预测尺度进而确定搜索半径，在指定区域内获取特征点； 遍历这些特征点，计算其描述子和地图点描述子的汉明距离，计录最好距离和索引； 如果索引对应的地图点不存在则新添加一个，存在的话则标记其将要被替换。 不使用相似变换的投影对应函数： 1int Fuse(KeyFrame* pKF, const vector&lt;MapPoint *&gt; &amp;vpMapPoints, const float th=3.0); 该函数用于局部建图线程，负责将地图点投影到关键帧，搜索重叠的地图点，返回融合的地图点数量。具体实现： 遍历地图点，将其转换至相机坐标系检查深度，投影并检查是否在图像内，检查尺度和视角范围是否合适； 根据深度预测尺度从而确定搜索半径，获得范围内的特征点； 遍历这些点，计算它们和地图点投影后的坐标误差，误差太大的跳过，计算描述子距离，记录距离最小的最佳匹配点； 对于最佳匹配，如果某个点已经有了地图点，则选择观测次数多的那个点，舍弃观测次数少的点，即两点融合；如果没有对应的地图点，则添加地图点，添加观测关系。 SearchForInitialization函数函数原型： 1int SearchForInitialization(Frame &amp;F1, Frame &amp;F2, std::vector&lt;cv::Point2f&gt; &amp;vbPrevMatched, std::vector&lt;int&gt; &amp;vnMatches12, int windowSize=10); 该函数用于单目初始化过程，地图初始化时两个图像帧之间的匹配，只用于单目。具体实现： 构造直方图； 遍历第一帧中的特征点，对于每一个特征点kp1； 首先在第二帧中设置搜索范围（与特征点kp1在同一金字塔层级），获取范围内的特征点； 遍历搜索范围内的点，对于每一个特征点，计算其描述子与kp1描述子的汉明距离，记录相关信息； 根据最好距离和次好距离以及阈值的关系，记录匹配，记录特征点角度变化到直方图； 根据角度变化剔除不好的匹配。 SearchForTriangulation函数函数原型： 12int SearchForTriangulation(KeyFrame *pKF1, KeyFrame* pKF2, cv::Mat F12, std::vector&lt;pair&lt;size_t, size_t&gt; &gt; &amp;vMatchedPairs, const bool bOnlyStereo); 该函数利用三角化，在两个关键帧之间恢复出一些地图点，检测对极约束。具体实现： 对第一个关键帧中的特征点进行遍历，跳过已经有对应地图点的点，因为这个函数的目的是为了三角化进行的搜索匹配； 对于第二个关键帧中的特征点，计算描述子之间的距离，检查特征点距离极线的距离，满足要求的记录信息； 进行角度变化的统计和检查。 SearchBySim3函数函数原型： 1int SearchBySim3(KeyFrame* pKF1, KeyFrame* pKF2, std::vector&lt;MapPoint *&gt; &amp;vpMatches12, const float &amp;s12, const cv::Mat &amp;R12, const cv::Mat &amp;t12, const float th); 该函数在关键帧pKF1和通过相似变换得到的关键帧pKF2观测到的地图点之间搜索匹配，主要是匹配之前漏匹配的点，只用于双目和RGB-D。 通过sim3变换，确定pKF1中的特征点在pKF2中的大致区域，同理，确定pKF2的特征点在pKF1中的大致区域。在该区域内通过描述子进行匹配捕获pKF1和pKF2之前漏匹配的特征点，更新vpMatches12（之前使用SearchByBow进行特征点匹配时会有漏匹配）。最后有一个检查的条件，某一对匹配点在pKF1至pKF2的匹配及pKF2至pKF1的匹配都存在时才认为是有效的匹配。 DescriptorDistance函数对应函数： 1static int DescriptorDistance(const cv::Mat &amp;a, const cv::Mat &amp;b); 该函数负责计算两个ORB描述子的汉明距离。 总结 各种特征匹配的基本思路都是根据距离预测尺度，进而根据尺度确定一个合理的搜索范围，之后对这个范围内的点进行匹配，从匹配结果中选出满足条件的最佳匹配。 何时用投影匹配，何时用DBow2进行匹配？ 在Relocalization和LoopClosing中进行匹配的是在很多帧关键帧集合中匹配，属于Place Recognition，因此需要用DBow。 而投影匹配适用于两帧之间，或者投影范围内（局部地图，前一个关键帧对应地图点）的MapPoints与当前帧之间。 以上函数在检测特征点时，都会用到角度直方图用来剔除不满足两帧之间角度旋转的外点，也就是旋转一致性检测。具体实现： 将关键帧与当前帧匹配关键点的angle相减，得到rot（0≤rot＜360），放入一个直方图中，对于每一对匹配点的角度差，均可以放入一个bin的范围内（360/HISTO_LENGTH）； 统计直方图最高的三个bin保留，其他范围内的匹配点剔除。 参考资料 ORB-SLAM（八）ORBmatcher 特征匹配 ORBSLAM2源码学习（6） ORBmatcher类 ORBmatacher]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记四-单目相机中的三角测量]]></title>
    <url>%2F2018%2F08%2F29%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E5%9B%9B-%E4%B8%89%E8%A7%92%E6%B5%8B%E9%87%8F%2F</url>
    <content type="text"><![CDATA[这篇文章是视觉SLAM十四讲第7讲三角测量部分阅读过程中总结和记录的学习内容，属于单目SLAM的基础理论内容。 概述三角测量/三角化（triangulation）就是估计地图点的深度值。在使用对极几何约束估计了相机运动之后，需要使用相机的运动估计特征点的空间位置。在单目SLAM中，由于我们无法通过单张图像获得相机的深度，因此我们通过三角测量的方法来估计地图点的深度。 三角测量是指：通过两处观察同一个点的夹角，来确定该点的距离。 求解过程 如上图所示，考虑两幅图像中的特征点分别为$p_1$，$p_2$。理论上直线$O_1p_1$和$O_2p_2$在场景中会相交于一个点$P$，该点即是两个特征点所对应的地图点在三维场景中的位置。但是由于噪声的影响，两条直线往往无法相交。假设点$P$在相机坐标系下的坐标为$c_1$和$c_2$，根据针孔相机中的坐标变换公式可知： $s_1p_1=Kc_1$ $s_2p_2=Kc_2$ 其中$s1$是$c1$在$Z$轴上的坐标，$s2$是$c2$在$Z$轴上的坐标，$K$是相机的内参矩阵。 令： $p_1=Kx_1$ $p_2=Kx_2$ 其中$x1$,$x2$称为点$P$在相机坐标系下的归一化坐标。 根据三维空间中的运动模型有： $c_2=Rc_1+t$ $k^{−1}s_1p_1=Rk^{−1}s_2p_2+t$ $s_1x_1=s_2Rx_2+t$ 我们已经知道了$R$，$t$，需要求解的就是两个特征点的深度值$s_1$，$s_2$。先同时左乘$x1$的反对称矩阵可得： $s_1x^\wedge_1x_1=0=s_2x^\wedge_1Rx_2+x^\wedge_1t$ 上述等式的右边是个关于未知变量$s_2$的方程，求解此方程即可得到$s_2$。有了$s_2$，$s_1$也就迎刄而解了。 但由于误差的存在，估计的$R$，$t$不一定精确到使上述方程成立，所以更常见的做法是求最小二乘解而不是零解。 三角测量的矛盾三角化测量的前提是两幅图像之间存在平移。有平移才会由对极几何中的三角形，才谈得上三角化测量。要提高三角化的精度，其一是提高特征点的提取精度，即提高图像分辨率；其二是使平移量增大。如下图所示，当平移很小时，深度估计的不确定性会增大。因此要使深度的精确度增加，则需要增加平移，但是增加平移会导致图像的外观发生明显的变化，使得特征提取和匹配的难度增加，这称为三角测量的矛盾。 参考资料 视觉slam十四讲第7讲 单目相机中的三角化测量 Monocular slam 中的理论基础(2)]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
        <tag>单目SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析八-单目初始化再学习]]></title>
    <url>%2F2018%2F08%2F29%2FORB-SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%AB-%E5%8D%95%E7%9B%AE%E5%88%9D%E5%A7%8B%E5%8C%96%E5%86%8D%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录单目初始化过程中初始化器的工作，该过程通过对极几何方法计算基础矩阵、单应矩阵进而估计相机运动，再利用三角测量计算特征点的空间位置。 概述单目初始化的内容定义在Inlitializer.h头文件中，该文件定义的内容都是用于单目的，因为双目和RGB-D不需要初始化。地图初始化的目的是得到两帧图像的相对运动$R$，$t$，然后根据相机运动和这两帧匹配好的关键点，三角化初始的三维地图。主要分为五个步骤。 主要过程 特征提取与匹配。在当前帧与参考帧之间（金字塔0层上）提取ORB特征，进行匹配，得到像素坐标下的匹配点，如果匹配点对数过少，就重置参考帧，在Tracking::MonocularInitialization中； 并行计算H矩阵和F矩阵。H矩阵用于特征点都在同平面的情况，使用归一化的DLT（直接线性变换）（normalized DLT）方法、四个匹配点进行计算；F矩阵用于特征点不在同一平面的情况，使用归一化8点法。代码中随机的选8个点（当前帧和参考帧的8个匹配好的点对，一共8对），一共挑选200次（迭代次数），选出得分最高的$H$，$F$；得分计算方法如下： 具体解释：$M$可取$F$或$H$。我们以$M=F$为例，首先看到的是，$S_F$是一个累加。因为选了8个点，所以累加8次。$ρ_F$的计算已经给出。最重要的是$d^2$的计算， 是指把第一帧里面的每一个选中的点，重投影到第二帧后得到的重投影误差求平方，以及第二帧里面选中的投影到第一帧里面形成的重投影误差求平方。然后$Γ$ 取5.99，计算出得分。 使用打分机制选择模型。如果场景接近平面，或者视差较小，可以使用单应性矩阵来解释。根据公式得到$R_H$ $RH&gt;0.40$选择$H$，否则选择$F$。选择模型公式如下： 恢复出运动和地图的三维结构（sfm）。利用选择的模型恢复运动和地图点，从$H$或$F$中分解出$R$和$T$，然后根据$R$，$T$三角化匹配好的关键点，得到这些关键点的3D坐标。这就是所谓的初始化地图。分解H矩阵可以恢复出8种姿态，SVD分解E矩阵也可以恢复出4种姿态，通过深度值以及场景的先验信息，一般可以得到唯一满足要求的。但是小视角情况下会出现判断错误的情况出现，因此ORB-SLAM中选择使用这些备选姿态直接三角化出地图点，再通过视角，深度以及重投影误差来判定是否有唯一解，若没有，则放弃，重新回到第一步去初始化。 若初始化成功，则进行GlobalBundleAdjustment。 重要函数 构造函数：给出参考帧和迭代系数。 1Initializer(const Frame &amp;ReferenceFrame, float sigma = 1.0, int iterations = 200); 初始化器：单目初始化调用接口。 12bool Initialize(const Frame &amp;CurrentFrame, const vector&lt;int&gt; &amp;vMatches12, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated); 参数 CurrentFrame：输入的参考帧 vMatches12：输入的匹配 R21：输出的旋转矩阵 t21：输出的平移向量 vP3D：输出的三维地图，即初始化的地图 vbTriangulated：标记一组特征点能否进行三角化 具体实现首先找到参考帧和当前帧的配对关系。把这个一一对应关系放入mvMatches12变量。然后构造一个200行8列的二维数组mvSets。最后开启两个线程计算$H$，$F$。 单应矩阵和基础矩阵获取算法 1void FindHomography(vector&lt;bool&gt; &amp;vbMatchesInliers, float &amp;score, cv::Mat &amp;H21); 1void FindFundamental(vector&lt;bool&gt; &amp;vbInliers, float &amp;score, cv::Mat &amp;F21); 两个函数分别按照设定的规则获取单应矩阵和基础矩阵，它们的三个参数都是输出。 具体实现两个函数实现是类似的。首先将第一帧图像的关键点和第二帧图像的关键点全部normalize。然后，循环200次（默认的迭代次数），每次循环通过ComputeH21（ComputeF21）计算出单应矩阵（基础矩阵），并通过CheckHomography（CheckFundamental）计算它们的得分，找出最高的得分，以及最高得分对应的$H$（$F$）。 单应矩阵和基础矩阵计算函数 1cv::Mat ComputeH21(const vector&lt;cv::Point2f&gt; &amp;vP1, const vector&lt;cv::Point2f&gt; &amp;vP2); 1cv::Mat ComputeF21(const vector&lt;cv::Point2f&gt; &amp;vP1, const vector&lt;cv::Point2f&gt; &amp;vP2); 两个函数的参数都是输入，函数返回值就是$H$（$F$）矩阵。 具体实现根据8个点对，由对极约束或者是$H$的约束，获得$F$或$H$。具体可参考对极几何相关的内容。 单应矩阵和基础矩阵得分计算 1float CheckHomography(const cv::Mat &amp;H21, const cv::Mat &amp;H12, vector&lt;bool&gt; &amp;vbMatchesInliers, float sigma); 1float CheckFundamental(const cv::Mat &amp;F21, vector&lt;bool&gt; &amp;vbMatchesInliers, float sigma); 两个函数输入单应矩阵（基础矩阵），输出相应的得分。 具体实现主要过程2内容的实现。 分解单应矩阵和基础矩阵恢复运动 12bool ReconstructH(vector&lt;bool&gt; &amp;vbMatchesInliers, cv::Mat &amp;H21, cv::Mat &amp;K, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated, float minParallax, int minTriangulated); 12bool ReconstructF(vector&lt;bool&gt; &amp;vbMatchesInliers, cv::Mat &amp;F21, cv::Mat &amp;K, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated, float minParallax, int minTriangulated); 参数 vbMatchesInliers：输入参数 F21：输入参数，计算得到的F矩阵 K：输入参数，代表相机参数 R21：输出的R t21：输出的t vP3D：输出的3D点 vbTriangulated：输出，表示点是不是三角化了 minParallax：输入，最小的视差 minTriangulated：输入，最少需要三角化的点（50） 具体实现因为从$E$，$H$中直接分解出$R$和$T$是不唯一的，会分解出4种（8种）情况，这两个函数就是根据一些手段和先验知识恢复得到正确唯一的$R$和 $t$。其中ReconstructF函数首先通过基础矩阵计算得到本质矩阵，再使用DecomposeE计算$E$，然后完成验证解。 三角化 1void Triangulate(const cv::KeyPoint &amp;kp1, const cv::KeyPoint &amp;kp2, const cv::Mat &amp;P1, const cv::Mat &amp;P2, cv::Mat &amp;x3D); 三角化实现的函数。 参数 kp1：输入的第一帧关键点 kp2：输入与第一帧匹配的第二帧关键点 P1 ：第一帧的相机参数KT P2：第二帧的相机参数KT x3D：输出的三角化的点 正则化 1void Normalize(const vector&lt;cv::KeyPoint&gt; &amp;vKeys, vector&lt;cv::Point2f&gt; &amp;vNormalizedPoints, cv::Mat &amp;T); 参数 vKeys：输入的关键点 vNormalizedPoints：输出的点 T：输出的T，在void FindHomography(vector&lt;bool&gt; &amp;vbMatchesInliers, float &amp;score, cv::Mat &amp;H21);和void FindFundamental(vector&lt;bool&gt; &amp;vbInliers, float &amp;score, cv::Mat &amp;F21);函数里面用到。 具体实现首次算出输入的所有关键点的平均值meanX，meanY（关键点x，y的平均值）； 然后，每个关键点的x，y与这个平均值meanX，meanY做差，将所有得到的差值的绝对值加起来求平均值，得到meanDevX，meanDevY； 将上面那些差值除以meanDevXm，meanDevY，就得到输出的vNormalizedPoints。 检测视差、深度、重投影误差 12345int CheckRT(const cv::Mat &amp;R, const cv::Mat &amp;t, const vector&lt;cv::KeyPoint&gt; &amp;vKeys1, const vector&lt;cv::KeyPoint&gt; &amp;vKeys2, const vector&lt;Match&gt; &amp;vMatches12, vector&lt;bool&gt; &amp;vbInliers, const cv::Mat &amp;K, vector&lt;cv::Point3f&gt; &amp;vP3D, float th2, vector&lt;bool&gt; &amp;vbGood, float &amp;parallax); 参数 R：输入 1-&gt;2 t ：输入 1-&gt;2 vKeys1：输入 vKeys2：输入 vMatches12：输入 vbInliers：输入 K：相机参数输入 vP3D：输出 th2 ：输入（重投影误差的平方误差） vbGood ：输出 parallax ：输出视差 具体实现首先将第一个相机放在原点，构造两个3X4的矩阵，该矩阵等于相机参数$K$乘以该帧的$T$，得到第二个相机中心在第一个相机坐标系（可以看成世界坐标系）下的坐标；然后进入一个循环，循环每次都要计算视差；最后检测视差；分别检测两帧下的深度和重投影误差；返回3D点；返回视差。 关于视差首先计算匹配好的点在世界坐标系下（第一个相机坐标系下）的3D坐标，从第一个相机中心出发连接这个3D点得到一条射线，从第二个相机的中心出发连接这个3D点得到一条射线，这两条射线之间的夹角就是视差(parallax)。 本质矩阵分解 1void Initializer::DecomposeE(const cv::Mat &amp;E, cv::Mat &amp;R1, cv::Mat &amp;R2, cv::Mat &amp;t) 该函数实现本质矩阵$E$的分解。 参考资料 ORB-SLAM2详解（三）自动地图初始化 ORB-SLAM2学习4 initializer.h ORB-SLAM （四）Initializer单目初始化]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记三-单目相机中的对极几何]]></title>
    <url>%2F2018%2F08%2F24%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%89-%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%2F</url>
    <content type="text"><![CDATA[这篇文章是视觉SLAM十四讲第7讲对极几何部分阅读过程中总结和记录的学习内容，属于单目SLAM的基础理论内容。 概述在特征提取和匹配任务完成后，我们希望根据匹配的点对估计相机的运动。但是由于相机原理的不同需要采取不同的方法： 对于单目相机，只知道2D的像素坐标，所以问题是根据两组2D点估计相机运动。该问题用对极几何解决。 对于双目、RGB-D相机，或经某种方法得到了距离信息，问题就可以根据两组3D点估计运动。该问题用ICP解决。 如果有3D点及其在相机的投影位置，也能估计相机的运动。该问题用PnP求解。 在ORB_SLAM2系统中，单目初始化过程会使用专门的初始化器完成由两帧图像完成的初始化，该过程使用先后两帧满足关键点数条件的图像帧，使用对极几何约束方法进行求解，得到基础矩阵和单应矩阵，估计出相机的位姿；接着会使用三角测量，计算图像帧中特征点的空间位置。再往后的处理过程，在LocalMapping线程也会使用对极约束、三角测量生成新的地图点。因此，对极约束和三角测量在ORB_SLAM2系统中有着举足轻重的地位，有必要深入学习一下。 对极约束假设得到了一堆匹配好的特征点，如下图所示。如果由若干对（具体多少呢？）这样的匹配点，就可以通过这些二维图像点的对应关系，恢复出在两帧之间相机的运动（位姿）。 在第一帧（左）图像的坐标系下，设$P$的空间位置为： $P=[X,Y,Z]^T$ 根据针孔相机模型可以得到两个像素点（特征点）$p_1$，$p_2$的像素位置为： $s_1p_1=KP$，$s_2p_2=K(RP+t)$ 其中$K$为相机内参矩阵，$R$，$t$为两个坐标系的相机运动，$s1$和$s2$分别表示两个像素点的深度。使用齐次坐标，可以把上式写成在乘以非零常数下成立的等式： $p_1=KP$，$p_2=K(RP+t)$ 在2D-2D问题中，我们只有像素坐标$p_1$和$p_2$，$P$是未知的，那该怎么办？ 此时可以引入一个归一化坐标的概念。取： $x_1=K^{-1}p_1$，$x_2=K^{-1}p_2$ 其中$x_1$，$x_2$是两个像素点的归一化平面上的坐标。带入上式，得： $x_2=Rx_1+t$ 两边同时左乘$t^\wedge$，根据$^\wedge$的定义，相当于两侧同时与$t$做外积： $t^\wedge x_2=t^\wedge Rx_1$ 两侧同时左乘$x^T_2$： $x^T_2t^\wedge x_2=x^T_2t^\wedge Rx_1$ 等式左侧$t^\wedge x_2$是一个与$t$和$x_2$都垂直的向量，再和$x_2$做内积时，将得到0。因此，有如下简洁的式子： $x^T_2t^\wedge Rx_1=0$ 重新代入$p_1$，$p_2$，有： $p^T_2K^{-T}t^\wedge RK^{-1}p_1=0$ 上述两个式子都称为对极约束，其几何意义是$O_1$，$P_1$，$O_2$三者共面。对极约束中同时包含了平移和旋转。把中间部分分别记作两个矩阵：基础矩阵（Fundamental Matrix）$F$和本质矩阵（Essential Matrix）$E$： $E=t^\wedge R$，$F=K^{-T}EK^{-1}$ 化简得到简化的对极约束： $x^T_2Ex_1=p^T_2Fp_1=0$ 对极约束简洁地给出了两个匹配点的空间位置关系。于是，相机位姿估计问题变为以下两步： 根据配对点的像素位置求出$E$或者$F$； 根据$E$或者$F$求出$R$，$t$。 由于$E$和$F$相差了相机内参，而内参在SLAM中通常是已知的，所以实践中往往使用形式更加简单的$E$，当然了ORB_SLAM2中使用的是基础矩阵和单应矩阵。 本质矩阵$E$求解本质矩阵$E=t^\wedge R$，它是一个$3×3$的矩阵，内有9个未知数。从$E$的构造方式看，它由几个重要的特性： 本质矩阵是由对极约束定义的。由于对极约束是等式为0的约束，所以对$E$乘以任何非零常数后，对极约束依然满足。这一点被称为E在不同尺度下是等价的。 根据$E=t^\wedge R$，可以证明，本质矩阵$E$的奇异值必定式$[\sigma, \sigma,0]^T$。这称为本质矩阵的内在性质。可以理解为：一个$3×3$的矩阵是本征矩阵的充要条件是对它奇异值分解后，它有两个相等的奇异值，并且第三个奇异值为0。 由于平移和旋转各有3个自由度，所以$t^\wedge R$共有6个自由度。但由于尺度等价性，故$E$实际上只有5个自由度。 现在，考虑一对匹配点的像素坐标$x_1$为$[u_1,v_1,1]$，$x_2$为$[u_2,v_2,1]$，则根据对极约束有： $\left[ \begin{matrix}u_1,v_1,1\end{matrix} \right]\left[ \begin{matrix}e_1 &amp; e_2 &amp;e_3\ e_4 &amp; e_5 &amp; e_6 \e_7&amp;e_8&amp;e_9 \end{matrix} \right]\left [\begin{matrix}u_2,v_2,1\\end{matrix} \right]=0$ 另把矩阵$E$展开，写成向量的形式： $e=\left[ \begin{matrix}e_1,e_2,…,e_9\end{matrix} \right]^T$ 对极约束可以写成与$e$有关的线性形式： $\left[ \begin{matrix}u_1u_2,u_1v_2,u_1,v_1u_2,v_1v_2,v_1,u_2,v_2,1\end{matrix} \right]·e=0$ 对于这个线性方程，存在尺度等价性，即对$e$乘以任何常数，等式仍然成立。因此即使$e$有9个未知变量，只需要8个方程，构成线程方程组即可对$e$进行求解。这就是求解本质矩阵最经典的八点法。至此，我们求得了本质矩阵$E$。接下来的问题是如何根据已知估得的本质矩阵，恢复出相机的运动$R$，$t$。这个过程由奇异值分解（SVD）完成。设$E$的SVD分解为： $E=UΣV^T$ 根据$U$，$V$为正交阵，$Σ$为奇异值矩阵，即$Σ=diag(\sigma, \sigma,0)$。在SVD分解中，对于任意一个$E$，存在两个可能的$t$，$R$与它对应： $t^\wedge_1=UR_Z(\frac{\pi}2)ΣU^T,R_1=UR^T_Z(\frac{\pi}2)V^T$ $t^\wedge_2=UR_Z(-\frac{\pi}2)ΣU^T,R_2=UR^T_Z(-\frac{\pi}2)V^T$ 其中$R_Z(\frac{\pi}2)$表示绕$Z$轴旋转90度得到的旋转矩阵。由于$-E$,$E$等价，所以对任意一个$t$取负号，也会得到同样的结果。因此，从$E$分解到$t$,$R$时，一共存在4个可能的解。如下图所示。 幸运的是，只有一种解中$P$在两个相机中都具有正的深度。因此，只要把任意一点代入四种解中，检测该点在两个相机下的深度，就可以确定哪个解是正确的。用用 利用本质矩阵的内在性质，它只有5个自由度，所以最小可以通过5对点来求解相机运动。然而这种做法形式复杂，从工程实现角度考虑，平时通常会有几十对甚至上百对匹配点，从8对减到5对意义并不明显。 因为一些误差的存在，通过8点法计算得到的本质矩阵$E$可能不会严格满足本质矩阵的特性2，因此通常会对奇异值矩阵$Σ$做调整。通常的做法是，对八点法求得的$E$进行SVD分解后，会得到奇异值矩阵$Σ=diag(σ1,σ2,σ3)$。不妨设$σ1≥σ2≥σ3$，取： $ E=Udiag(\frac{σ1+σ2}2,\frac{σ1+σ2}2,0)V^T $ 这相当于是把求出来的矩阵投影到了$E$所在的流形上。由于$E$具有尺度等价性，因此更简单的做法是将奇异值矩阵取为$diag(1,1,0)$。 单应矩阵$H$除了基本矩阵和本质矩阵T，还有一种单应矩阵$H$(Homography)，它描述了两个平面之间的映射关系。若场景中的特征点都落同一个平面上（比如墙，地面等），则可以通过单应性来进行运动估计。 单应矩阵通常描述处于共同平面上的一些点（3D点共面）在两张图像之间的变换关系。 考虑两帧图像上匹配到的特征点$p_1$，$p_2$，这些特征点落在平面$P$上，设这个平面满足方程： $n^TP+d=0$ 则： $-\frac{n^TP}d=1$ 推理可得： $p_2=K(R−\frac{tn^T}d)K^{−1}p_1$ 这里就得到了一个直接描述图像坐标$p_1$，$p_2$之间的变换，把中间这部分记作$H$（3×3矩阵），则有： $p_2=Hp_1$ 单应矩阵的定义与旋转、平移以及平面的参数有关。对上式展开可得： $\left[ \begin{matrix}u_2\v_2\1\end{matrix} \right]=\left[ \begin{matrix}h_1 &amp; h_2 &amp;h_3\ h_4 &amp; h_5 &amp; h_6 \h_7&amp;h_8&amp;h_9 \end{matrix} \right]\left [\begin{matrix}u_1\v_1\1\\end{matrix} \right]$ 转换为： $u_2=\frac{h_1v_1+h_2v_1+h_3}{h_7v_1+h_8v_1+h_9}$ $v_2=\frac{h_4v_1+h_5v_1+h_6}{h_7v_1+h_8v_1+h_9}$ 这样一组匹配点就可以构造2个方程。因为像素坐标为齐次坐标，所以单应矩阵乘以任意常数项，等式仍然成立。因此即使单应矩阵的未知变量为9个，但其自由度为8。因此需要4组匹配点，构造8个方程，来对单应矩阵进行求解。与本质矩阵相似，求出单应矩阵后需要对其进行分解，才可以得到相应的旋转矩阵$R$和平移向量$t$。 单应性在SLAM中具有重要意义。当特征点共面时或相机发生纯旋转时，基础矩阵的自由度下降，这就出现了所谓的退化。这时候继续用8点法求解基础矩阵会导致受到噪声的影响增加。为了避免这种情况，通常会同时估计基础矩阵$F$和单应矩阵$H$，选择重投影误差比较小的那个作为最终的运动估计矩阵。 讨论—对极约束方法的局限性尺度不确定性首先由于E具有尺度等价性，因此它分解得到的$t$和$R$也有一个尺度等价性。而$R∈SO(3)$自身具有约束，因此可以认为$t$也具有尺度等价性。$t$的尺度等价性直接导致$t$的大小与现实世界中平移的大小并不是对应的。也就是说在单目SLAM中，每次都要确定$t$的尺度，通常的做法是对$t$进行归一化。这称为单目SLAM的初始化。初始化之后就可以用3D-2D来计算相机运动，初始化之后的轨迹和地图的单位，就是初始化时固定的尺度。 如何理解上述最后一句话表达的过程？ 答：参见总结。 如何理解“$t$的尺度等价性”和“初始化时固定尺度”？ 答：参见参考资料3后面的内容。 初始化的纯旋转问题即使可以用单应矩阵来处理纯旋转的情况，但是由于纯旋转缺少位移，因此无法对空间点进行三角化。也就是说对于单目SLAM来说，初始化必须要有位移。 多于8对点的情况最后我们提到用8点法求解本质矩阵。但是实际中往往匹配点会远远多于8对，此时有两种求解思路。 当匹配正确率比较高的时候，可以构造最小二乘。当匹配错误率比较高的时候，可以用随机采样一致性（RANSAC）求解。 总结由于对极几何方法需要使用8个或者8个以上的点对，并且存在上述需要初始化、纯旋转和尺度不确定性等问题。如果两帧图像中其中一帧图像特征点的3D位置已知，则最少可以使用3对点对（需要至少一个额外点验证结果）就可以估计相机运动。特征点的3D位置可以由三角化或者RGB-D相机的深度图确定。因此： 双目或RGB-D的视觉里程计中，可以直接使用PnP估计相机运动。 单目视觉里程计中，必须先进行初始化（这样就有了3D点），然后才能使用PnP。 参考资料 视觉SLAM十四讲第7讲 单目相机中的对极几何 Monocular slam 的理论基础(1) 视觉slam十四讲学习笔记]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
        <tag>单目SLAM</tag>
        <tag>对极几何</tag>
        <tag>2D-2D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析七-LocalClosing]]></title>
    <url>%2F2018%2F08%2F23%2FORB-SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83-LocalClosing%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录LocalClosing模块部分。 概述LocalClosing模块是SLAM系统非常重要的一部分，由于VO过程存在累计误差（漂移），LocalClosing模块主要任务是检测闭环，即确定是否曾到达过此处，并在找到闭环之后计算Sim3变换，进行关键帧位姿、地图点的优化（后端优化），可以将这个累计误差缩小到一个可接受的范围内。闭环检测模块使用LocalMapping模块传送过来的关键帧，插入关键帧的操作是在LocalMapping线程主循环剔除冗余关键帧之后进行的，执行语句mpLoopCloser-&gt;InsertKeyFrame(mpCurrentKeyFrame)。 具体操作 检测闭环对应LoopClosing::DetectLoop()函数。检测回环候选（在KeyFrameDataBase中找到与mlpLoopKeyFrameQueue相似的闭环候选帧）并检查共视关系（检查候选帧连续性），函数会将所有通过一致性检测的候选关键帧统一放到集合mvpEnoughConsistentCandidates中，由下一步最终确定闭环帧。主要过程如下： 计算当前帧mpCurrentKF和所有与当前帧有共视关系的关键帧的Bow得分，得到最小得分minScore ； 根据这个最小得分minScore 从mpKeyFrameDB（关键帧库）中找出候选的关键帧集合vpCandidateKFs； 使用KeyFrameDatabase::DetectLoopCandidates完成候选帧的筛选，具体过程： Bow得分&gt;minScore； 统计满足1的关键帧中有共同单词最多的单词数maxcommonwords； 筛选出共同单词数大于mincommons(=0.8maxcommons)的关键帧； 相连的关键帧分为一组，计算组得分（总分），得到最大总分bestAccScore，筛选出总分大于minScoreToRetain(=0.75*bestAccScore)的组，用组中得分最高的候选帧lAccScoreAndMatch代表该组。计算组得分的目的是剔除单独一帧得分较高，但是没有共视关键帧，作为闭环来说不够鲁棒。 ​ 对于vpCandidateKFs里面的每一个关键帧，作为当前关键帧。找出其有共视关系的关键帧组成一个当前集合spCandidateGroup。如果当前关键帧是vpCandidateKFs中第一帧的话，直接把这个spCandidateGroup集合，以分数0直接放到mvConsistentGroups中。如果不是第一帧的话，就从mvConsistentGroups中依次取出里面的元素pair&lt;set&lt;KeyFrame*&gt;,int&gt;的first，这些元素也是关键帧们组成以前集合sPreviousGroup。只要是当前集合中的任意一个关键帧能在以前集合里面找到，就要将当前集合的得分加1，并把当前集合放到mvConsistentGroups里面。如果当前集合的得分大于3（mnCovisibilityConsistencyTh）的话，当前帧就通过了一致性检测，把当前帧放到mvpEnoughConsistentCandidates，最后会找到很多候选的关键帧。下一步用sim3找出闭环帧。 注意该函数改变的就是mvpEnoughConsistentCandidates，mvpEnoughConsistentCandidates作为该函数的输出。 计算Sim3对应LoopClosing::ComputeSim3()函数。该函数计算相似变换，从mvpEnoughConsistentCandidates中找出真正的闭环帧mpMatchedKF。主要过程如下： 通过SearchByBow，搜索当前关键帧中和候选帧匹配的地图点，当匹配的数目超过20，就为该候选帧和当前帧构造一个sim3Solver，即相似求解器； 用相似求解器Sim3Solver求解出候选帧与当前帧之间的相似变换Scm，这里使用RANSAC方法； 根据计算出的位姿，通过相似变换求找出更多的匹配地图点（SearchBySim3），更新vpMapPointMatches； 使用更新后的匹配，使用g2o优化Sim3位姿（OptimizeSim3），优化的结果足够好的话（内点数nInliers&gt;20）就把候选的关键帧作为当前帧的闭环帧mpMatchedKF，break，跳过对其他候选闭环帧的判断，同时也得到了mScw； 获取mpMatchedKF及其相连关键帧对应的地图点。调用SearchByProjection将这些地图点通过上面优化得到的mScw变换投影（重投影）到当前关键帧进行匹配，若匹配点&gt;=40个，则返回ture，进行闭环调整；否则，返回false，线程暂停5ms后继续检查Tracking发送来的关键帧队列。 注意SearchByProjection得到的当前关键帧中匹配上闭环关键帧共视地图点（mvpCurrentMatchedPoints），将用于后面CorrectLoop时当前关键帧地图点的冲突融合。 至此，完成第4、5操作后，不仅确保了当前关键帧与闭环帧之间匹配度高，而且保证了闭环帧的共视图中的地图点和当前关键帧的特征点匹配度更高（20—-&gt;40），说明该闭环帧是正确的。 闭环矫正对应LoopClosing::CorrectLoop()函数，完成回环地图点融合和位姿图优化。具体来说，在回环检测到之后，对当前帧（回环帧）周围的帧的影响、回边的连接以及地图点做出的相应的改变。闭环矫正时，LocalMapper和Global BA必须停止。注意Global BA使用的是单独的线程。主要过程如下： 使用计算出的当前帧的相似变换Sim3，即mScw，对当前位姿进行调整，并且传播到当前关键帧相连的关键帧（相连关键帧之间相对位姿是知道的，通过当前关键帧的Sim3计算相连关键帧的Sim3）； 经过上一步处理，回环两侧的关键帧完成对齐，然后利用调整过的位姿更新这些相连关键帧对应的地图点（修正关键帧看到的地图点）； 将闭环帧及其相连帧的地图点都投影到当前帧以及相连帧上，投影匹配上的和Sim3计算过的地图点进行融合（就是替换成质量高的）； 涉及融合的关键帧还需要更新其在共视地图中的观测边关系，因为找到了闭环，需要在covisibility里面增加闭环边（不止一条边）。这是为了剥离出因为闭环产生的新的连接关系LoopConnections，用于优化Essential Graph。添加当前帧与闭环匹配帧之间的边，该边不参与优化； 然后进行EssentialGraph优化（只是优化一些主要关键帧）； 新开一个线程进行全局优化，优化所有的位姿与MapPoints。 参考资料 ORB-SLAM2学习7 LocalClosing.h ORB-SLAM（十）LoopClosing ORB-SLAM（六）回环检测 ORBSlam2学习研究(Code analysis)-ORBSlam2中的闭环检测和后端优化LoopClosing 单目slam LoopClosing之Sim3优化]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析六-LocalMapping]]></title>
    <url>%2F2018%2F08%2F22%2FORB-SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%AD-LocalMapping%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录LocalMapping模块部分。 概述LocalMapping模块作用是将Tracking中送来的关键帧放在mlNewKeyFrame列表中；处理新关键帧、地图点检查剔除、生成新地图点、Local BA、关键帧剔除。主要工作在于维护地图，也就是SLAM中的Map。 前面的记录中提到，Tracking模块会将满足一定条件的图像帧加入关键帧，但是其实ORB-SLAM中关键帧的加入是比较密集的，这样确保了定位的精度。同时LocalMapping线程会对关键帧进行剔除，确保了关键帧的数量不会无限增加，不会对large scale的场景造成计算负担。Tracking模块会判断是否需要将当前帧创建为关键帧，对地图中的关键帧、地图点具体的处理，包括如何加入、如何删除的工作是在LocalMapping线程完成的。论文中的图： 具体操作 处理新关键帧对应LocalMapping::ProcessNewKeyFrame()函数。处理新关键帧与局部地图点之间的关系。具体步骤如下： 获取关键帧。从mlNewKeyFrame队列（待插入的关键帧）中弹出队首的关键帧做为当前帧，并计算该关键帧的BoW，后面三角化恢复地图点有用； 获取关键帧中与关键点关联的地图点。将TrackLocalMap中跟踪局部地图匹配上的地图点关联到当前关键帧（在Tracking线程中只是利用关键点的匹配关系进行位姿计算，优化当前关键帧姿态）。具体来说，可以分两种情况，地图点没有关联到关键帧，则完成关联（AddObservation），更新地图点normal和描述子，这种地图点是在追踪过程创建并被关键帧关联的，它们在UpdateLastFrame过程就已经有了空间位置；否则记录该地图点为最新添加（这些地图点是在创建关键帧时创建的，已经关联到关键帧），加入mlpRecentAddedMapPoints； 更新共视图连接关系。使用UpdateConnections函数更新加入当前关键帧之后关键帧之间的连接关系，包括更新Covisibility图和Essential图（最小生成树spanning tree）； 关键帧插入地图。 地图点剔除对应LocalMapping::MapPointCulling()函数。任务是对上一函数获取到的最新加入的局部地图点mlpRecentAddedMapPoints进行检查，每个地图点要被保留，在该地图点被创建后的三个关键帧里必须要经过严格的测试，这样保证其能被正确的跟踪和三角化。满足如下条件之一就被剔除： 该地图点是坏点，直接从检查列表去掉； 跟踪（匹配上）到该地图点的普通帧帧数（IncreaseFound）&lt;应该观测到该地图点的普通帧数量（25%*IncreaseVisible），即比值mnFound/mnVisible&lt;0.25，设置为坏点，并从检查列表去掉。比值低说明这样的地图点该地图点虽在视野范围内，但很少被普通帧检测到； 从添加该地图点的关键帧算起，当前关键帧至少是第三个添加该地图点的关键帧的条件下，看到该地图点的帧数&lt;=2（双目和RGBD模式是帧数&lt;=3），设置为坏点，并从检查列表去掉；因此在地图点刚建立的阶段，要求比较严格，很容易被剔除；而且单目的要求更严格，需要三帧都看到； 若从添加该地图点的关键帧算起，一共有了大于三个关键帧，还存在列表中，则说明该地图点是高质量的，从检查列表中去掉。 一旦经过了这样比较严格的筛选，地图点只有在观测到它的关键帧&lt;3时才会被剔除，这主要发生在关键帧被剔除（90%以上匹配点可以被其他帧检测到）；或者局部捆集优化时，将地图点归为外点从观测中剔除了的情况。因此地图点中的外点是比较少的，所以整套ORB-SLAM中除了重定位和闭环很少去使用RANSAC。 ORB-SLAM中关键帧和地图点的加入和删除秉承的是送入严出的标准，因此在提高了定位建图准确性的前提下又很好地限制了计算量，可以用于large scale的场景。 生成新地图点（三角化方法）对应LocalMapping::CreateNewMapPoints()函数。任务是根据当前关键帧恢复出一些新的地图点，不包括和当前关键帧匹配的局部地图点（已经在ProcessNewKeyFrame中处理，单目模式除了初始化过程会生成地图点外，其它地图点都在这里生成）。具体步骤如下： 找出与当前帧有共视关系的10个（单目模式是20个）关键帧，准备使用对极约束搜索匹配并进行三角化，循环进行以下操作，注意是在当前帧和每一帧共视关键帧之间进行； 检测基线是否过短； 计算基础矩阵F； 搜索满足对极约束的匹配； 通过各种条件判断是不是要三角化这些地图点； 完成对匹配结果的三角化，创建新的地图点； 为新的地图点进行一系列配置。 跟论文里面提到的一样。1、三角化的地图点在两帧都有正深度。2、视差、在每个帧的投影误差、尺度一致性都有被检测。三角化过程还要再深入学习。 临近关键帧搜索更多匹配对应LocalMapping::SearchInNeighbors()函数。如果关键帧队列中还有新的关键帧，则进行该操作，即在临近的关键帧中搜索到更多的匹配，更新并融合当前关键帧以及两级相连（共视关键帧及其共视关键帧）的关键帧的地图点。 Local Bundle Adjustment对应Optimizer::LocalBundleAdjustment()函数。这里优化的是当前帧，以及与当前帧在covisibility graph里面有连接关系的那些关键帧的位姿，以及这些帧看到的地图点，其实就是对局部地图进行优化。详细分析参见优化部分博客。 冗余关键帧剔除对应LocalMapping::KeyFrameCulling()函数。候选的pKF是LocalMapping中当前处理的关键帧的共视关键帧，不包括第一帧关键帧与当前关键帧。如果一个关键帧检测到的90%的地图点，在其他不少于三个具有相同或更精确尺度的关键帧里面都被检测到，就认定该关键帧冗余（该关键帧的存在提供的地图点观测信息有限），并剔除。 参考资料 ORB_SLAM(九) LocalMapping ORB_SLAM2学习6 LocalMapping]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2学习之源码分析五-一个变量引发的探索]]></title>
    <url>%2F2018%2F08%2F20%2FORB-SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%94-%E4%B8%80%E4%B8%AA%E5%8F%98%E9%87%8F%E5%BC%95%E5%8F%91%E7%9A%84%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录KeyFrame::mvuRight有关的内容。 问题的引出+点击查看 123456789101112void MapPoint::AddObservation(KeyFrame* pKF, size_t idx)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); if(mObservations.count(pKF))//count是map映射，使用key查找元素 return; mObservations[pKF]=idx;//idx是当前地图点在该pKF中对应的关键点的编号 if(pKF-&gt;mvuRight[idx]&gt;=0)//mvuRight？？ nObs+=2; else nObs++;//观测到该地图点的关键帧数目&#125; 在阅读MapPoint::AddObservation函数时，注意到mvuRight这个变量，程序中通过判断其值对nObs变量进行相关操作。一直没搞明白它的意思。终于在知乎这个问题找到了答案。这里稍微整理一下内容，并记录下自己由此理解的一些东西。 在构造关键帧时，地图点和关键帧之间的观测关系是很重要的一个点，参考关键帧是哪一帧，该地图点被哪些关键帧观测到，对应的哪些（idx）特征点，都是通过一下两个成员变量维护： 1234//观测到该地图点的关键帧集合std::map&lt;KeyFrame*,size_t&gt; mObservations;// 观测到该地图点的关键帧数int nObs; 通过MapPoint::AddObservation()函数添加地图点观测，即将能够观测到同一个地图点的关键帧（它们之间存在共视关系）加入到该地图点的mObservations集合中。函数中对于mvuRight的使用其实是RGB-D和双目模式使用到的”双目信息“之一。 分析首先需要知道的是，RGBD虽然具有深度信息，但是深度图和RGB图不是完全匹配，其中还是有无深度值的区域，中间也存在空洞。在ORB_SLAM优化过程中，无深度信息的特征点是一条射线，就视作Mono模式一同处理；有深度信息的特征点，视作与Stereo模式一同处理。 具体说来，为了让RGB-D模式与Mono和Stereo模式保持一致，在创建图像帧时，会调用Frame::ComputeStereoFromRGBD函数，如果有深度就设置mvuRight和mvDepth的对应值，下面一行代码就是设置虚拟右图像上的 u 坐标值，即mvuRight。 1mvuRight[i] = kpU.pt.x-mbf/d; 对应公式： $u_R=u_L-\frac{bf}{d}$ 其中$f$是$u$方向上的焦距（即$f_x$），$d$是左图像上的深度，$b$是左右图像基线，$\frac{bf}{d}$是视差（disparity）。 生成 Frame 之后就是调用 Tracking::Track()函数，通过特征匹配和局部地图追踪进行初步位姿估计。首先，初始化 Tracking::StereoInitialization()只会使用那些有深度的 Stereo 点。初始化完成之后，不论是 Tracking::TrackWithMotionModel()与上一普通帧匹配，还是 Tracking::TrackReferenceKeyFrame()与上一关键帧匹配，或是Tracking::Relocalization()与窗口中所有的关键帧匹配，或者Tracking::TrackLocalMap局部地图追踪，都会调用函数 Optimizer::PoseOptimization()函数，进行优化。此函数中，对于Mono模式会使用g2o::EdgeSE3ProjectXYZOnlyPose，对于Stereo模式会使用g2o::EdgeStereoSE3ProjectXYZOnlyPose，具体的代码部分如下。 12345678910111213141516171819202122232425int Optimizer::PoseOptimization(Frame *pFrame)&#123; //... for(int i=0; i&lt;N; i++) &#123; MapPoint* pMP = pFrame-&gt;mvpMapPoints[i]; if(pMP) &#123; // Monocular observation 单目模式观测到的地图点 if(pFrame-&gt;mvuRight[i]&lt;0) &#123; //... g2o::EdgeSE3ProjectXYZOnlyPose* e = new g2o::EdgeSE3ProjectXYZOnlyPose(); //... &#125; else // Stereo observation 双目观测到的地图点 &#123; //... g2o::EdgeStereoSE3ProjectXYZOnlyPose* e = new g2o::EdgeStereoSE3ProjectXYZOnlyPose(); //... &#125; &#125; &#125; //...&#125; 上述两个函数中有computeError()函数，分别返回Vector2d、Vector3d，具体代码如下： 1234567Vector2d EdgeSE3ProjectXYZ::cam_project(const Vector3d &amp; trans_xyz) const&#123; Vector2d proj = project2d(trans_xyz); Vector2d res; res[0] = proj[0]*fx + cx; res[1] = proj[1]*fy + cy; return res;&#125; 12345678Vector3d EdgeStereoSE3ProjectXYZ::cam_project(const Vector3d &amp; trans_xyz, const float &amp;bf) const&#123; const float invz = 1.0f/trans_xyz[2]; Vector3d res; res[0] = trans_xyz[0]*invz*fx + cx; res[1] = trans_xyz[1]*invz*fy + cy; res[2] = res[0] - bf*invz; return res;&#125; res[0]、res[1] 是$u_L$、$v_L$，res[2] 是$u_R=u_L-\frac{bf}{d}$。 延伸思考正如在源码分析二-初始化中所述，SLAM系统可以接收各种传感器的图像，不管是双目、单目还是RGB-D，都会将原始图像封装成图像帧，为图像帧的各种信息赋值（使用原始图像进行ORB特征提取在这个过程中完成），之后原始图像就被丢弃，之后的处理过程和原始图像没有关系了。创建图像帧之后进行初始化，再之后的处理过程，除了优化过程（Optimizer::PoseOptimization）外，不再区分单目、双目模式（真是这样吗？？？？）。 参考资料 ORB-SLAM六MapPoint与Map ORB-SLAM的RGBD为什么要分Mono和Stereo?]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析四-ORB特征及其提取]]></title>
    <url>%2F2018%2F08%2F19%2FORB-SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%9B%9B-ORB%E7%89%B9%E5%BE%81%E5%8F%8A%E5%85%B6%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录ORB特征的简单原理和提取有关的内容。 ORB特征提取（涉及到Fast关键点、rBRIEF描述子、SIFT特征）一种直观的提取特征的方式就是在不同图像之间辨认角点，确认它们的对应关系，角点就是所谓的特征。但是单纯的角点无法满足需求，为此设计的许多更加稳定的局部图像特征，如著名的SIFT、SURF、ORB，相比于朴素的角点这些人工设计的特征点具备可重复性、可区分性、高效率、本地性等优点。特征点由关键点（Key-point）和描述子（Descriptor）组成。如当提及SIFT特征时，是指“提取SIFT关键点，计算SIFT描述子”。 ORB特征包括Oriented FAST关键点和rBRIEF描述子，是在FAST关键点和BRIEF描述描述子基础上改进而来。 改进FAST关键点（它没有描述子）：FAST关键点不具有尺度不变性，ORB特征提取方法中，通过构建高斯金字塔，然后在每一层金字塔图像上检测角点，nlevels幅不同比例的图像提取特征点总和作为这幅图像的oFAST（FAST Keypoint Orientation，改进后的FAST关键点）关键点，来实现尺度不变性；FAST关键点也不具有旋转不变性，ORB特征提出使用矩（moment）法（灰度质心法）来确定FAST关键点的方向，即通过矩来计算特征点（该图像块的几何中心）以r为半径范围内的质心（该图像块的灰度质心），特征点坐标到质心形成一个向量作为该关键点的方向。 改进BRIEF描述子：首先进行旋转不变性改进，加入旋转因子得到steered BRIEF；然后改进特征点描述子的相关性（即描述子可区分性，对误匹配率影响较大），得到rBRIEF特征描述子。 在SLAM视觉里程计中的应用—特征点法 ORB_SLAM中的特征提取SLAM初始化过程，首先需要创建图像帧，其关键一步就是对图像进行ORB特征提取，调用函数ExtractORB()，其内部调用了ORBextractor类中的重载操作符void operator()，完成特征提取，提取结果被保存在Frame类的成员变量std::vector&lt;cv:KeyPoint&gt; mvKeys和cv:Mat mDescriptors中，即提取出特征的关键点和描述子。操作符重载的函数如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162void ORBextractor::operator()( InputArray _image, InputArray _mask, vector&lt;KeyPoint&gt;&amp; _keypoints, OutputArray _descriptors)&#123; if(_image.empty()) return; Mat image = _image.getMat(); assert(image.type() == CV_8UC1 ); // Pre-compute the scale pyramid ComputePyramid(image);//计算图像尺度金字塔 vector &lt; vector&lt;KeyPoint&gt; &gt; allKeypoints; ComputeKeyPointsOctTree(allKeypoints);//提取图像关键点 并保存在八叉树 //ComputeKeyPointsOld(allKeypoints); Mat descriptors; int nkeypoints = 0; for (int level = 0; level &lt; nlevels; ++level) nkeypoints += (int)allKeypoints[level].size(); if( nkeypoints == 0 ) _descriptors.release(); else &#123; _descriptors.create(nkeypoints, 32, CV_8U); descriptors = _descriptors.getMat(); &#125; _keypoints.clear(); _keypoints.reserve(nkeypoints); //计算每个关键点对应的描述子 int offset = 0; for (int level = 0; level &lt; nlevels; ++level) &#123; vector&lt;KeyPoint&gt;&amp; keypoints = allKeypoints[level]; int nkeypointsLevel = (int)keypoints.size(); if(nkeypointsLevel==0) continue; // preprocess the resized image 进行高斯模糊，用BORDER_REFLECT_101方法处理边缘 Mat workingMat = mvImagePyramid[level].clone(); GaussianBlur(workingMat, workingMat, Size(7, 7), 2, 2, BORDER_REFLECT_101); // Compute the descriptors 计算描述子 Mat desc = descriptors.rowRange(offset, offset + nkeypointsLevel); computeDescriptors(workingMat, keypoints, desc, pattern); offset += nkeypointsLevel; // Scale keypoint coordinates if (level != 0) &#123; float scale = mvScaleFactor[level]; //getScale(level, firstLevel, scaleFactor); for (vector&lt;KeyPoint&gt;::iterator keypoint = keypoints.begin(), keypointEnd = keypoints.end(); keypoint != keypointEnd; ++keypoint) keypoint-&gt;pt *= scale; &#125; // And add the keypoints to the output _keypoints.insert(_keypoints.end(), keypoints.begin(), keypoints.end()); &#125;&#125; ORB特征提取主要过程 构建图像尺度金字塔（构造过程另作详细记录） 提取ORB关键点、生成八叉树并保存关键点 计算每个关键点对应的描述子 提取ORB特征时，每一帧图像共提取1000个特征点，分布在金字塔8层中，层间尺度比例1.2，计算下来金字塔0层大约有217个特征点，7层大约有50个特征点。 同时，为了提取出的特征点能够在图像中分布比较均匀（实际情况中，特征点通常分布得比较集中，这样不利于进行匹配，也不利于精确地求解相机间的位姿从而得到精确的VO轨迹），使用了八叉树（其实是平面上的四叉树）的数据结构来存储提取出的特征点。 这部分内容在ORBextractor.h和ORBextractor.cc中，代码详细理解可以参考一起学ORBSLAM2ORB特征点提取这篇文章和参考资料11学习，有时间再详细学习。 参考资料（有待详细阅读） ORB特征提取详解 ORB特征点检测（墙裂推荐，作者博客里有很多图像特征检测相关的介绍，包括斑点、角点检测和SIFT特征、SURF特征、BRIEF特征描述子等） 视觉SLAM十四讲P132-特征点法（推荐，关于特征点的内容比较清晰整洁） Fast源码分析 Fast API ORBextractor特征提取 SIFT：Distinctive image features from scale-invariant keypoints SURF：Surf: Speededup robust features ORB：Orb: an efficient alternative to sift or surf Brief：Brief: Binary robust independent elementary features 尺度金字塔(Scale pyramid)构建 层数(ScaleLevels)：金字塔层数，金字塔中包含的不同尺度的图像层数 尺度因子(ScaleFactor)：金字塔层与层图像之间的尺度参数，缩放比例 ORBextractor::ComputePyramid就是根据尺度因子对图像进行缩放处理。 供学习参考资料 尺度空间理论 OpenCV 图像金字塔 数字图像处理9—尺度空间]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析三-优化]]></title>
    <url>%2F2018%2F08%2F18%2FORB_SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89-%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录位姿优化有关的内容。 概述因为摄像机标定（camera calibration）和追踪（tracking）的精度不够。摄像机标定的误差会体现在重建中（比如三角法重建时），而追踪的误差则会体现在不同关键帧之间的位姿中和重建中（单目）。误差的不断累积会导致后面帧的位姿离实际位姿越来越远，最终会限制系统整体的精度。 关于摄像机标定单目SLAM文献中一般假设摄像机标定的结果是准确的，并不考虑这个因素带来的误差（大概因为很多时候跑标准的数据集，认为摄像机标定的误差是相似的）。然而对于一个产品，不同类型的传感器对应的标定误差并不相同，甚至有可能差异很大。因此，如果要评估整个系统的精度，这方面的误差必须要考虑进去。 关于追踪无论在单目、双目还是RGBD中，追踪得到的位姿都是有误差的。单目SLAM中，如果两帧之间有足够的对应点，那么既可以直接得到两帧之间的位姿（像初始化中那样），也可以通过求解一个优化问题得到（如solvePnP）。由于单目中尺度的不确定性，还会引入尺度的误差（尺度漂移scale-drift）。由于tracking得到的总是相对位姿，前面某一帧的误差会一直传递到后面去，导致tracking到最后位姿误差有可能非常大。为了提高tracking的精度，可以1. 在局部和全局优化位姿；2. 利用闭环检测（loop closure）来优化位姿。 优化方法在SLAM问题中，优化的目标函数常见的几种约束条件为： 三维点到二维特征的映射关系（通过投影矩阵）； 位姿和位姿之间的变换关系（通过三维刚体变换）； 二维特征到二维特征的匹配关系（通过F矩阵）； 其它关系（比如单目中有相似变换关系）。 关于相似变换Sim3相似变换比欧式变换多了一个自由度，有7个自由度，它允许物体进行均匀缩放（体积比不变），其矩阵表示为： $T_s=Sim3=\left[ \begin{matrix}sR &amp;t\ 0^T &amp; 1 \end{matrix} \right]$ 式中旋转部分多了一个缩放因子$s$，表示我们在对向量旋转之后，可以在x，y，z三个坐标上进行均匀缩放。由于含有缩放，相似变换不再保持图形的面积不变。 如果我们能够知道其中的某些关系是准确的，那么可以在g2o中定义这样的关系及其对应的残差，通过不断迭代优化位姿来逐步减小残差和，从而达到优化位姿的目标。下面介绍ORB-SLAM2系统中主要的优化函数，它们是定义在Optimizer.h文件中的静态函数，可以直接通过类名访问，而不必创建类的对象。ORB中使用的这些优化函数是非常重要的，在视觉SLAM中有很强的通用性，自己实现的时候完全可以参考其实现方法。 局部优化对应函数1void Optimizer::LocalBundleAdjustment(KeyFrame *pKF, bool* pbStopFlag, Map* pMap) 函数中优化求解器初始化过程如下： 123456789g2o::SparseOptimizer optimizer;g2o::BlockSolver_6_3::LinearSolverType * linearSolver;linearSolver = new g2o::LinearSolverEigen&lt;g2o::BlockSolver_6_3::PoseMatrixType&gt;();g2o::BlockSolver_6_3 * solver_ptr = new g2o::BlockSolver_6_3(linearSolver);g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg(solver_ptr);optimizer.setAlgorithm(solver); 具体实现用于LocalMapping线程中，在剔除关键帧之前进行的局部地图优化。当新的关键帧加入到convisibility graph时，作者在关键帧附近进行一次局部优化，如下图所示。Pos3是新加入的关键帧，其初始估计位姿已经得到。此时，Pos2是和Pos3相连的关键帧，X2是Pos3看到的三维点，X1是Pos2看到的三维点，这些都属于局部信息，共同参与Bundle Adjustment。同时，Pos1也可以看到X1，但它和Pos3没有直接的联系，属于Pos3关联的局部信息，参与Bundle Adjustment，但取值保持不变。Pos0和X0不参与Bundle Adjustment。 因此，参与优化的是下图中红色椭圆圈出的部分，其中红色代表取值会被优化，灰色代表取值保持不变。(u,v)是X在Pos下的二维投影点，即X在Pos下的测量（measurement）。优化的目标是最小重投影误差。 全局优化对应函数123void Optimizer::GlobalBundleAdjustemnt(Map* pMap, int nIterations, bool* pbStopFlag, const unsigned long nLoopKF, const bool bRobust)//调用 void Optimizer::BundleAdjustment(const vector&lt;KeyFrame *&gt; &amp;vpKFs, const vector&lt;MapPoint *&gt; &amp;vpMP, int nIterations, bool* pbStopFlag, const unsigned long nLoopKF, const bool bRobust) 函数中优化求解器初始化过程如下： 12345678910g2o::SparseOptimizer optimizer;// solver for BA/3D SLAMg2o::BlockSolver_6_3::LinearSolverType * linearSolver;// 线性方程求解器 使用的求解库为EigenlinearSolver = new g2o::LinearSolverEigen&lt;g2o::BlockSolver_6_3::PoseMatrixType&gt;();// 稀疏矩阵块求解器g2o::BlockSolver_6_3 * solver_ptr = new g2o::BlockSolver_6_3(linearSolver);// 梯度下降算法g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg(solver_ptr);optimizer.setAlgorithm(solver); 具体实现用于单目初始化的CreateInitialMapMonocular函数以及闭环优化的RunGlobalBundleAdjustment函数（在闭环结束前新开一个线程，进行全局优化，在此之前会OptimizeEssentialGraph，论文中说其实这里全局优化提升的精度有限，所以其实可以考虑不使用全局优化）。在全局优化中，所有的关键帧（除了第一帧）和三维点都参与优化。 CorrectLoop ### 闭环处的Sim3位姿优化 #### 对应函数 1int Optimizer::OptimizeSim3(KeyFrame *pKF1, KeyFrame *pKF2, vector&lt;MapPoint *&gt; &amp;vpMatches1, g2o::Sim3 &amp;g2oS12, const float th2, const bool bFixScale) 如果DetectLoop检测到回环，则调用ComputeSim3计算相似变换矩阵，再进行下一步的闭环调整。计算相似变换矩阵过程中，在用RANSAC求解过Sim3以及通过Sim3匹配更多的地图点后，对当前关键帧、闭环关键帧以及匹配的地图点进行优化，以获得更准确的Sim3位姿。获得当前关键帧相对于闭环关键帧的Sim3，然后传播到相连关键帧，并调整地图点，从而完成闭环调整。函数中优化求解器初始化过程如下： 12345678910g2o::SparseOptimizer optimizer;//variable size solverg2o::BlockSolverX::LinearSolverType * linearSolver;// 线性方程求解器 使用的求解库为DenselinearSolver = new g2o::LinearSolverDense&lt;g2o::BlockSolverX::PoseMatrixType&gt;();// 稀疏矩阵块求解器g2o::BlockSolverX * solver_ptr = new g2o::BlockSolverX(linearSolver);// 梯度下降算法g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg(solver_ptr);optimizer.setAlgorithm(solver); 具体实现当检测到闭环时，闭环连接的两个关键帧的位姿需要通过Sim3优化（以使得其尺度一致）。优化求解两帧之间的相似变换矩阵，使得二维对应点（feature）的投影误差最小。如下图所示，Pos6和Pos2为一个可能的闭环。通过$(u{4,2},v{4,2})$和$(u{4,6},v{4,6})$之间的投影误差来优化$S_{6,2}$。 Sim3上的位姿优化对应函数1234void Optimizer::OptimizeEssentialGraph(Map* pMap, KeyFrame* pLoopKF, KeyFrame* pCurKF,const LoopClosing::KeyFrameAndPose &amp;NonCorrectedSim3,const LoopClosing::KeyFrameAndPose &amp;CorrectedSim3,const map&lt;KeyFrame *, set&lt;KeyFrame *&gt; &gt; &amp;LoopConnections, const bool &amp;bFixScale) EssentialGraph包括所有的关键帧顶点，但是优化边大大减少，包括spanning tree（生成树）和共视权重θ&gt;100的边，以及闭环连接边。用于闭环检测Sim3调整后，闭环调整CorrectLoop过程中的优化。函数中优化求解器初始化过程如下： 123456789g2o::SparseOptimizer optimizer;optimizer.setVerbose(false);g2o::BlockSolver_7_3::LinearSolverType * linearSolver =new g2o::LinearSolverEigen&lt;g2o::BlockSolver_7_3::PoseMatrixType&gt;();g2o::BlockSolver_7_3 * solver_ptr= new g2o::BlockSolver_7_3(linearSolver);g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg(solver_ptr);solver-&gt;setUserLambdaInit(1e-16);optimizer.setAlgorithm(solver); 具体实现单目SLAM一般都会发生尺度（scale）漂移，因此Sim3上的优化是必要的。相对于SE3，Sim3的自由度要多一个，而且优化的目标是矫正尺度因子，因此优化并没有加入更多的变量（如三维点）。作者在检测到闭环时在Sim3上对所有的位姿进行一次优化。定义Sim3上的残差如下： $e{i,j}=log{Sim3}(S{ij}S{jw}S^{−1}_{iw})$ 其中$S{iw}$的初值是尺度为1的Pos i相对于世界坐标系的变换矩阵。$S{i,j}$为Pos i和Pos j之间的（Sim3优化之前的）相对位姿矩阵，表示$S{iw}$和$S{j,w}$之间的测量（measurement）。此处相当于认为局部的相对位姿是准确的，而全局位姿有累计误差，是不准确的。 位姿优化对应函数1int Optimizer::PoseOptimization(Frame *pFrame) 具体实现 只优化当前帧pose，地图点固定。 用于LocalTracking中运动模型跟踪、参考帧跟踪、地图跟踪TrackLocalMap、重定位，每进行过一次PnP投影操作将地图点投影到当前平面上之后，都会进行一次PoseOptimization位姿优化，通过BA优化重投影误差。 ORB-SLAM2中的图参考：知乎问题 Covisibility Graph共视图，是一个无向有权图（Graph），这个概念最早来自2010的文章[Closing Loops Without Places]。该图中每个顶点就是关键帧，如果两个关键帧有相同的地图点（即它们有共视点），就把这两个顶点连接起来，连接边的权重就是两个关键帧共享的3D点的个数。 Essential Graph为了在优化阶段减小计算量，ORB-SLAM2作者提出了Essential Graph的概念，主要用它来进行全局优化。它是共视图的子集，即Covisibity Graph的最小生成树（MST）。该图中顶点是关键帧，但只连接某个关键帧和与之拥有最多共享的地图点的关键帧。这样能够连接所有的顶点，但是边会减少很多。 Spanning Graph生成树 参考资料 ORB-SLAM（五）优化 ORB-SLAM（十二）优化 ORBSlam2的位姿优化算法]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析二-初始化]]></title>
    <url>%2F2018%2F08%2F16%2FORB-SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%8C-%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录单目、双目、RGB-D初始化过程，并进行比较。 概述SLAM过程初始化的目的是创建3d地图点，为后续跟踪提供初值。其中单目初始化较为复杂，双目、RGB-D初始化类似。 图像帧创建SLAM系统可以接收各种传感器的图像，不管是双目、单目还是RGB-D，都会将原始图像封装成帧，帧中包含所有SLAM过程需要的信息，包括矫正前后的关键点及数量、特征描述子、右图成像坐标信息（单目除外）、深度信息（单目除外）、时间戳、词袋、内参矩阵、畸变参数、相机位姿、旋转矩阵、平移向量、尺度金字塔信息、参考关键帧等等，通过原始图像获取到帧所需信息之后，原始图像就被丢弃，之后的处理过程和原始图像没有关系了。 在进行初始化之前都要将彩色图像（3或4通道图像）处理成灰度图像（无论图片是RGB、BGR， 还是RGBA、BGRA，均转化为灰度图，放弃彩色信息），继而将图片封装成帧（Frame类对象）。下面介绍下图像帧的创建过程，三者初始化分别调用重载的Frame类构造函数，重载的构造函数都有ORB特征提取、畸变矫正等环节。重载函数传入的参数有差别，这一点很显然。值得注意的是函数内部构造过程的一些区别，比如双目初始化需要进行双目匹配ComputeStereoMatches、RGB-D初始化需要进行双目信息计算ComputeStereoFromRGBD，这一点涉及到mvuRight、mvDepth这两个变量，详细的理解另作记录。 单目创建图像帧函数调用1mCurrentFrame = Frame(mImGray, timestamp, mpIniORBextractor, mpORBextractor, mpORBVocabulary, mK, mDistCoef, mbf, mThDepth); 主要过程设置尺度金字塔信息、ORB特征提取、畸变矫正、设置无双目信息（设置mvuRight、mvDepth两个变量为负）、关键点分布到网格。 双目创建图像帧函数调用1mCurrentFrame = Frame(mImGray, imGrayRight, timestamp, mpORBextractorLeft, mpORBextractorRight, mpORBVocabulary,mK, mDistCoef, mbf, mThDepth); 主要过程设置尺度金字塔信息、ORB特征提取、畸变矫正、双目匹配（计算左图中特征点对应的右图坐标，并恢复出的深度信息。设置mvuRight、mvDepth两个变量为负）、计算基线、关键点分布到网格。 RGB-D创建图像帧函数调用1mCurrentFrame = Frame(mImGray, imDepth, timestamp, mpORBextractorLeft, mpORBVocabulary, mK, mDistCoef, mbf, mThDepth); 主要过程设置尺度金字塔信息、ORB特征提取、畸变矫正、根据RGB-D计算双目信息（设置mvuRight、mvDepth两个变量）、关键点分布到网格。 ORB特征提取1234567void Frame::ExtractORB(int flag, const cv::Mat &amp;im)&#123; if(flag==0) (*mpORBextractorLeft)(im,cv::Mat(),mvKeys,mDescriptors); else (*mpORBextractorRight)(im,cv::Mat(),mvKeysRight,mDescriptorsRight);&#125; 创建图像帧的关键一步是进行ORB特征提取，ExtractORB()调用了ORBextractor类中的重载操作符void operator()，完成特征提取，提取结果被保存在Frame类的成员变量std::vector&lt;cv:KeyPoint&gt; mvKeys和cv:Mat mDescriptors中，即提取出特征的关键点和描述子。关于ORB特征及其提取、匹配，会在后续继续学习。 单目初始化单目SLAM地图初始化的目标是构建初始的三维点云。由于不能仅仅从单帧得到深度信息，因此需要从图像序列中选取两帧以上的图像，估计摄像机姿态并重建出初始的三维点云。单目初始化调用Tracking::MonocularInitialization()函数。 重点分析 需要获取连续两帧特征点数量超过100的图像帧，并且两帧图像匹配点大于100，才可以开始初始化，否则重新接收数据帧；连续两帧的前一帧设为参考帧； 找到连续可用的图像帧后，需要使用专门的初始化器进行初始化（Initializer.cc），这个通过并行计算分解单应矩阵H和基础矩阵F，得到帧间运动（位姿）（关于对极几何方法，使用并行计算F和H矩阵的初始化后续再做专门的记录）； 创建初始地图过程。首先创建关键帧和地图点，通过将关键帧和地图点插入初始地图完成初始地图的构建，接着更新关键帧之间的连接关系（以共视地图点的数量作为权重），对两帧姿态图像进行全局优化重投影误差（和回环检测调整后的大回环优化使用的是同一函数）； 比较重要的三个对象：地图、地图点、关键帧。地图就是整个的位姿和地图点。一个关键帧提取出的特征点对应一个地图点集，因此需要记下每个地图点在该帧中的编号；一个地图点会被多帧关键帧观测到，需要记下每个关键帧在该点中的编号。因此，地图点和关键帧的关系是：每个地图点会记录观测到自身的关键帧，关键帧中会记录观测到的所有地图点； 创建地图点时，地图点中需要加入的一些属性：观测到该地图点的关键帧（以及对应的特征点）；该地图点的描述子（观测到该地图点的多个特征点中（对应多个关键帧），挑选出区分度最高的描述子，作为这个地图点的描述子）； 该MapPoint的平均观测方向和观测距离的，为后面做描述子融合做准备； 局部地图、局部关键帧、局部地图点，是为了进行局部Bundle Adjustment。 单目初始化特点 单目通过一帧无法估计深度，所以初始化时需要使用两帧图像； 需要使用专门的初始化器进行初始化，使用对极约束几何方法恢复运动，得到Rcw、tcw； 恢复运动之后使用三角化测量方法得到特征点的空间位置； 由于通过分解基础矩阵$E$恢复相机运动，得到$R$，$t$，如果相机发生的纯旋转，导致$t$为0，得到的$E$也将为0，无法进一步求解$R$。虽然可以依靠单应矩阵$H$求解旋转，但仅有旋转无法使用三角测量方法估计特征点的空间位置；因此，单目初始化不能只有纯旋转，必须要有一定程度的平移（平移太小会使得位姿求解和三角化结果不稳定，从而导致失败）。 为了让单目成功初始化（单目的初始化需要通过平移运动归一化尺度因子），初始化Tracking时mpIniORBextractor提取的特征点数量设定为普通帧的2倍（Tracking.cc）。 双目、RGB-D初始化双目和RGB-D相机不需要通过两个相邻帧来恢复地图点深度，所以初始化过程极其相似，只要当前到来帧满足条件即可开始初始化，调用的是同一个函数Tracking::StereoInitialization()。 重点分析 创建初始地图过程。首先创建关键帧和地图点，注意这里只会使用有深度的点进行初始化；然后通过将关键帧和地图点插入初始地图完成初始地图的构建。因为此初始化只用到一个图像帧，所以没有关键帧连接关系的更新和姿态的优化； 其他同单目4、5、6条。 疑问：初始化过程中，满足条件的第一个图像帧作为参考关键帧，后来帧都以该参考关键帧为参考吗？即参考关键帧会更新吗？ 答：后续的追踪过程还会有新的关键帧的插入（在Tracking::CreateNewKeyFrame()中完成），最新插入的关键帧作为参考关键帧的。即参考关键帧会更新，总是当前帧最临近的关键帧，或者说上一个关键帧。 初始化比较单目初始化的特点是双目、RGD-D初始化过程不具备的。（废话么这不是()&gt; _ &lt;) ）单目初始化一定要有一定程度的平移。值得关注的一点是，在三者初始化之前图像帧的创建过程也是有区别的，单目不涉及深度和右图特征点成像平面坐标的计算，后两者需要此过程，这一点涉及到mvuRight、mvDepth这两个变量，详细的理解另作记录。 区分图像帧相关 上一帧mLastFrame：即上一帧图像 上一关键帧mpLastKeyFrame：最邻近当前帧的关键帧，不一定是上一帧，因为图像帧要经过判断后，满足条件才能称为关键帧 参考关键帧mpReferenceKF：就是当前帧的上一关键帧；若创建了新的关键帧，参考关键帧就更新为新创建的关键帧；或者是局部地图中与当前帧共享地图点最多的关键帧。 参考关键帧更新的时机 创建新的关键帧时（初始化、追踪过程两处） TrackLocalMap()更新局部关键帧的过程中 当前帧的参考关键帧：或者是上一关键帧，或者是由当前帧创建的关键帧（即当前帧满足关键帧创建的条件），或者是局部地图中与当前帧共享地图点最多的关键帧。 地图相关 全局地图mpMap 是Map类的对象，本质上也是由关键帧和地图点组成。 局部地图 局部地图不像全局地图一样有Map类表示，它只是一个概念叫法，其作用是在局部地图追踪过程中通过地图点重投影匹配关键点，进一步优化初步估计的当前帧位姿（是否还有其他作用？？？）。实际上它包括三个变量： 参考关键帧mpReferenceKF 局部关键帧集合mvpLocalKeyFrames 局部地图点集合mvpLocalMapPoints 局部建图线程接口指针mpLocalMapper 主要是和局部建图线程建立连接，给该线程传递关键帧并由其维护地图，也就是全局地图。 地图点相关地图点有两个私有成员变量mnVisible、mnfound，前者表示该地图点在图像帧视野范围内，在创建地图点时构造函数就将该值置为1；后者表示该地图点有对应特征点的图像帧帧数。通常来说，found的地图点一定是visible的，但是visible的地图点很可能not found。 参考资料 ORB-SLAM 代码笔记（四）tracking代码结构 单目、双目和RGB-D视觉SLAM初始化比较 视觉SLAM十四讲P152]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之源码分析一-追踪]]></title>
    <url>%2F2018%2F08%2F12%2FORB_SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%80-%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ORB_SLAM2系统源码分析的内容，主要记录Tracking模块部分。 ORB-SLAM2系统追踪、局部建图、回环检测、可视化四个线程，其中追踪模块是在主线程中完成的，SLAM视觉里程计主体就是在该线程中完成的。先介绍追踪模块的算法内容，这里从已经完成模块初始化开始。ORB_SLAM2中，重定位和闭环检测过程主要使用DBoW2来完成。 Tracking代码分析程序分为两种模式：SLAM模式和Localization模式，由变量mbOnlyTracking标记。SLAM模式中，三个线程全部都在工作，即在定位也在建图。而Localization模式中，只有Tracking线程在工作，即只定位，输出追踪结果（姿态），不会更新地图和关键帧。Localization模式主要用于已经有场景地图的情况下（在SLAM模式下完成建图后可以无缝切换到Localization模式）。Localization模式下追踪方法涉及到的关键函数是一样的，只是策略有所不同。 上图是参考资料1中的流程图，介绍的比较详细，可供参考。 初始追踪初始化完成后，对于相机获取当前图像mCurrentFrame，通过跟踪匹配上一帧mLastFrame特征点的方式，可以获取一个相机位姿的初始值；为了兼顾计算量和跟踪鲁棒性，追踪部分主要用了三种模型：运动模型（TrackWithMotionModel）、关键帧（TrackReferenceKeyFrame）和重定位（Relocalization）。三种跟踪模型都是为了获取相机位姿一个粗略的初值，后面会通过跟踪局部地图TrackLocalMap对位姿进行BundleAdjustment（捆集调整），进一步优化位姿。 TrackWithMotionModel 该模型根据两帧之间的约束关系来求解估算位姿。假设物体处于匀速运动，那么可以用上一帧的位姿和速度来估计当前帧的位姿。上一帧的速度可以通过前面几帧的位姿计算得到。这个模型适用于运动速度和方向比较一致、没有大转动的情形，比如匀速运动的汽车、机器人、人等。如果是静止状态或者运动模型匹配失效（运用恒速模型后反投影发现LastFrame的地图点和CurrentFrame的特征点匹配很少），通过增大参考帧的地图点反投影匹配范围，获取较多匹配后，计算当前位姿；而对于运动比较随意的目标，上述操作失效的情况下，就会用到下面两个模型。 TrackReferenceKeyFrame 假如motion model已经失效，那么首先可以尝试和最近一个关键帧（即参考关键帧）去做匹配。毕竟当前帧和上一个关键帧的距离还不是很远。作者利用了bag of words（BoW）来加速匹配。首先，计算当前帧的BoW，并设定初始位姿为上一帧的位姿；其次，根据位姿和BoW词典来寻找特征匹配（参见ORB－SLAM（六）回环检测）；最后，利用匹配的特征优化位姿（参见ORB－SLAM（五）优化）。 Relocalization 使用DBoW2实现。假如当前帧与最近邻关键帧的匹配也失败了，意味着此时当前帧已经丢了，无法确定其真实位置。此时，只有去和所有关键帧匹配，看能否找到合适的位置。 重定位的过程大概是这样的： 计算当前帧的特征BoW向量； 利用BoW词典，根据词袋模型的特征匹配度，在关键帧数据库中找到与当前图像帧相似的候选关键帧，使用KeyFrameDatabase::DetectRelocalizationCandidates（注意这里是参考普通图像帧（当前图像帧）寻找候选关键帧，与回环检测过程不同，回环检测使用参考关键帧去寻找闭环候选帧，使用KeyFrameDatabase::DetectLoopCandidates，所以两种情况选取候选关键帧的策略不同）； 通过BoW匹配当前帧和每一个候选关键帧，如果匹配数足够，进行EPnP求解； 对求解结果使用BA优化，如果内点较少，则反投影当前帧的地图点到候选关键帧获取额外的匹配点；若这样依然不够，放弃该候选关键帧，若足够，则将通过反投影获取的额外地图点加入，再进行优化。 如果内点满足要求(&gt;50)则成功重定位，将最新重定位的id更新：mnLastRelocFrameId = mCurrentFrame.mnId; 否则返回false。 TrackLocalMap一旦我们通过上面三种模型获取了初始的相机位姿和初始的特征匹配，就可以将完整的地图（地图点）投影到当前帧中，去搜索更多的匹配。但是投影完整的地图，在large scale的场景中是很耗计算而且也没有必要的，因此，这里使用了局部地图LocalMap来进行投影匹配。对局部地图的更新包括对局部关键帧和局部地图点的更新。局部地图包含：与当前帧相连的关键帧K1（所有能观察到当前帧对应地图点的关键帧，图中Pos2），以及与K1相连的关键帧K2（一级二级相连关键帧，图中Pos1），并且限制了关键数量不超过80；K1、K2对应的地图点（图中X1，貌似X0不包括在内，为啥？？）；参考关键帧Kf。下图局部地图就是红色椭圆圈出的部分，参与局部优化，其中红色代表取值会被优化，灰色代表取值保持不变。 匹配过程如下： 抛弃投影范围超出相机画面的； 抛弃观测视角和地图点平均观测方向相差60o以上的； 抛弃特征点的尺度和地图点的尺度（通过高斯金字塔层数表示）不匹配的； 计算当前帧中特征点的尺度； 将地图点的描述子和当前帧ORB特征的描述子匹配，需要根据地图点尺度在初始位姿获取的粗略x投影位置附近搜索； 根据所有匹配点进行PoseOptimization优化。 位姿优化姿态优化部分的主要思路是在当前帧和（局部）地图之间寻找尽可能多的对应关系，来优化当前帧的位姿。实际程序中，作者选取了非常多的关键帧和地图点。在跑Euroc数据集MH_01_easy时，几乎有一半以上的关键帧和地图点（后期&gt;3000个）会在这一步被选中。然而，每一帧中只有200~300个地图点可以在当前帧上找到特征匹配点。这一步保证了非关键帧姿态估计的精度和鲁棒性。 其他操作在处理完mCurrentFrame的跟踪定位后，需要更新motion model，并判断当前帧是否是新的关键帧。 新的关键帧的创建以下条件必须同时满足，才可以加入关键帧，但是其实ORB-SLAM中关键帧的加入是比较密集的，这样确保了定位的精度，同时在LocalMapping线程最后会进行关键帧的剔除，又确保了关键帧的数量不会无限增加，不会对large scale的场景造成计算负担。但是，ORB的作者又提到了，Tracking中除了提取特征点，TrackLocalMap也挺耗时，可以通过减少关键帧的数量来降低Local Map的规模，提高Tracking速度（但是精确度可能降低）。 距离上一次重定位距离至少20帧； 局部地图线程空闲，或者距离上一次加入关键帧过去了20帧；如果需要关键帧插入（过了20帧）而LocalMapping线程忙，则发送信号给LocalMapping线程，停止局部地图优化，使得新的关键帧可以被及时处理（20帧，大概过去了不到1s）； 当前帧跟踪至少50个点；确保了跟踪定位的精确度； 当前帧跟踪到LocalMap中参考帧的地图点数量少于90%；确保关键帧之间有明显的视觉变化。 如果满足了创建关键帧的条件，在Tracking::CreateNewKeyFrame()函数中完成关键帧创建，将关键帧传递到LocalMapping线程（mpLocalMapper-&gt;InsertKeyFrame(pKF)），再由LocalMapping完成其他工作。此外，在创建关键帧之后，对于双目、RGB-D的情况，会使用深度值大于0的关键点重投影得到地图点，得到的地图点会和当前关键帧关联，并加入到Map。 注意： 这里只是判断是否需要将当前帧创建为关键帧，并没有真的加入全局地图，因为Tracking线程的主要功能是局部定位，而处理地图中的关键帧、地图点，包括如何加入、如何删除的工作是在LocalMapping线程完成的，这里也可以看出作者的思路是比较清楚的，Tracking负责localization，LocalMapping负责Mapping，就构建了粗略的完整SLAM框架，然后加入初始化和闭环检测以及一些可视化模块，形成完整的SLAM。 总结 追踪模块是实现SLAM框架中视觉里程计模块的主体部分，其主要过程包括：初始化、初始追踪（初始位姿估计）、局部地图追踪（进一步的位姿估计）、局部优化、决定是否创建新的关键帧等。初始位姿估计仅仅完成了视觉里程计中的帧间追踪，该过程要么选择上一帧，要么选择参考关键帧，要么从全局关键帧数据库中选取候选关键帧与当前帧进行特征匹配，分别进行恒速运动模型（通过投影上一帧的地图点到当前帧，实现投影匹配）、参考关键帧追踪模型（通过投影参考关键帧的地图点到当前帧，实现投影匹配）、重定位模型（用DBow2实现匹配）。此外，还需要进行局部地图的追踪（通过投影局部地图点到当前帧，实现投影匹配），提高精度。 小记在Relocalization和LoopClosing中进行匹配的是在很多帧关键帧集合中匹配，属于PlaceRecognition，因此需要用DBow； 投影匹配适用于两帧之间，或者投影范围内（局部地图，前一个关键帧对应地图点）的MapPoints与当前帧之间，恒速运动模型、参考关键帧追踪模型、局部地图追踪模型都是使用的投影匹配。 后续再深入学习其他内容 ORB特征提取（涉及到Fast关键点检测、rBRIEF描述子、SIFT特征） 尺度空间、金字塔、变化尺度（尺度因子） DBoW2、BoW向量 以上在ORB_SLAM中的应用 相机运动估计 地图点投影（匹配）、三角测量2D-2D：对极几何（本质矩阵 单应矩阵）http://zhehangt.win/2017/03/05/SLAM/EpipolarGeometry/ 参考资料 视觉slam十四讲P153 计算机视觉算法与应用P264 视觉slam十四讲P141 双目立体成像与畸变校正（双目矫正） 三角剖分 参考资料 http://www.cnblogs.com/luyb/p/5357790.html https://www.cnblogs.com/shang-slam/p/6395514.html]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB_SLAM2学习之运行ROS模块]]></title>
    <url>%2F2018%2F08%2F12%2FORB_SLAM2%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%BF%90%E8%A1%8CROS%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[这篇文章是有关运行ORB_SLAM2系统ROS模块，包括单目和双目部分的学习内容。 ORB_SLAM2运行ROS模块需要从相应的话题接收图像用于SLAM系统，对于我这种不方便使用相机进行实时采集图像的渣渣来说，使用数据集图像是很好的选择。因此需要从本地数据集中获取图像，再利用ROS中的话题进行图像的发布和接收。下面的内容将介绍利用ROS进行简单的图像发布和接收操作，以及ORB_SLAM2系统ROS模块运行起来的整个过程。 系统环境 Ubuntu 16.04 ROS kinetic 基于ROS话题发布、接收图像创建相关软件包在catkin_ws/src目录下新建软件包并编译： 1234catkin_create_pkg my_image_transport image_transport cv_bridgecd ..catkin_make -DCATKIN_WHITELIST_PACKAGES="my_image_transport"source devel/setup.bash 创建图像发布者程序新建my_image_transport/src/my_publisher.cpp： 12345678910111213141516171819202122#include &lt;ros/ros.h&gt;#include &lt;image_transport/image_transport.h&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;cv_bridge/cv_bridge.h&gt;int main(int argc, char** argv)&#123; ros::init(argc, argv, "image_publisher"); ros::NodeHandle nh; image_transport::ImageTransport it(nh); image_transport::Publisher pub = it.advertise("camera/image", 1); cv::Mat image = cv::imread(argv[1], CV_LOAD_IMAGE_COLOR); cv::waitKey(30);//不断刷新图像，频率时间为delay，单位为ms sensor_msgs::ImagePtr msg = cv_bridge::CvImage(std_msgs::Header(), "bgr8", image).toImageMsg(); ros::Rate loop_rate(5); while (nh.ok()) &#123; pub.publish(msg); ros::spinOnce(); loop_rate.sleep(); &#125;&#125; 代码解释： line 1-4：ros.h头文件是所有的ros节点中必须要包含的，下面三个分别是实现图像的发布和订阅，调用opencv库，完成opencv图像格式转化为ROS图像格式所要用到的头文件； line 11：告知结点管理器要在camera/image话题发布图像消息，参数1是话题名称，话题2是缓冲区大小（即消息队列的长度，在发布图像消息时消息队列的长度只能是1）； line 12：根据运行时给定的参数（图像文件的路径）读取图像； line 14：将opencv格式的图像转化为ROS所支持的消息类型，从而发布到相应的话题上； line 16-21：发布图片消息，使消息类型匹配的节点订阅该消息。 创建图像订阅者程序新建my_image_transport/src/my_subscriber.cpp： 1234567891011121314151617181920212223242526272829#include &lt;ros/ros.h&gt;#include &lt;image_transport/image_transport.h&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;cv_bridge/cv_bridge.h&gt;void imageCallback(const sensor_msgs::ImageConstPtr&amp; msg)&#123; try &#123; cv::imshow("view", cv_bridge::toCvShare(msg, "bgr8")-&gt;image); cv::waitKey(30); &#125; catch (cv_bridge::Exception&amp; e) &#123; ROS_ERROR("Could not convert from '%s' to 'bgr8'.", msg-&gt;encoding.c_str()); &#125;&#125;int main(int argc, char **argv)&#123; ros::init(argc, argv, "image_listener"); ros::NodeHandle nh; cv::namedWindow("view"); cv::startWindowThread(); image_transport::ImageTransport it(nh); image_transport::Subscriber sub = it.subscribe("camera/image", 1, imageCallback); ros::spin(); cv::destroyWindow("view");&#125; 代码解释： line 6：回调函数，当有新的图像消息到达camera/image时，该函数就会被调用； line 10：显示捕捉到的图像，其中cv_bridge::toCvShare(msg, &quot;bgr8&quot;)-&gt;image用于将ROS图像消息转化为Opencv支持的图像格式（采用BGR8编码方式）。这部分用法恰好与上一节中发布者节点中的CvImage(std_msgs::Header(), &quot;bgr8&quot;, image).toImageMsg(); 的作用相反 line 11：刷新图像的频率，实践过程中发现如果注释这一行图像将无法在窗口的显示 相关配置文件 CMakeLists.txt内容 12345678910111213141516171819202122232425262728293031323334cmake_minimum_required(VERSION 2.8.3)project(my_image_transport)## Compile as C++11, supported in ROS Kinetic and neweradd_compile_options(-std=c++11)find_package(catkin REQUIRED COMPONENTS cv_bridge image_transport)find_package(OpenCV REQUIRED)set(LIBS $&#123;OpenCV_LIBS&#125; $&#123;catkin_LIBRARIES&#125;) catkin_package(# INCLUDE_DIRS include# LIBRARIES my_image_transport# CATKIN_DEPENDS cv_bridge image_transport# DEPENDS system_lib)include_directories(# include $&#123;catkin_INCLUDE_DIRS&#125; $&#123;OpenCV_INCLUDE_DIRS&#125;)add_executable(my_publisher src/my_publisher.cpp)target_link_libraries(my_publisher $&#123;LIBS&#125;)add_executable(my_subscriber src/my_subscriber.cpp)target_link_libraries(my_subscriber $&#123;LIBS&#125;) package.xml文件中添加 12&lt;build_depend&gt;opencv2&lt;/build_depend&gt;&lt;exec_depend&gt;opencv2&lt;/exec_depend&gt; 编译软件包12cd ~/catkin_wscatkin_make -DCATKIN_WHITHELIST_PACKAGES="my_image_transport" 运行节点单独开启一个终端执行roscore，启动ros节点管理器。 开启另一个终端，启动发布者节点： 1rosrun my_image_transport my_publisher /home/eric/catkin_ws/src/my_image_transport/000.png 运行订阅者节点： 1rosrun my_image_transport my_subscriber 运行结果如下所示： 查看当前活动节点及交互情况查看当前活动节点： 1rosnode list 查看各节点交互情况： 1rosrun rqt_graph rqt_graph 可以执行rosnode kill命令关闭相关节点。 ORB_SLAM2 ROS模块结点的编译在环境变量ROS_PACKAGE_PATH中添加Examples/ROS/ORB_SLAM2的路径： 1echo "source ~/slam/ORB_SLAM2/Examples/ROS/ORB_SLAM2/build/devel/setup.sh" &gt;&gt; ~/.bashrc 在~/slam/ORB_SLAM2目录下执行： 12chmod +x build_ros.sh./build_ros.sh 等待编译成功。 ROS Mono首先在my_image_transport目录下创建图像发布者程序mono_tum.cpp： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#include &lt;ros/ros.h&gt;#include &lt;image_transport/image_transport.h&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;cv_bridge/cv_bridge.h&gt;#include &lt;fstream&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;void LoadImages(const string &amp;strFile, vector&lt;string&gt; &amp;vstrImageFilenames, vector&lt;double&gt; &amp;vTimestamps);int main(int argc, char** argv)&#123; ros::init(argc, argv, "mono_tum"); if(argc != 2) &#123; cerr &lt;&lt; endl &lt;&lt; "Usage: rosrun my_image_transport mono_tum path_to_sequence" &lt;&lt; endl; return 1; &#125; ros::NodeHandle nh; image_transport::ImageTransport it(nh); image_transport::Publisher pub = it.advertise("/camera/image_raw", 1); vector&lt;string&gt; vstrImageFilenames; vector&lt;double&gt; vTimestamps; string strFile = string(argv[1])+"/rgb.txt"; LoadImages(strFile, vstrImageFilenames, vTimestamps); int nImages = vstrImageFilenames.size(); cv::Mat im; // double tframe; sensor_msgs::ImagePtr msg; std_msgs::Header header; ros::Rate loop_rate(5); for(int ni = 0; ni &lt; nImages; ni++) &#123; im = cv::imread(string(argv[1])+"/"+vstrImageFilenames[ni],CV_LOAD_IMAGE_UNCHANGED); header.stamp = ros::Time(vTimestamps[ni]); if(im.empty()) &#123; cerr &lt;&lt; endl &lt;&lt; "Failed to load image at: " &lt;&lt; string(argv[1]) &lt;&lt; "/" &lt;&lt; vstrImageFilenames[ni] &lt;&lt; endl; return 1; &#125; cv::waitKey(30); msg = cv_bridge::CvImage(header, "bgr8", im).toImageMsg(); pub.publish(msg); cv::waitKey(1); ros::spinOnce(); loop_rate.sleep(); &#125; return 0;&#125;void LoadImages(const string &amp;strFile, vector&lt;string&gt; &amp;vstrImageFilenames, vector&lt;double&gt; &amp;vTimestamps)&#123; ifstream f; f.open(strFile.c_str()); // skip first three lines string s0; getline(f,s0); getline(f,s0); getline(f,s0); while(!f.eof()) &#123; string s; getline(f,s); if(!s.empty()) &#123; stringstream ss; ss &lt;&lt; s; double t; string sRGB; ss &gt;&gt; t; vTimestamps.push_back(t); ss &gt;&gt; sRGB; vstrImageFilenames.push_back(sRGB); &#125; &#125;&#125; 在CMakeLists.txt文件中添加： 12add_executable(mono_tum src/mono_tum.cpp)target_link_libraries(mono_tum $&#123;LIBS&#125;) 下载TUM-rgbd_dataset_freiburg1_xyz数据集，保存/media/eric/linux/DATA/TUM/rgbd_dataset_freiburg1_xyz。 测试系统，启动ORB_SLAM2 Mono： 1rosrun ORB_SLAM2 Mono /home/eric/slam/ORB_SLAM2/Vocabulary/ORBvoc.txt ~/slam/ORB_SLAM2/Examples/Monocular/TUM1.yaml 启动发布者节点： 1rosrun my_image_transport mono_tum /media/eric/linux/DATA/TUM/rgbd_dataset_freiburg1_xyz 测试效果： ROS Stereo首先在my_image_transport目录下创建图像发布者程序左相机节点stereo_left_kitti.cpp： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#include &lt;ros/ros.h&gt;#include &lt;ros/package.h&gt;#include &lt;image_transport/image_transport.h&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;cv_bridge/cv_bridge.h&gt;#include &lt;fstream&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;void LoadImages(const string &amp;strPathToSequence, vector&lt;string&gt; &amp;vstrImageLeft, vector&lt;double&gt; &amp;vTimestamps);int main(int argc, char** argv)&#123; ros::init(argc, argv, "stereo_left_kitti"); string packagePath = ros::package::getPath("my_image_transport"); string configPath = packagePath + "//config//stereo_kitti.yaml"; ros::NodeHandle nh; image_transport::ImageTransport it(nh); image_transport::Publisher pub = it.advertise("/camera/left/image_raw", 1); vector&lt;string&gt; vstrImageLeft; vector&lt;double&gt; vTimestamps; cv::FileStorage fsSettings(configPath, cv::FileStorage::READ); if(!fsSettings.isOpened()) &#123; cerr &lt;&lt; "ERROR: Wrong path to settings" &lt;&lt; endl; return -1; &#125; string bagPath = fsSettings["bagPath"]; LoadImages(bagPath, vstrImageLeft, vTimestamps); int nImages = vstrImageLeft.size(); cv::Mat imLeft; sensor_msgs::ImagePtr msg; std_msgs::Header header; ros::Rate loop_rate(5); for(int ni = 0; ni &lt; nImages; ni++) &#123; imLeft = cv::imread(vstrImageLeft[ni],CV_LOAD_IMAGE_UNCHANGED); header.stamp = ros::Time(vTimestamps[ni]); if(imLeft.empty()) &#123; cerr &lt;&lt; endl &lt;&lt; "Failed to load image at: " &lt;&lt; string(vstrImageLeft[ni]) &lt;&lt; endl; return 1; &#125; cv::waitKey(30); msg = cv_bridge::CvImage(header, "mono8", imLeft).toImageMsg(); pub.publish(msg); cv::waitKey(1); ros::spinOnce(); loop_rate.sleep(); &#125; return 0;&#125;void LoadImages(const string &amp;strPathToSequence, vector&lt;string&gt; &amp;vstrImageLeft, vector&lt;double&gt; &amp;vTimestamps)&#123; ifstream fTimes; string strPathTimeFile = strPathToSequence + "/times.txt"; fTimes.open(strPathTimeFile.c_str()); while(!fTimes.eof()) &#123; string s; getline(fTimes,s); if(!s.empty()) &#123; stringstream ss; ss &lt;&lt; s; double t; ss &gt;&gt; t; vTimestamps.push_back(t); &#125; &#125; string strPrefixLeft = strPathToSequence + "/image_0/"; const int nTimes = vTimestamps.size(); vstrImageLeft.resize(nTimes); for(int i=0; i&lt;nTimes; i++) &#123; stringstream ss; ss &lt;&lt; setfill('0') &lt;&lt; setw(6) &lt;&lt; i; vstrImageLeft[i] = strPrefixLeft + ss.str() + ".png"; &#125;&#125; 右相机节点stereo_right_kitti： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#include &lt;ros/ros.h&gt;#include &lt;ros/package.h&gt;#include &lt;image_transport/image_transport.h&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;cv_bridge/cv_bridge.h&gt;#include &lt;fstream&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;void LoadImages(const string &amp;strPathToSequence, vector&lt;string&gt; &amp;vstrImageRight, vector&lt;double&gt; &amp;vTimestamps);int main(int argc, char** argv)&#123; ros::init(argc, argv, "stereo_right_kitti"); string packagePath = ros::package::getPath("my_image_transport"); string configPath = packagePath + "//config//stereo_kitti.yaml"; ros::NodeHandle nh; image_transport::ImageTransport it(nh); image_transport::Publisher pub = it.advertise("/camera/right/image_raw", 1); vector&lt;string&gt; vstrImageRight; vector&lt;double&gt; vTimestamps; cv::FileStorage fsSettings(configPath, cv::FileStorage::READ); if(!fsSettings.isOpened()) &#123; cerr &lt;&lt; "ERROR: Wrong path to settings" &lt;&lt; endl; return -1; &#125; string bagPath = fsSettings["bagPath"]; LoadImages(bagPath, vstrImageRight, vTimestamps); int nImages = vstrImageRight.size(); cv::Mat imRight; sensor_msgs::ImagePtr msg; std_msgs::Header header; ros::Rate loop_rate(5); for(int ni = 0; ni &lt; nImages; ni++) &#123; imRight = cv::imread(vstrImageRight[ni],CV_LOAD_IMAGE_UNCHANGED); header.stamp = ros::Time(vTimestamps[ni]); if(imRight.empty()) &#123; cerr &lt;&lt; endl &lt;&lt; "Failed to load image at: " &lt;&lt; string(vstrImageRight[ni]) &lt;&lt; endl; return 1; &#125; cv::waitKey(30); msg = cv_bridge::CvImage(header, "mono8", imRight).toImageMsg(); pub.publish(msg); cv::waitKey(1); ros::spinOnce(); loop_rate.sleep(); &#125; return 0;&#125;void LoadImages(const string &amp;strPathToSequence, vector&lt;string&gt; &amp;vstrImageRight, vector&lt;double&gt; &amp;vTimestamps)&#123; ifstream fTimes; string strPathTimeFile = strPathToSequence + "/times.txt"; fTimes.open(strPathTimeFile.c_str()); while(!fTimes.eof()) &#123; string s; getline(fTimes,s); if(!s.empty()) &#123; stringstream ss; ss &lt;&lt; s; double t; ss &gt;&gt; t; vTimestamps.push_back(t); &#125; &#125; string strPrefixRight = strPathToSequence + "/image_1/"; const int nTimes = vTimestamps.size(); vstrImageRight.resize(nTimes); for(int i=0; i&lt;nTimes; i++) &#123; stringstream ss; ss &lt;&lt; setfill('0') &lt;&lt; setw(6) &lt;&lt; i; vstrImageRight[i] = strPrefixRight + ss.str() + ".png"; &#125;&#125; 在CMakeLists.txt文件中添加： 12345add_executable(stereo_left_kitti src/stereo_left_kitti.cpp)target_link_libraries(stereo_left_kitti $&#123;LIBS&#125;)add_executable(stereo_right_kitti src/stereo_right_kitti.cpp)target_link_libraries(stereo_right_kitti $&#123;LIBS&#125;) 在my_image_transport/config目录下添加配置文件stereo_kitti.yaml，保存数据集路径： 12%YAML:1.0bagPath: "/media/eric/linux/DATA/KITTI/odometry/data_odometry_gray/sequences/03" 这里需要启动三个节点，每个单独启动会比较麻烦，所以使用ROS Launch文件，同时启动左、右图像发布节点和ORB_SLAM2 Stereo_eric节点。在my_image_transport/launch目录下添加ROS节点启动文件stereo_image_transport.yaml： 12345678&lt;launch&gt; &lt;node name="stereo_left_kitti" pkg="my_image_transport" type="stereo_left_kitti"&gt; &lt;/node&gt; &lt;node name="stereo_right_kitti" pkg="my_image_transport" type="stereo_right_kitti"&gt; &lt;/node&gt; &lt;node name="Stereo_eric" pkg="ORB_SLAM2" type="Stereo_eric"&gt; &lt;/node&gt;&lt;/launch&gt; 修改ORB_SLAM2的ros_stereo.cc文件，新建为ros_stereo_eric.cc： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;fstream&gt;#include&lt;chrono&gt;#include&lt;ros/ros.h&gt;#include &lt;cv_bridge/cv_bridge.h&gt;#include &lt;message_filters/subscriber.h&gt;#include &lt;message_filters/time_synchronizer.h&gt;#include &lt;message_filters/sync_policies/approximate_time.h&gt;#include&lt;opencv2/core/core.hpp&gt;#include &lt;ros/package.h&gt;#include"../../../include/System.h"using namespace std;class ImageGrabber&#123;public: ImageGrabber(ORB_SLAM2::System* pSLAM):mpSLAM(pSLAM)&#123;&#125; void GrabStereo(const sensor_msgs::ImageConstPtr&amp; msgLeft,const sensor_msgs::ImageConstPtr&amp; msgRight); ORB_SLAM2::System* mpSLAM; bool do_rectify; cv::Mat M1l,M2l,M1r,M2r;&#125;;int main(int argc, char **argv)&#123; ros::init(argc, argv, "Stereo_eric"); ros::start(); string packagePath = ros::package::getPath("ORB_SLAM2"); string configPath = packagePath + "//config//ros_stereo_eric.yaml"; cv::FileStorage fsSettings(configPath, cv::FileStorage::READ); if(!fsSettings.isOpened()) &#123; cerr &lt;&lt; "ERROR: Wrong path to settings" &lt;&lt; endl; return -1; &#125; string vocPath = fsSettings["vocPath"]; string settingPath = fsSettings["settingPath"]; // Create SLAM system. It initializes all system threads and gets ready to process frames. ORB_SLAM2::System SLAM(vocPath,settingPath,ORB_SLAM2::System::STEREO,true); ImageGrabber igb(&amp;SLAM); string do_rectify = fsSettings["do_rectify"]; stringstream ss(do_rectify); ss &gt;&gt; boolalpha &gt;&gt; igb.do_rectify;//boolalpha函数把bool值显示为true或false if(igb.do_rectify) &#123; // Load settings related to stereo calibration cv::FileStorage fsSettings(settingPath, cv::FileStorage::READ); if(!fsSettings.isOpened()) &#123; cerr &lt;&lt; "ERROR: Wrong path to settings" &lt;&lt; endl; return -1; &#125; cv::Mat K_l, K_r, P_l, P_r, R_l, R_r, D_l, D_r; fsSettings["LEFT.K"] &gt;&gt; K_l; fsSettings["RIGHT.K"] &gt;&gt; K_r; fsSettings["LEFT.P"] &gt;&gt; P_l; fsSettings["RIGHT.P"] &gt;&gt; P_r; fsSettings["LEFT.R"] &gt;&gt; R_l; fsSettings["RIGHT.R"] &gt;&gt; R_r; fsSettings["LEFT.D"] &gt;&gt; D_l; fsSettings["RIGHT.D"] &gt;&gt; D_r; int rows_l = fsSettings["LEFT.height"]; int cols_l = fsSettings["LEFT.width"]; int rows_r = fsSettings["RIGHT.height"]; int cols_r = fsSettings["RIGHT.width"]; if(K_l.empty() || K_r.empty() || P_l.empty() || P_r.empty() || R_l.empty() || R_r.empty() || D_l.empty() || D_r.empty() || rows_l==0 || rows_r==0 || cols_l==0 || cols_r==0) &#123; cerr &lt;&lt; "ERROR: Calibration parameters to rectify stereo are missing!" &lt;&lt; endl; return -1; &#125; cv::initUndistortRectifyMap(K_l,D_l,R_l,P_l.rowRange(0,3).colRange(0,3),cv::Size(cols_l,rows_l),CV_32F,igb.M1l,igb.M2l); cv::initUndistortRectifyMap(K_r,D_r,R_r,P_r.rowRange(0,3).colRange(0,3),cv::Size(cols_r,rows_r),CV_32F,igb.M1r,igb.M2r); &#125; ros::NodeHandle nh; message_filters::Subscriber&lt;sensor_msgs::Image&gt; left_sub(nh, "/camera/left/image_raw", 1); message_filters::Subscriber&lt;sensor_msgs::Image&gt; right_sub(nh, "camera/right/image_raw", 1); typedef message_filters::sync_policies::ApproximateTime&lt;sensor_msgs::Image, sensor_msgs::Image&gt; sync_pol; message_filters::Synchronizer&lt;sync_pol&gt; sync(sync_pol(10), left_sub,right_sub); sync.registerCallback(boost::bind(&amp;ImageGrabber::GrabStereo,&amp;igb,_1,_2)); ros::spin(); // Stop all threads SLAM.Shutdown(); // Save camera trajectory SLAM.SaveKeyFrameTrajectoryTUM("KeyFrameTrajectory_TUM_Format.txt"); SLAM.SaveTrajectoryTUM("FrameTrajectory_TUM_Format.txt"); SLAM.SaveTrajectoryKITTI("FrameTrajectory_KITTI_Format.txt"); ros::shutdown(); return 0;&#125;void ImageGrabber::GrabStereo(const sensor_msgs::ImageConstPtr&amp; msgLeft,const sensor_msgs::ImageConstPtr&amp; msgRight)&#123; // Copy the ros image message to cv::Mat. cv_bridge::CvImageConstPtr cv_ptrLeft; try &#123; cv_ptrLeft = cv_bridge::toCvShare(msgLeft); &#125; catch (cv_bridge::Exception&amp; e) &#123; ROS_ERROR("cv_bridge exception: %s", e.what()); return; &#125; cv_bridge::CvImageConstPtr cv_ptrRight; try &#123; cv_ptrRight = cv_bridge::toCvShare(msgRight); &#125; catch (cv_bridge::Exception&amp; e) &#123; ROS_ERROR("cv_bridge exception: %s", e.what()); return; &#125; if(do_rectify) &#123; cv::Mat imLeft, imRight; cv::remap(cv_ptrLeft-&gt;image,imLeft,M1l,M2l,cv::INTER_LINEAR); cv::remap(cv_ptrRight-&gt;image,imRight,M1r,M2r,cv::INTER_LINEAR); mpSLAM-&gt;TrackStereo(imLeft,imRight,cv_ptrLeft-&gt;header.stamp.toSec()); &#125; else &#123; //cv_ptrLeft-&gt;image is cv::Mat mpSLAM-&gt;TrackStereo(cv_ptrLeft-&gt;image,cv_ptrRight-&gt;image,cv_ptrLeft-&gt;header.stamp.toSec()); &#125;&#125; ROS/ORB_SLAM2/config目录下添加配置文件ros_stereo_eric.yaml： 1234%YAML:1.0vocPath: "/home/eric/slam/ORB_SLAM2/Vocabulary/ORBvoc.txt"settingPath: "/home/eric/slam/ORB_SLAM2/Examples/Stereo/KITTI03.yaml"do_rectify: "false" 下载KITTI-odometry/data_odometry_gray数据集，保存/media/eric/linux/DATA/KITTI/odometry/data_odometry_gray。 测试系统，启动launch文件，同时启动图像发布者节点和ORB_SLAM2 Stereo_eric节点： 1roslaunch my_image_transport stereo_image_transport.launch 测试效果： 参考资料 http://wiki.ros.org/image_transport/Tutorials/PublishingImages http://wiki.ros.org/image_transport/Tutorials/SubscribingToImages https://blog.csdn.net/github_30605157/article/details/50990493]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之OpenCV图像、ROS Image转换接口cv_bridge]]></title>
    <url>%2F2018%2F08%2F11%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8BOpenCV%E5%9B%BE%E5%83%8F%E3%80%81ROS%20Image%E8%BD%AC%E6%8D%A2%E6%8E%A5%E5%8F%A3cv_bridge%2F</url>
    <content type="text"><![CDATA[这篇文章是有关OpenCV图像与ROS Image转换接口ROS cv_bridge的学习内容。 由于项目中需要使用ROS消息发布器、接收器分别发布和接收图像消息，一般情况下发布消息之前需要将cv::Mat格式的图像转化为ROS Image message，接收到消息后也需要再转化到cv::Mat格式。这个过程就需要使用ROS cv_bridge，即是一个ROS和OpenCV库之间提供接口的开发包。 ROS Image message转OpenCV图像示例代码： 123456789101112131415void imageCallback(const sensor_msgs::ImageConstPtr&amp; msg)&#123; cv_bridge::CvImageConstPtr cv_ptr_image; try &#123; cv_ptr_image = cv_bridge::toCvShare(msg, "mono8"); cv::Mat = cv_ptr_image-&gt;image; cv::waitKey(30); &#125; catch (cv_bridge::Exception&amp; e) &#123; ROS_ERROR("cv_bridge exception: %s", e.what()); ROS_ERROR("Could not convert from '%s' to 'bgr8'.", msg-&gt;encoding.c_str()); &#125;&#125; CvImage类cvbridge定义了一个opencv图像CvImage的类型、包含了编码和ROS的信息头。CvImage包含准确的信息sensor_msgs /image，因此我们可以将两种数据类型进行转换。cv_bridge::CvImage类（#include &lt;cv_bridge.h&gt;）定义： 123456789101112namespace cv_bridge &#123;class CvImage&#123;//Public Attributespublic: std_msgs::Header header; // ROS header. std::string encoding; // Image encoding ("mono8", "bgr8", etc.) cv::Mat image; // Image data for use with OpenCV. &#125;;typedef boost::shared_ptr&lt;CvImage&gt; CvImagePtr;typedef boost::shared_ptr&lt;CvImage const&gt; CvImageConstPtr;&#125; toCvShare()函数原型12CvImageConstPtr cv_bridge::toCvShare(const sensor_msgs::ImageConstPtr &amp; source, const std::string &amp; encoding = std::string()) 函数功能：将sensor_msgs::Image类型的message转化为与OpenCV兼容的cv_bridge::CvImage类型； 输入：图像消息指针、可选的编码参数（编码是指CvImage的类型）；如果没有输入编码（或更确切地说，空字符串），则目标图像编码将与图像消息编码相同（即与消息发布器发布消息时转化图像时的图像编码）； 输出：CvImageConstPtr智能指针，指向CvImage类型的数据。 介绍几种cvbridge中常见的数据编码的形式，cv_bridge可以有选择的对颜色和深度信息进行转化。为了使用指定的特征编码，就有下面集中的编码形式： mono8: CV_8UC1， 灰度图像 mono16: CV_16UC1,16位灰度图像 bgr8: CV_8UC3,带有颜色信息并且颜色的顺序是BGR顺序 rgb8: CV_8UC3,带有颜色信息并且颜色的顺序是RGB顺序 bgra8: CV_8UC4, BGR的彩色图像，并且带alpha通道 rgba8: CV_8UC4,CV，RGB彩色图像，并且带alpha通道 注：这其中mono8和bgr8两种图像编码格式是大多数OpenCV的编码格式。 OpenCV图像编码格式： 12345678UC[1-4]8SC[1-4]16UC[1-4]16SC[1-4]32SC[1-4]32FC[1-4]64FC[1-4] 补充使用rosmsg show Header命令查看消息详细信息： 1234[std_msgs/Header]:uint32 seqtime stamp #ros::Timestring frame_id OpenCV图像转ROS Image message示例代码： 123456789101112131415161718#include &lt;cv_bridge/cv_bridge.h&gt;int main()&#123; cv::Mat imRight; sensor_msgs::ImagePtr msg; std_msgs::Header header; imRight = cv::imread("image_path",CV_LOAD_IMAGE_UNCHANGED); header.stamp = ros::Time("time_stamp");//不需要传时间戳就不用设置 if(imRight.empty()) &#123; cerr &lt;&lt; endl &lt;&lt; "Failed to load image at: " &lt;&lt; string("image_path") &lt;&lt; endl; return 1; &#125; cv::waitKey(30); msg = cv_bridge::CvImage(header, "mono8", imRight).toImageMsg();//如果不需要传时间戳，第一参数可以为std_msgs::Header()&#125; 实现这个过程，重点是调用了CvImage类的toImageMsg()函数。 12345678class CvImage&#123; sensor_msgs::ImagePtr toImageMsg() const; // Overload mainly intended for aggregate messages that contain // a sensor_msgs::Image as a member. void toImageMsg(sensor_msgs::Image&amp; ros_image) const;&#125;; 参考资料 http://docs.ros.org/hydro/api/cv_bridge/html/c++/namespacecv__bridge.html#aafa38a1d9be98d9efaefe45fd873133c https://www.cnblogs.com/li-yao7758258/p/6637079.html http://docs.ros.org/hydro/api/cv_bridge/html/c++/cv__bridge_8cpp.html]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zotero基于坚果云实现多机同步]]></title>
    <url>%2F2018%2F08%2F10%2FZotero%E5%9F%BA%E4%BA%8E%E5%9D%9A%E6%9E%9C%E4%BA%91%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%9C%BA%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[这篇文章是有关Zotero工具基于坚果云和webdav实现多机同步的内容。 首先参考该文章在Zotero中设置webdav连接到坚果云，其中输入的坚果云服务器地址为https://dav.jianguoyun.com/dav/work，需要提前在坚果云创建work文件夹。 提示：输入的用户名为坚果云账号邮箱，密码为第三方应用管理中获取到的应用密码。 Ubuntu设置方法 我的zotero源文件开始是在ubuntu系统下，在上面的操作后，坚果云会提示在本地创建work目录，我设置在/home/usrname/work/； zotero安装在/home/usrname/Zotero目录下，将该目录下的storage目录剪切到/home/usrname/work/zotero目录下； 命令行执行：ln -s /home/usrname/Zotero /home/usrname/work/zotero，会在Zotero下面会出现软链接目录storage； 等待坚果云同步完成，再回到zotero里面点击同步按钮。 提示：如果work目录已经创建好，里面同步了数据，这时候就可以直接将zotero下的storage删除，执行命令： ln -s /home/eric/work/zotero/storage /home/eric/Zotero创建软链接到zotero下即可。 win设置方法 由于前面在ubuntu系统已经设置过，进入windows后，坚果云会提示同步work目录（联网状态），这时选择同步到自己想要放置的磁盘，如D:\work。zotero默认安装在C盘，storage目录地址是C:\User\user\Zotero\storage； 剪切C:\User\user\Zotero\storage目录到D:\work\Zotero目录下（这时zotero下面没有storage目录了）； 运行cmd输入：mklink /j C:\User\user\Zotero\storage D:\work\Zotero，现在zotero下面出现了软链接目录storage。 参考：http://www.360doc.com/content/18/0506/18/46033958_751645308.shtml]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Zotero</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之消息过滤器messsage_filters]]></title>
    <url>%2F2018%2F08%2F08%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B6%88%E6%81%AF%E8%BF%87%E6%BB%A4%E5%99%A8messsage-filters%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中messsage_filters使用的学习内容。 消息滤波器概述一组消息过滤器，它们接收消息并可以在之后根据过滤器需要满足的条件输出这些消息。 message_filters是一个用于roscpp和rospy的实用程序库。 它集合了许多的常用的消息“过滤”算法。 消息过滤器message_filters类似一个消息缓存，当消息到达消息过滤器的时候，可能并不会立即输出，而是在稍后的时间点里满足一定条件下才输出。 举个例子，比如时间同步器，它接收来自多个源的不同类型的消息，并且仅当它们在每个源上接收到具有相同时间戳的消息时才输出它们，也就是起到了一个消息同步输出的效果。 Subscriber 订阅者C++ message_filters::Subscriber API docs 订阅者过滤器是对ROS订阅的封装，为其他过滤器提供源（source）。订阅者过滤器无法将另一个过滤器的输出作为其输入，而是使用ROS话题作为其输入。即通过过订阅ROS话题，从订阅的话题中获取相关信息作为其输入。 输入输出形式输入 ​ 无输入连接 输出 ​ C++: void callback(const boost::shared_ptr&lt;M const&gt;&amp;) 例子（C++）12message_filters::Subscriber&lt;std_msgs::UInt32&gt; sub(nh, "my_topic", 1);sub.registerCallback(myCallback); 等同于 1ros::Subscriber sub = nh.subscribe("my_topic", 1, myCallback); Policy-Based Synchronizer 基于策略的同步器 [ROS 1.1+]Synchronizer filter同步滤波器通过包含在其header中的时间戳来同步输入通道，并以单个回调的形式经过相同数量的通道输出它们。 C ++实现最多可以同步9个通道。 关于header，以sensor_msgs/Image消息为例： 123456789101112&gt; [sensor_msgs/Image]:&gt; std_msgs/Header header&gt; uint32 seq&gt; time stamp&gt; string frame_id&gt; uint32 height&gt; uint32 width&gt; string encoding&gt; uint8 is_bigendian&gt; uint32 step&gt; uint8[] data&gt; Synchronize滤波器在确定如何同步通道的策略上进行模板化。 有两种策略：ExactTime和ApproximateTime，理解为松同步与紧同步，紧同步是精确的同步，松同步是粗略的同步，分别对应message_filters::sync_policies::ExactTime 、message_filters::sync_policies::ApproximateTime。 C++ Header: message_filters/synchronizer.h 输入输出形式输入C ++： 最多9个独立的过滤器，每个过滤器的形式为void callback（const boost::shared_ptr &lt;M const&gt;＆）。 支持的过滤器数量由类创建的模板参数的数量决定。 输出C ++： 对于消息类型M0..M8，void callback（const boost::shared_ptr &lt;M0 const&gt;＆，...，const boost::shared_ptr &lt;M8 const&gt;＆）。 参数的数量由类创建的模板参数的数量决定。 ExactTime策略message_filters::sync_policies::ExactTime策略要求消息具有完全相同的时间戳以便匹配。 只有在具有相同确切时间戳的所有指定通道上收到消息时，才会调用回调。 从所有消息的header域读取时间戳（这是该策略所必需的）。 C++头文件：message_filters/sync_policies/exact_time.h 12345678910111213141516171819202122232425262728293031#include &lt;message_filters/subscriber.h&gt;#include &lt;message_filters/synchronizer.h&gt;#include &lt;message_filters/sync_policies/exact_time.h&gt;#include &lt;sensor_msgs/Image.h&gt;#include &lt;sensor_msgs/CameraInfo.h&gt;using namespace sensor_msgs;using namespace message_filters;void callback(const ImageConstPtr&amp; image, const CameraInfoConstPtr&amp; cam_info)&#123; // Solve all of perception here...&#125;int main(int argc, char** argv)&#123; ros::init(argc, argv, "vision_node"); ros::NodeHandle nh; message_filters::Subscriber&lt;sensor_msgs::Image&gt; image_sub(nh, "image", 1); message_filters::Subscriber&lt;sensor_msgs::CameraInfo&gt; info_sub(nh, "camera_info", 1); typedef message_filters::sync_policies::ExactTime&lt;sensor_msgs::Image, sensor_msgs::CameraInfo&gt; MySyncPolicy; // ExactTime takes a queue size as its constructor argument, hence MySyncPolicy(10) message_filters::Synchronizer&lt;MySyncPolicy&gt; sync(MySyncPolicy(10), image_sub, info_sub); sync.registerCallback(boost::bind(&amp;callback, _1, _2)); ros::spin(); return 0;&#125; ApproximateTime 策略message_filters::sync_policies::ApproximateTime策略使用自适应算法来匹配基于其时间戳的消息。 如果不是所有的消息都有一个标题字段，从中可以确定时间戳，请参见下面的解决方法。 C++头文件：message_filters/sync_policies/approximate_time.h 例子(C++)： 123456789101112131415161718192021222324252627282930#include &lt;message_filters/subscriber.h&gt;#include &lt;message_filters/synchronizer.h&gt;#include &lt;message_filters/sync_policies/approximate_time.h&gt;#include &lt;sensor_msgs/Image.h&gt;using namespace sensor_msgs;using namespace message_filters;void callback(const ImageConstPtr&amp; image1, const ImageConstPtr&amp; image2)&#123; // Solve all of perception here...&#125;int main(int argc, char** argv)&#123; ros::init(argc, argv, "vision_node"); ros::NodeHandle nh; message_filters::Subscriber&lt;sensor_msgs::Image&gt; image1_sub(nh, "image1", 1); message_filters::Subscriber&lt;sensor_msgs::Image&gt; image2_sub(nh, "image2", 1); typedef message_filters::sync_policies::ApproximateTime&lt;sensor_msgs::Image, sensor_msgs::Image&gt; MySyncPolicy; // ApproximateTime takes a queue size as its constructor argument, hence MySyncPolicy(10) message_filters::Synchronizer&lt;MySyncPolicy&gt; sync(MySyncPolicy(10), image1_sub, image2_sub); sync.registerCallback(boost::bind(&amp;callback, _1, _2)); ros::spin(); return 0;&#125; 如果某些消息的类型不包含header字段，则ApproximateTimeSynchronizer默认拒绝添加此类消息。 参考资料 https://blog.csdn.net/Start_From_Scratch/article/details/52337689?locationNum=10&amp;fps=1 ROS官网 https://blog.csdn.net/chishuideyu/article/details/77479758]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux设置环境变量]]></title>
    <url>%2F2018%2F08%2F07%2FLinux%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[这篇文章是有关Linux中环境变量设置的内容记录。 当前终端有效export PATH=$PATH:/home/.....(路径目录)：此方法只在当前终端有效 使用echo $PATH命令查看环境变量的内容。 命令source /devel/setup.bash将当前工作空间加入环境变量，也是只对当前终端有效。 永久有效将路径永久添加到PATH，每次启动终端都会找到路径： echo &quot;source ~/slam/ORB_SLAM2/Examples/ROS/ORB_SLAM2/build/devel/setup.sh&quot; &gt;&gt; ~/.bashrc]]></content>
      <categories>
        <category>系统</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记二-SLAM问题的数学表述与经典框架]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BA%8C-SLAM%E7%9A%84%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[这篇文章是视觉SLAM十四讲第2讲阅读过程中总结和记录的学习内容。 SLAM问题提出SLAM问题可以描述为：机器人在某个未知环境中从某个未知位置开始移动，在移动过程中根据传感器数据进行自身定位估计，同时在自身定位的基础上增量式构造地图，从而实现机器人对未知环境的地图构建和在地图中对自身的位置进行定位。对于机器人来说，SLAM主要回答了两个问题：1）我在什么地方？（定位） 2）周围环境是怎么样的？（建图），即机器人一方面要明白自身的状态（即位置），另一方面也要了解外在的环境（即地图）。SLAM问题的本质：对运动主体自身和周围环境空间不确定性的估计。 SLAM问题的数学表述SLAM要回答这两个问题，机器人需要通过传感器采集数据，然后根据这些数据推断出自身的位姿信息和所处的环境信息。 携带传感器的机器人在某未知环境中运动，由于相机通常是在某些时刻采集数据的，所以我们只关心这些时刻的位置和地图。这样就把一段连续时间的运动变成离散时刻$t=1,…,K$当中发生的事情。在这些时刻，用$x$表示小萝卜自身的位置。于是各时刻的位置就记为$x_1,…,x_K$，它们构成了机器人的轨迹。 地图方面，假设地图由许多个路标（landmark）组成，每个时刻传感器会测量到一部分路标点，设路标点一共有$N$个，用$y_1,…y_N$表示。机器人在某个时间内通过传感器获得一系列连续的传感器数据（路标点的观测数据），即观测值$z$。依据机器人所配备的传感器，观测值$z$可以是激光雷达（laser）数据、图像数据、里程计（odometer）数据和惯性导航单元（IMU）数据等。 运动方程SLAM需要解决的就是通过观测值$z$评估系统状态$x$。系统状态通常包含两部分，一部分用于表示机器人在环境中的位置，另一部分用于表示环境地图。通常，机器人会携带一个测量自身运动的传感器，比如里程计。因此可以构造一个评估函数，利用当前获得的传感器数据（不一定直接就是位置之差，还可能是加速度、角速度等信息），从前一时刻的系统状态评估当前时刻的系统状态，通用、抽象的数学模型如下所示： $xk=f(x{k-1},u_k,\omega_k)$ $u_k$是运动传感器的数据（或称为输入），$w_k$为噪声。这里的$f$指代一种计算模型，当输入的运动传感器类型不同时，$f$的具体形式会千差万别。我们通常把它称为运动方程。 观测方程与运动方程相对应是观测方程。观测方程描述的是当机器人在$xk$位置上利用传感器感知环境，看到了某个路标点$y_j$，产生了观测数据$z{k,j}$。此处同样用一个抽象的函数$h$来表述这个关系： $z{k,j}=h(y_j,x_k,\upsilon{k,j})$ 这里$v_k$表示此次观测的噪声。由于观测所用的传感器形式更多，因此这里的观测数据$z$以及观测方程$h$也有许多不同的形式。 参数化过程对于上述函数$f$、$h$，我们并没有给出具体的形式，没有具体地说明运动和观测是怎么回事，也不知道$x$、$y$、$z$如何表示的。其实根据机器人的真实运动和传感器的种类，存在着若干种参数化（Parameterization）方式。 举例来说，假设机器人在平面运动，其位姿由两个位置和一个转角来描述，即$x_k=[x,y,\theta]^T_k$。同时，运动传感器能够测量到机器人在任意两个时间间隔位置和转角的变化量$u_k=[\Delta x,\Delta y,\Delta \theta]^T_k$，所以运动方程就可以具体化为： $\left[ \begin{matrix}x\y\\theta\end{matrix} \right]k=\left[ \begin{matrix}x\y\\theta\end{matrix} \right]{k-1}+\left[ \begin{matrix}\Delta x\\Delta y\\Delta \theta\end{matrix} \right]_k+\omega_k$ 关于观测方程，机器人携带一个二维激光传感器，在激光传感器观测一个2D路标点时，能够测到两个量：路标点和机器人本体之间的距离$r$和夹角$\phi$，所以观测方程具体化为： $\left[ \begin{matrix}r\\phi\end{matrix} \right]=\left[ \begin{matrix}\sqrt{(p_x-x)^2+(p_y-y)^2}\ arctan(\frac{p_y-y}{p_x-x})\end{matrix} \right]+\upsilon$ 在视觉SLAM中，传感器是相机，那么观测方程就是“对路标点拍摄后，得到图像中的像素”的过程。该过程牵扯到相机模型的描述，在后续的阅读中再详细记录。 最基本的SLAM问题综上所述，针对不同的传感器，运动和观测方程会有不同的参数化形式。如果保持它们的通用性，取成通用的抽象形式，那么SLAM过程就可以表述为两个基本方程： $\left{ \begin{array}{ll} xk=f(x{k-1},uk,\omega_k)\ z{k,j}=h(yj,x_k,\upsilon{k,j})\end{array} \right.$ 这两个方程描述了最基本的SLAM问题：当知道运动测量的读数$u$，以及传感器的读数$z$时，如果求解定位问题（估计$x$）h和建图问题（估计$y$）。即其中$z$和$u$是已知的，当选定了传感器，运动方程和观测方程也是已知的，因此SLAM求解的目标就是对$x$和$y$进行估计，使得运动方程和观测方程等式两边尽可能的成立。这时，就把SLAM问题建模为一个状态估计问题：如何通过带有噪声的测量数据，估计内部的、隐藏的状态变量。 疑问：观测方程和运动方程会同时用到吗？视觉SLAM只用到观测方程？？？ 视觉SLAM中： 观测方程。就是“对路标点拍摄后，得到图像中的像素”的过程。具体来说，根据每个时刻的相机位置，计算出各像素对应的空间点的位置，就得到了地图。 运动方程。SLAM系统中视觉里程计（VO）估计了两张图像间相机的运动，只需把相邻时刻的运动“串”起来，就构成了机器人的运动轨迹，从而解决了定位问题。 后续深入理解本篇文章对SLAM问题的数学建模有了大致的了解，然而仍需深入理解一些问题。 如何说明机器人的位置$x$是什么，即如何表达位姿。对于三维空间中的机器人来说，其运动要由3个轴上的平移，以及绕着3个轴的旋转来描述，一共有6个自由度。那是否意味着随便用一个$ℝ^6$中的向量就可以描述它呢？其实并没那么简单。对6自由度的位姿（包括了旋转和平移），如何表达的问题涉及到三维空间刚体运动的问题，会在其他文章中详细介绍； 如何对位姿进行估计和优化。因为在SLAM中位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题，一种经典的解决方式是把它们构成一个优化问题，求解最优的$R$和$t$，使得误差最小化。而旋转矩阵自身是带有约束的（正交且行列式为1）.它们作为优化变量时，会引入额外的约束，使优化变得困难。通过李群-李代数间的转换关系，我们希望把位姿估计变成无约束的优化问题，以简化求解方式。涉及到李群和李代数会在后续记录； 视觉SLAM中观测方程如何参数化。即空间中的路标点是如何投影到一张照片上的，这就需要解释相机的成像模型； 如何求解运动、观测方程。需要用到非线性优化的知识。 SLAM问题的求解思路从应用的角度来看，SLAM问题涉及很多方面，包括传感器的选择，对$x$的估计方式，地图的表示形式等。从传感器的角度来讲，SLAM依赖的传感器主要包括激光传感器和视觉传感器两大类。从X的估计方式来讲，SLAM的求解思路主要包括基于滤波的求解和基于非线性优化的求解。目前普遍认为非线性优化的方法要由于滤波方法。从地图的表示形式来讲，SLAM得到的地图表示方式主要分为度量地图与拓扑地图，度量地图有二维，三维，稀疏，稠密等多种表现形式。 视觉SLAM经典框架 传感器获取。在视觉SLAM中主要为相机图像信息的读取和预处理。 视觉里程计（Visual Odometry，VO）。任务是估算相邻图像间相机的运动，以及局部地图的样子，（恢复场景的空间结构）。又称前端（Front End）。为了定量地估计相机运动，必须在了解相机与空间点的几何关系后进行。视觉里程计估计了两张图像间的相机运动之后，只要把相邻时刻的运动“串”起来，就构成了机器人的运动轨迹，从而解决了定位问题。另一方面，根据每个时刻的相机位置，计算出各像素对应的空间点的位置，就得到了地图。仅仅通过视觉里程计估计轨迹，将不可避免地出现累计漂移（Accumulatin Drift），将导致无法建立一致的地图。为了解决漂移问题，需要用到后端优化和回环检测，回环检测负责把“机器人回到原始位置”的事情检测出来，后端优化则根据该信息，校正整个轨迹的形状。 后端优化（Optimization）。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。由于接在VO之后，又称为后端（Back End）。笼统地说，它主要是指处理SLAM过程中噪声的问题，即考虑如何从带有噪声的数据中，估计整个系统的状态，以及这个状态估计的不确定性有多大——称为最大后验概率估计（Maximum-a-Posteriori，MAP），这里的状态既包括机器人自身的轨迹，也包括地图。前端为后端提供待优化的数据，以及这些数据的初始值。而后端负责整体的优化过程，往往面对的只有数据，不必关心这些数据到底来自什么传感器。视觉SLAM中，前端和计算机视觉研究领域更为相关，比如图像的特征提取和匹配等，后端则主要是滤波与非线性优化算法。 回环检测（Loop Closing）。回环检测判断机器人是否曾经到达过先前的位置。如果检测到回环，它会把信息提供给后端处理。又称闭环检测（Loop Closure Detection），主要解决位置估计随时间漂移的问题。为了实现回环检测，需要让机器人具有识别曾到达过的场景的能力，如果通过机器人通过相机获取的图像来完成这一任务，就可以采取判断图像间相似性的方法。视觉回环检测实质上是一种计算图像数据相似性的算法。 建图（Mapping）。它根据估计的轨迹，建立与任务要求对应的地图。建图并没有一个固定的形式和算法，例如地图可以是一组空间点的集合，也可以是一个漂亮的3D模型。大体上分为度量地图和拓扑地图两种，前者更精确，后者则更强调地图元素之间的关系。拓扑地图是一个图，由节点和边组成，只考虑节点间的连通性，而不考虑节点间到达的过程，放松了地图对精确位置的需要，去掉了地图的细节问题，更为紧凑，但不擅长表达具有复杂结构的地图。 有待研究问题及主要挑战 拓扑地图不擅长表达具有复杂结构的地图，如何对地图进行分割形成节点和边，又如何使用拓扑地图进行导航和路径规划，有待研究。 视觉SLAM中，产业化过程中仍需要解决一些关键性难题，比如，如何高效地获得尽可能长而且准确的特征轨迹并将多视频序列之间的复杂回路闭合起来， 如何对于海量图像/视频数据在有限的内存下进行高效的全局优化，如何在动态环境下进行鲁棒的同时定位与地图构建， 如何处理相机快速运动和强旋转， 如何在线动态调整重建的三维几何表面。 如何对回环序列或多序列进行处理，如何高效率、高精度处理大尺度环境，如何处理动态环境，如何处理快速移动和剧烈旋转的情况？ Covisibility 是一直在用的概念，而Essential Graph是orbslam自己提出的概念，为了减小全局回环的计算量。当你自己实现SLAM时，也会碰到这些困难，并设计一些应对的策略，这些就是你的创新性。事实上，随着SLAM时间的增长，如何控制图的结构和优化的规模，仍是现在SLAM有待解决的一个问题。 研究热点 在复杂场景中的三维视觉感知问题，主要包含三个方面：一是如何通过视觉和 IMU 融合，进行滑动窗口内的 Bundle Adjustment，来提高运动估计的准确性、稳定性和鲁棒性；二是如何基于深度学习的方法仅用单目相机来构建稠密的 3D 地图，并用于飞控避障、 AR 虚实融合等；三是如何基于单目视觉惯导融合，来实时跟踪动态物体，并恢复物体的绝对物理尺度。 语义SLAM、激光SLAM 深度学习与SLAM结合，与新的传感器进行融合 应用领域：自动驾驶、AR、VR、无人机、无人车、扫地机器人 热门公司和部门 商汤科技 Momenta 镭神智能 图森未来 纵目科技 地平线 旷视科技 阿里巴巴达摩院AI Labs 参考资料 《视觉SLAM十四讲》第2讲 一索哥传奇《SLAM的基本概念》]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视觉SLAM十四讲阅读笔记一-三维空间刚体运动]]></title>
    <url>%2F2018%2F08%2F04%2F%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[这篇文章是视觉SLAM十四讲第3讲阅读过程中总结和记录的学习内容。 三维空间中的刚体（相机、机器人）不光有位置，还有姿态，即位姿，位置表示相机在空间中哪个地方，姿态表示相机的指向。SLAM中一个很基本的问题就是计算机器人在三维空间中的位姿。而机器人在三维空间中位姿的计算往往与三维空间中的刚体运动有关。本篇是学习《视觉SLAM十四讲》第3讲三维空间刚体运动有关记录，参考了童博士的笔记内容。介绍在三维空间中刚体运动的位姿表示方法。 位姿=位置+姿态SLAM中的位姿有6个自由度，即位置和姿态，其变换过程包含了旋转（Rotation）和平移（Translation）。直观地理解就是，对于在平面中运动的机器人，其位姿的变换由两个位置和一个转角来描述，转角描述旋转过程，两个位置描述平移过程。机器人任意两个时间间隔位置和转角的变化都可以通过运动传感器检测得到。这里涉及到SLAM中一个很重要的问题，即位姿的表达、优化。 位置表示 准确地说应该是机器人运动过程中位置变化的表示。 在一个三维空间中，建立三维坐标系之后，就可以用一个三维坐标来表示机器人的位置。对于机器人的位置变换，可以用一个三维的平移向量来表示，比如机器人从初始位置为$(x_0,y_0,z_0)$经过一个平移向量$t=(a,b,c)$，可以得到平移后的位置$(x_0+a,y_0+b,z_0+c)$。相对来说，机器人的位置表达比较简单，只需要一个平移向量即可。 姿态表示 准确地说应该是机器人运动过程中姿态变化的表示。 在一个三维空间中，通常用一个三维向量来表示机器人的姿态。更直观的讲，机器人的姿态可以想象成机器人自带一个坐标系，这个坐标系的原点就是机器人，z轴表示机器人面向的方向，这样的一个坐标系可以通过表示机器人姿态的三维向量来构造。 机器人的姿态表达相对复杂，表达方法通常有旋转矩阵、旋转向量、欧拉角、四元数。 旋转矩阵$SO(3)$在SLAM问题中，旋转矩阵是表示姿态变换最常用的方式。假设空间中的某一点$P$，机器人旋转前的坐标系的单位正交基为$(e1,e2,e3)$，$P$的坐标为$(x_1,y_1,z_1)$。。机器人旋转后的坐标系的单位正交基为$(e′_1,e′_2,e′_3)$，$P$的坐标为$(x′_1,y′_1,z′_1)$。 根据坐标的定义有： $\left[ \begin{matrix}e_1&amp;e_2&amp;e_3\end{matrix} \right] \left[ \begin{matrix}a_1\a_2\a_3\\end{matrix} \right]=[e′_1 e′_2 e′_3]\left [\begin{matrix}a′_1\a′_2\a′_3\\end{matrix} \right]$ 在等式的左右同时左乘$\left[ \begin{matrix}a^T_1\a^T_2\a^T_3\\end{matrix} \right]​$，可得 $\left[ \begin{matrix}a1\a2\a3\\end{matrix} \right]=\left[ \begin{matrix}e^T_1e′_1 &amp; e^T_1e′_2 &amp;e^T_1e′_3\ e^T_2e′_1 &amp; e^T_2e′_2 &amp; e^T_2e′_3 \e^T_3e′_1&amp;e^T_3e′_2&amp;e^T_3e′_3 \end{matrix} \right]\left [\begin{matrix}a′_1\a′_2\a′_3\\end{matrix} \right]=Ra′​$ 矩阵$R$即为旋转矩阵。旋转矩阵是一个行列式为1的正交矩阵。同样所有行列式为1的正交矩阵都是旋转矩阵，因此可以把旋转矩阵做如下定义： $SO(n)={Rϵℝ^{n×n}∣RR^T=I,det(R)=1}$ $SO(n)$称为特殊正交群，这个集合由$n$维空间的旋转矩阵组成，特别的，$SO(3)$就是三维空间下的旋转矩阵。 值得注意的是，旋转矩阵本质上是表示两个坐标系之间的旋转，而坐标系恰恰能够表示机器人姿态，因此旋转矩阵可以描述机器人(相机)的旋转，即姿态变换。 变换矩阵$SE(3)$在旋转矩阵的基础之上，加上平移向量，就可以完整的刻画三维空间中的刚体运动，即：$a′=Ra+t$。该式用一个旋转矩阵$R$和一个平移向量$t$完整地描述了一个欧式空间的坐标变换关系。 但是这种形式下会存在一个问题，假设我们进行了两次变换$R_1$，$t_1$和$R_2$，$t_2$。相应的三维空间中经历了从a点到b点到c点的变换。则满足公式： $b=R{1}a+t_1,c=R{2}b+t_2$ 从a到c的变换为：$c=R2(R{1}a+t_1)+t_2$ 这样的形式在变换多次之后会过于复杂。聪明的数学家引入了齐次坐标和变换矩阵的概念，使得三维空间中的刚体运动有如下的变换形式： $\left[ \begin{matrix}a′\1\end{matrix} \right]=\left[ \begin{matrix}R&amp;t\0^T&amp;1\end{matrix} \right]\left [\begin{matrix}a\1\\end{matrix} \right]=T\left [\begin{matrix}a\1\\end{matrix} \right]$ 矩阵$T$即称为变换矩阵。 此时从a到c的变换可表示为：$c=T_2T_1a$ 此处的a和c为相应齐次坐标。 变换矩阵的左上角为旋转矩阵，右侧为平移向量，左下角为0向量，右下角为1。这种矩阵称为特殊欧式群(Special Euclidean Group)： $SE(3)={T=\left[ \begin{matrix}R&amp;t\0^T&amp;1\end{matrix} \right]ϵℝ^{4×4}∣RϵSO(3),tϵℝ^3}$ 齐次坐标透视变换是非线性变换，可以通过引入齐次坐标以线性表示透视变换。齐次坐标是在普通坐标上增加一维（值为1）后的坐标表示。在是三维向量的末尾添加1，将其变成了四维向量，多了一个自由度，允许把变换写成线性的形式。对于四维向量，就可以把平移和旋转写在一个矩阵里面，使得整个关系变成线性关系。 将普通坐标转换为齐次坐标：增加一维坐标，值为1。 (x,y) \Rightarrow \left[ \begin{matrix} a \\ b \\ 1 \\ \end{matrix} \right] (像点齐次坐标) (x,y,z) \Rightarrow \left[ \begin{matrix} x \\ y \\ z \\ 1\\ \end{matrix} \right] (物点齐次坐标) 齐次坐标转换为普通坐标：除以最后一维坐标。 \left[ \begin{matrix} x \\ y \\ z \\ w \\ \end{matrix} \right] \Rightarrow \left[ \begin{matrix} x/w, & y/w, & z/w\\ \end{matrix} \right] 齐次坐标是缩放不变的（Invariant to scaling） k\left[ \begin{matrix} x \\ y \\ w \\ \end{matrix} \right] = \left[ \begin{matrix} kx \\ ky \\ kw \\ \end{matrix} \right] \Rightarrow \left[ \begin{matrix} \frac{kx}{kw} \\ \frac{ky}{kw} \\ \end{matrix} \right] = \left[ \begin{matrix} \frac {x}{w}\ \\ \frac{y}{w} \\ \end{matrix} \right] 旋转向量用旋转矩阵来表示旋转有两个缺点： $SO(3)$的旋转矩阵有九个量，但一次旋转只有三个自由度。因此这种表达方式是冗余的。 旋转矩阵自身两个约束，即必须是正交矩阵和行列式为1，这些约束会使得求解变得更困难。 从直观上讲，任意旋转都可以用一个旋转轴和一个旋转角来刻画。于是可以使用一个向量，其方向与旋转轴一致，而长度等于旋转角。这种向量称为旋转向量（或轴角，Axis-Angle），这样只需要一个三维向量就可以描述旋转。旋转向量跟之后要介绍的李代数是相对应的。旋转向量到旋转矩阵的变换可由罗德里格斯公式求得。假设旋转轴为$n$，角度为$θ$，则旋转向量为$θn$对应的旋转矩阵$R$为： $R=cosθI+(1−cosθ)nn^T+sinθ[n]^\wedge$ 反之，对于转角$\theta$有： $tr(R)=cos\theta tr(I)+(1-cos\theta)tr(nn^T)+sin\theta tr(n^\wedge)\=3cos\theta+(1-cos\theta)\=1+2cos\theta$ 因此： $θ=arccos(\frac{tr(R)−1}2)$ 由于旋转轴上的向量在旋转后不发生改变，说明：$Rn=n$ 因此转轴$n$是矩阵$R$特征值1对应的特征向量。求解此方程，再归一化，就得到了旋转轴。 欧拉角欧拉角是一种最为直观的姿态变换描述方式。在欧拉角的表示方式中，将旋转分解成沿三个坐标轴旋转的量：滚转角－俯仰角－偏航角(roll-pitch-yaw)，此时可以使用$[r,p,y]^T$这样一个三维向量描述任意旋转。 欧拉角的一个重大缺点就是万向锁问题：在俯仰角为±90度时，第一次旋转与第三次旋转将使用同一个轴，这被称为奇异性问题。由于欧拉角不适于插值和迭代，所以在SLAM问题中通常不使用欧拉角来表示旋转，所以不再展开记录。 四元数旋转矩阵具有冗余性，欧拉角和旋转向量不冗余但是具有奇异性。其实，我们找不到不带奇异性的三维向量描述方式。四元数只有四个自由度，即是紧凑的而且没有奇异性。它是一种扩展的复数的表达方式。缺点是不够直观，运算稍复杂。 四元数q拥有一个实部和三个虚部。如下： $q=q_0+q_1i+q_2j+q_3k$ 其中： $\left{ \begin{array}{ll} i^2=j^2=k^2=−1\ ij=k,ji=−k \jk=i,kj=−i,ki=j,ik=−j\end{array} \right.​$ 我们可以用单位四元数表示三维空间中的任意一个旋转。 假设某个旋转绕单位向量$n=[n_x,n_y,n_z]^T$进行了角度$θ$的旋转，那么对应的四元数为： $q=[cos\fracθ2,n_xsin\fracθ2,n_ysin\fracθ2,n_zsin\fracθ2]$ 反之也可以从单位四元数中计算出对应旋转轴和夹角： $\left{ \begin{array}{ll} θ=2arccosq_0\ [n_x,n_y,n_z]^T=\frac{[q_1,q_2,q_3]^T}{sin\fracθ2}\end{array} \right.$ 对$θ$加上$2π$可以得到一个相同的旋转，但此时对应的四元数变成了$−q$。因此任意的旋转都可以由两个互为相反数的四元数表示，即两个互为相反数的四元数可以表示同一个旋转。 四元数的运算见《视觉SLAM十四讲》P55，不再做记录。 四元数表示旋转假设空间中有一点$p=[x,y,z]$，，其绕旋转轴$n$，进行了角度为$θ$的旋转得到了点$p′$，使用旋转矩阵表述有： $p′=Rp$。 使用四元数描述旋转该如何表达？首先，把三维空间点用一个虚四元数表示： $p=[0,x,y,z]=[0,v]$ 相当于把四元数的3个虚部与空间中的3个轴相对应。然后，由前述公式可知，用四元数$q$表示该旋转： $q=[cos\fracθ2,n_xsin\fracθ2,n_ysin\fracθ2,n_zsin\fracθ2]$ 则可以验证：$p′=qpq^{−1}$ 最终结果实部为0,故为纯虚四元数，其虚部的3个分量表示旋转后的3D点的坐标。 四元数到旋转矩阵的转换设四元数$q=q_0+q_1i+q_2j+q_3k$，对应的旋转矩阵$R$为： $R=\left[ \begin{array}{ccc}1-2q^2_2-2q^2_3&amp;2q_1q_2-2q_0q_3&amp;2q_1q_3+2q_0q_2\2q_1q_2+2q_0q_3&amp;1-2q^2_1-2q^2_3&amp;2q_2q_3-2q_0q_1\2q_1q_3-2q_0q_2&amp;2q_2q_3+2q_0q_1&amp;1-2q^2_1-2q^2_2\end{array} \right]$ 反之，假设旋转矩阵为$R={m_{ij}},i,j\epsilon[1,2,3]$，其对应的四元数$q$为： $q0=\frac{\sqrt{tr(R)+1}}{2},q_1=\frac{m{23}-m{32}}{4q_0},q_2=\frac{m{31}-m{13}}{4q_0},q_3=\frac{m{12}-m_{21}}{4q_0}​$ 注意：实际编程中，当$q_0$的值接近0时，其它3个分量会非常大，导致解不稳定，此时可以再考虑使用其他方式进行转换。 旋转的四种表示方法比较 表示方法 形式 是否奇异性 分量数目 紧凑or冗余程度 直观性 旋转矩阵 $R$ 否 9 冗余 不直观 旋转向量 $\theta{n}$ 是 3 较紧凑 不直观 欧拉角 $[r,p,y]^T$ 是 3 较紧凑 直观 四元数 $q=q_0+q_1i+q_2j+q_3k$ 否 4 较紧凑 较不直观 旋转的四种表示方法间互相转换1.矩阵转换成其他方式 旋转矩阵 $R={m_{ij}},i,j\epsilon[1,2,3]$ 旋转向量 $θ=arccos(\frac{tr(R)−1}2)$；求解方程$Rn=n$，归一化处理得到旋转轴n 欧拉角 - 四元数 $q0=\frac{\sqrt{tr(R)+1}}{2},q_1=\frac{m{23}-m{32}}{4q_0},q_2=\frac{m{31}-m{13}}{4q_0},q_3=\frac{m{12}-m_{21}}{4q_0}$ 2.向量转换成其他方式 旋转向量 $θn$ 旋转矩阵 $R=cosθI+(1−cosθ)nn^T+sinθ[n]^\wedge$ 欧拉角 - 四元数 $q=[cos\fracθ2,n_xsin\fracθ2,n_ysin\fracθ2,n_zsin\fracθ2]$ 3.数转换成其他方式 四元数 $q=q_0+q_1i+q_2j+q_3k$ 旋转矩阵 $R=\left[ \begin{array}{ccc}1-2q^2_2-2q^2_3&amp;2q_1q_2-2q_0q_3&amp;2q_1q_3+2q_0q_2\2q_1q_2+2q_0q_3&amp;1-2q^2_1-2q^2_3&amp;2q_2q_3-2q_0q_1\2q_1q_3-2q_0q_2&amp;2q_2q_3+2q_0q_1&amp;1-2q^2_1-2q^2_2\end{array} \right]$ 旋转向量 $\left{ \begin{array}{ll} θ=2arccosq_0\ [n_x,n_y,n_z]^T=\frac{[q_1,q_2,q_3]^T}{sin\fracθ2}\end{array} \right.$ 欧拉角 - 参考资料 《视觉SLAM十四讲》第二讲、第三讲 一索哥传奇-《三维空间中的刚体运动》]]></content>
      <categories>
        <category>机器人</category>
        <category>SLAM</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SLAM基础</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMake学习之查找链接库--find_package使用方法]]></title>
    <url>%2F2018%2F08%2F03%2FCMake%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9F%A5%E6%89%BE%E9%93%BE%E6%8E%A5%E5%BA%93-find-package%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[这篇文章是有关CMake中使用find_package指令的内容。 如果编译软件使用了外部库，事先并不知道它的头文件和链接库的位置。得在编译命令中加上包含它们的查找路径。CMake使用find_package命令来解决这个问题。本文讨论了如何在CMake项目中使用外部库，即find_package()的工作原理。 FIND_PACKAGEFIND_PACKAGE( &lt;name&gt; [version][EXACT] [QUIET][NO_MODULE] [ [ REQUIRED | COMPONENTS ][ componets... ] ] ) 用来调用预定义在 CMAKE_MODULE_PATH 下的 Find&lt;name&gt;.cmake模块。也可以自己定义Find&lt;name&gt;模块，将其放入工程的某个目录中，通过SET(CMAKE_MODULE_PATH dir)设置查找路径，供工程FIND_PACKAGE使用。 这条命令执行后，CMake 会到变量 CMAKE_MODULE_PATH 指示的目录中查找文件 Find&lt;name&gt;.cmake并执行。 version参数：需要一个版本号，它是正在查找的包应该兼容的版本号。 EXACT选项：要求版本号必须精确匹配。如果在find-module内部对该命令的递归调用没有给定[version]参数，那么[version]和EXACT选项会自动地从外部调用前向继承。对版本的支持目前只存在于包和包之间（详见下文）。 QUIET 参数：会禁掉包没有被发现时的警告信息。对应于Find&lt;name&gt;.cmake模块中的 NAME_FIND_QUIETLY。 REQUIRED 参数：其含义是指是否是工程必须的，表示如果报没有找到的话，cmake的过程会终止，并输出警告信息。对应于Find&lt;name&gt;.cmake模块中的 NAME_FIND_REQUIRED 变量。 COMPONENTS参数：在REQUIRED选项之后，或者如果没有指定REQUIRED选项但是指定了COMPONENTS选项，在它们的后面可以列出一些与包相关（依赖）的部件清单（components list） 示例： FIND_PACKAGE( libdb_cxx REQUIRED) 这条命令执行后，CMake 会到变量 CMAKE_MODULE_PATH 指示的目录中查找文件 Findlibdb_cxx.cmake 并执行。 包查找是如何工作的 find_package() 命令会在模块路径中寻找Find&lt;name&gt;.cmake ，这是查找库的一个典型方式。首先CMake查看${CMAKE_MODULE_PATH}中的所有目录，然后再查看它自己的模块目录&lt;CMAKE_ROOT&gt;/share/cmake-x.y/Modules/（$CMAKE_ROOT的具体值可以通过CMake中message命令输出）。这称为模块模式。 如果没找到这样的文件，在~/.cmake/packages/或/usr/local/share/中的各个包目录中查找，寻找&lt;库名字的大写&gt;Config.cmake或者&lt;库名字的小写&gt;-config.cmake(比如库Opencv，它会查找/usr/local/share/OpenCV中的OpenCVConfig.cmake或opencv-config.cmake)。这称为配置模式。配置模式的文件的编写见 这里的文档 。可能还会用到 importing and exporting targets 这篇文档。 不管使用哪一种模式，只要找到包，就会定义下面这些变量： 1234&lt;NAME&gt;_FOUND&lt;NAME&gt;_INCLUDE_DIRS or &lt;NAME&gt;_INCLUDES&lt;NAME&gt;_LIBRARIES or &lt;NAME&gt;_LIBRARIES or &lt;NAME&gt;_LIBS&lt;NAME&gt;_DEFINITIONS 这些都在 Find&lt;name&gt;.cmake文件中。 找到NAME包后，变量NAME_INCLUDE_DIRS中将包括指定NAME库头文件的查找路径。变量NAME_LIBRARY_DIRS中将包含指定NAME库的.a或.so文件的所在目录的路径。 参考链接： http://blog.csdn.net/bytxl/article/details/50637277 https://blog.csdn.net/u011092188/article/details/61425924]]></content>
      <categories>
        <category>工具</category>
        <category>CMake</category>
      </categories>
      <tags>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coding.net git]]></title>
    <url>%2F2018%2F08%2F02%2Fcoding-net-git%2F</url>
    <content type="text"><![CDATA[这篇文章是有关git相关指令的内容。 Git Directory：使用Git管理的一个目录，也就是一个仓库，包含我们的工作空间和Git的管理空间。 WorkSpace：需要通过Git进行版本控制的目录和文件，这些目录和文件组成了工作空间。 .git：存放Git管理信息的目录，初始化仓库的时候自动创建。 Index/Stage：暂存区，或者叫待提交更新区，在提交进入repo之前，我们可以把所有的更新放在暂存区。 Local Repo：本地仓库，一个存放在本地的版本库；HEAD会只是当前的开发分支（branch）。 Stash：是一个工作状态保存栈，用于保存/恢复WorkSpace中的临时状态。 命令12345678910111213git init #创建本地仓库，自动产生'.git'隐藏文件git status #查看workspace状态git add filename or git add . #将文件或更新放到暂存区git commit -m "message" #提交更新，从暂存区到本地仓库，-m后是描述信息git commit -a -m "message" #跳过git add步骤提交更新，修改某个文件后提交可以执行该命令git diff #显示workspace与暂存区的差异git diff HEAD~n #显示workspace与本地仓库的差异git remote #显示已经配置的远程仓库服务器git remote -v #显示远程仓库服务器简写及对应的URLgit remote add shortname url #添加远程仓库git push shortname branchname #推送数据到远程仓库git log #查看提交历史git log -p -2 #-p显示每次提交的内容差异，-2显示最近两次提交 ubuntu git远程仓库管理：https://blog.csdn.net/maclechan/article/details/44964439 详细的git原理及使用 ：https://git-scm.com/book/zh/v2 简易指南：http://www.bootcss.com/p/git-guide/]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像处理之基础知识]]></title>
    <url>%2F2018%2F06%2F21%2F%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[这篇文章是图像处理相关的基础知识学习记录。 尺度空间对于一副图像，近距离观察和远距离观察效果是不同的，前者比较清晰、比较大，能看到图像的一些细节信息；后者比较模糊、比较小，能看到图像的一些轮廓的信息，这就是图像的尺度，图像的尺度是自然存在的，并不是人为创造的。图像不同的尺度在一起称为尺度空间，量化表示即（o，s），变量o是某一八度（octave），控制的是金字塔中尺寸这个尺度，s是该八度中的某一层，区分同一尺寸尺度下的图像，控制一个八度中不同的模糊程度。尺度是在二维图像的基础上得到的图像中自然存在的一个维度。因为高斯核是唯一的线性核，也就是说使用高斯核对图像模糊不会引入其他噪声，因此就选用了高斯核来构建图像的尺度。高斯卷积核是尺度变换的唯一的线性核。 多尺度和多分辨率金字塔多分辨率金字塔是早期图像多尺度的表示形式，一个图像金字塔是一系列图像的集合，所有图像来源于同一张原始图像 ，通过对原始图像连续采样获得，直到达到某个终止条件才停止采样。 有两种类型的图像金字塔常常出现在文献和应用中： 高斯金字塔(Gaussian pyramid): 用来向下采样，为一层一层的图像，层级越高，图像越小。 拉普拉斯金字塔(Laplacian pyramid): 用来从金字塔低层图像向上采样重建图像，高斯金字塔其逆形式。 图像金字塔化一般包括两个步骤：使用低通滤波器平滑图像（高斯平滑/高斯核模糊？一个八度内进行不同的高斯核模糊？）；对平滑图像进行降采样（通常是水平，竖直方向1/2），从而得到一系列尺寸缩小的图像。对于二维图像，一个传统的金字塔中，每一层图像由上一层分辨率的长、宽各一半，也就是四分之一的像素组成。 尺度空间表达和金字塔多分辨率表达之间最大的不同是： 尺度空间表达是由不同高斯核平滑卷积得到，在所有尺度上有相同的分辨率； 而金字塔多分辨率表达每层分辨率减少固定比率。 所以，金字塔多分辨率生成较快，且占用存储空间少；而多尺度表达随着尺度参数的增加冗余信息也变多。 多尺度表达的优点在于图像的局部特征可以用简单的形式在不同尺度上描述；而金字塔表达没有理论基础，难以分析图像局部特征。 图像的上采样(up-sampling)和下采样(down-sampling)缩小图像（或称为下采样（subsampled）或降采样（downsampled））的主要目的有两个： 使得图像符合显示区域的大小； 生成对应图像的缩略图。 放大图像（或称为上采样（upsampling）或图像插值（interpolating））的主要目的是放大原图像，从而可以显示在更高分辨率的显示设备上。对图像的缩放操作并不能带来更多关于该图像的信息，因此图像的质量将不可避免地受到影响。然而，确实有一些缩放方法能够增加图像的信息，从而使得缩放后的图像质量超过原图质量的。 下采样原理：对于一幅图像I尺寸为M N，对其进行s倍下采样，即得到(M/s) (N/s)尺寸的得分辨率图像，当然s应该是M和N的公约数才行，如果考虑的是矩阵形式的图像，就是把原始图像s * s窗口内的图像变成一个像素，这个像素点的值就是窗口内所有像素的均值。 上采样原理：图像放大几乎都是采用内插值方法，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之roslaunch]]></title>
    <url>%2F2018%2F05%2F24%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Broslaunch%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中roslaunch使用的学习内容。 wiki官方文档roslaunch命令：http://wiki.ros.org/roslaunch/Commandline%20Tools launch文件格式：http://wiki.ros.org/roslaunch/XML 介绍rosrun只能运行一个node，roslaunch可以同时运行多个nodes。roslaunch工具是ros中python实现的程序启动工具，可以通过读取启动文件（launch file）中的参数配置、属性配置等，同时启动节点管理器（master）和多个节点，在启动任何一个节点前，roslaunch 将会确定 roscore节点（节点管理器） 是否已经在运行，如果没有，自动启动它；可以在本地或者远程（使用SSH）启动ROS节点，要通过参数服务器设置参数。 roscore会做三件事： 启动master节点，该节点是隐藏的，用于通过消息名查询目标节点，实现消息、服务在各个节点之间的连接 启动参数服务器parameter server，用于设置与查询参数 启动日志节点，记录所有消息收发和stdout、stderr，目前roscore暂不会加入其他功能 任务名称 任务功能 特性 master 通过消息名查询目标节点，实现消息、服务在各个节点之间的连接 隐藏 parameter server 设置与查询参数 - 日志节点 记录所有消息收发和stdout、stderr - 任何包含两个或两个以上节点的系统都可以利用启动文件来指定和配置需要使用的节点。通常的命名方案是以.launch作为启动文件的后缀，启动文件是XML文件。一般把启动文件存储在取名为launch的目录中。 roslaunch命令执行launch文件的命令格式： 1roslaunch [package] [filename.launch] launch文件编写一般格式： 123456789101112&lt;launch&gt; &lt;node .../&gt; &lt;rosparam ..../&gt; &lt;param .../&gt; &lt;include .../&gt; &lt;env .../&gt; &lt;remap .../&gt; &lt;arg .../&gt; &lt;group&gt; ... &lt;/group&gt;&lt;/launch&gt; &lt;launch&gt;...&lt;/launch&gt;：根元素，作为放置其他元素的容器，其他元素必须在该标记之间。 &lt;node .../&gt;：标记用于定义一个希望启动的ROS节点。格式： 1&lt;node name="bar1" pkg="foo_pkg" type="bar" /&gt; 三个必须的属性：pkg, type, name。 pkg是节点所在的程序包名字； type是节点的类型，是可执行文件的名字； name是节点名字，不能包含namespace，可以任意给出的，它覆盖了原有文件中ros::init指定的node name； 在默认状态下，从启动文件启动节点的标准输出被重定向到一个日志文件中，而不是像 rosrun 命令那样，将 log 信息显示在终端(console)。该日志文件的名称是：~/.ros/log/run_id/node_name-number-stout.log 其中，run_id 是节点管理器（master）启动时生成的一个唯一标示符； 如果需要将标准输出信息输出到终端，使用属性output，即output=screen； 其他属性：args（可以通过命令行启动参数赋值，将参数传递给节点）、ns（节点定义为某个namespace下）等。 &lt;param .../&gt;：定义一个设置在参数服务器中的参数，该标记可以放在&lt;node .../&gt;标记内部，作为私有参数。格式： 1&lt;param name="publish_frequency" type="double" value="10.0" /&gt; name是参数名，可以给出参数所在namesapce；value是可选属性，用于定义参数值，省略时要通过其他方式（命令行或者binfile、testfile文件）指定参数值；type定义参数的数据类型，也是可选属性，省略时roslaunch会尝试自动定义参数的类型（根据参数的形式进行判断）。 &lt;include .../&gt;：允许当前launch文件包含（调用）其他launch文件，包括该文件中的所有nodes和parameters。格式： 1&lt;include file=”$(find package_name)/launch_file_name”/&gt; 需要写出该launch文件的绝对路径，比较繁琐，一般使用上述方式。roslaunch会搜索package下的所有子目录；因此，必须给出package_name。另外，include也支持ns属性，将它的内容放进指定的namespace，格式： 1&lt;include file=”...” ns=”namespace_name”/&gt; &lt;arg .../&gt;：启动参数，是局部的，只能在一个launch文件中使用，类似于局部变量，声明格式： 1&lt;arg name=”arg_name”&gt; launch文件中的每个argument必须有指定值。指定argument的值有多种方式，包括命令行赋值、声明argument时赋值。 还支持启动参数，有时也简称为参数甚至args。 命令行赋值。命令格式： 1roslaunch package_name launch_file_name arg_name:=arg_value 声明时赋值（两种形式）。格式： 12&lt;arg name=”arg_name” default=”arg_name”/&gt;&lt;arg name=”arg_name” value=”arg_name”/&gt; 两种形式的区别在于，命令行参数可以覆盖default，但是不能重写value的值。 可以通过arg获取变量的值，格式： 1$(arg arg_name) 另外，还可以将变量值传给included launch文件，格式： 1234&lt;include file=”path-to-file”&gt; &lt;arg name=”arg_name” value=”arg_value”/&gt; ...&lt;/include&gt; 若在launch文件中，launch文件及其包含的launch文件出现出现相同的arguments，则需在launch文件及included launch文件中同时写： 1&lt;arg name=”arg_name” value=”$(arg arg_name)”/&gt; 第一个arg_name表示included launch文件中的argument，第二个arg_name表示launch文件中的argument。其结果是指定的argument在launch文件及included launch文件中都有相同的值。 在ROS中prarmeter和argument是不同的，虽然翻译一样。parameter是运行中的ROS系统使用的数值，存储在参数服务器（parameter server）中，每个活跃的节点都可以通过 ros::param::get 函数来获取parameter的值，用户也可以通过rosparam来获得parameter的值。而argument只在启动文件内才有意义，它们的值是不能被节点直接获取的。 &lt;remap .../&gt;：重映射。重映射是基于替换的思想，每个重映射包含一个原始名称和一个新名称。每当节点使用重映射中的原始名称时，ROS客户端库就会将它默默地替换成其对应的新名称。例如，运行一个 turtlesim 的实例，如果想要把海龟的姿态数据发布到话题/tim 而不是/turtle1/pose，就可以使用如下命令： 1rosrun turtlesim turtlesim_node turtle1/pose:=tim 通过启动文件的方式，只需在启动文件内使用重映射（remap）元素即可： 1&lt;remap from=”turtle1/pose” to ”tim”/&gt; 例如，节点mono订阅了/camera/image_raw话题，但是现在只有/camera_node/image_raw话题在发布和/camera/image_raw话题一样的数据，可以使用重映射完成数据的订阅，这样就可以使得节点mono能够订阅/camera_node/image_raw话题的数据： 1&lt;remap from="/camera/image_raw" to="/camera_node/image_raw"/&gt; &lt;group&gt; ... &lt;/group&gt;：可以将指定的nodes组织起来，只能使用ns、if、unless三个属性。group有两个作用/好处： 可以将几个nodes放进同一个namespace，从而使该组标签有独立的名称空间，格式： 12345&lt;group ns=”namespace”&gt; &lt;node pkg=”..” .../&gt; &lt;node pkg=”..” .../&gt; ......&lt;/group&gt; 如果grouped node已经有它自己的namespace，并且是relative name，那么该node的namespace是其relative name，并以group namespace为后缀。 可以同时启动或者终止一组nodes，格式： 123456&lt;group if=”$(arg arg_name)”&gt; ......&lt;/group&gt;&lt;group unless=”$(arg arg_name)”&gt; ......&lt;/group&gt; 其中arg_name的值只有0或1，若真，则包含group标签，其中的nodes都可运行；否则其中的nodes都不会运行。​ ​ 参考链接： https://blog.csdn.net/fengmengdan/article/details/42984429（启动文件的编写） http://www.cnblogs.com/zjiaxing/p/5542614.html（启动文件的编写） https://www.cnblogs.com/zjiaxing/p/5541841.html（ros命名空间的解释）]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>roslaunch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中使用YAML语言]]></title>
    <url>%2F2018%2F05%2F23%2FC%2B%2B%E4%B8%AD%E4%BD%BF%E7%94%A8YAML%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++中使用YAML语言的基础，以及在结合ROS和OpenCV使用的学习内容。 YAML语言使用基础YAML语言在C++中，可以用于应用程序相关配置文件的保存，可读性、可修改性比较好。直接贴代码。 测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231#include &lt;fstream&gt;#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;assert.h&gt;#include "yaml-cpp/yaml.h"using namespace std;//=============================================================================void test1()&#123; YAML::Node config = YAML::LoadFile("site.yaml"); std::cout&lt;&lt;"here test1!!"&lt;&lt;endl; YAML::Node config_systemLog = config["systemLog"]; if ( config_systemLog["path"] ) std::cout &lt;&lt; config_systemLog["path"] &lt;&lt; std::endl;//输出c:\data\log\mongod.log std::ofstream fout( "test1.yaml" ); fout &lt;&lt; config;&#125; //=============================================================================void test2()&#123; YAML::Node node; node["username"] = "glimix"; node["password"] = "111222"; std::ofstream fout( "test2.yaml" ); fout &lt;&lt; node;&#125; //=============================================================================void test3()&#123; try &#123; YAML::Node doc = YAML::LoadFile( "Night.jpg.meta" ); std::cout &lt;&lt; doc &lt;&lt; "\n"; &#125; catch( const YAML::Exception &amp;e ) &#123; std::cerr &lt;&lt; e.what() &lt;&lt; "\n";//读取文件异常 &#125;&#125; //=============================================================================void test4()&#123; YAML::Node node; node.push_back( "glimix" ); node.push_back( 123 ); node.push_back( 3.1415926 ); node["char"].push_back( 'a' ); node["bool"].push_back( true ); std::ofstream fout( "test4.yaml" ); fout &lt;&lt; node;&#125; //=============================================================================void test5()&#123; class Vector3 &#123; public: float x, y, z; void encode( YAML::Node &amp;node ) &#123; node.push_back( x ); node.push_back( y ); node.push_back( z ); &#125; bool decode( YAML::Node &amp;node ) &#123; if ( !node.IsSequence() || node.size() != 3 ) return false; x = node[0].as&lt;float&gt;(); y = node[1].as&lt;float&gt;(); z = node[2].as&lt;float&gt;(); return true; &#125; &#125;; Vector3 pos; pos.x = 100.0f; pos.y = -45.0f; pos.z = 50.0f; YAML::Node node; pos.encode( node ); std::ofstream fout( "test5.yaml" ); fout &lt;&lt; node; Vector3 pos2; pos2.decode( node );&#125; //=============================================================================void test6()&#123; YAML::Node node; node["name"] = "John Smith"; node["age"] = 37; YAML::Node node2; node2["name"] = "Jane Smith"; node2["age"] = 25; YAML::Node node3; node3["name"] = "Jimmy Smith"; node3["age"] = 15; YAML::Node node4; node4["name"] = "Jenny Smith"; node4["age"] = 12; node["spouse"] = node2; node["children"].push_back( node3 ); node["children"].push_back( node4 ); node["children"].push_back( "&#123;name: Alex Smith, age: 14&#125;" ); node["children"].push_back( "name: Alex Smith, age: 14" ); node["children"].push_back( YAML::Load( "&#123;name: Alex Smith, age: 14&#125;" ) ); YAML::Node node5 = YAML::Load( "&#123;name: Alex Smith, age: 14&#125;" ); node["children"].push_back( node5 ); std::ofstream fout( "test6.yaml" ); fout &lt;&lt; node;&#125; //=============================================================================void test7()&#123; YAML::Node node; // starts out as null node["key"] = "value"; // it now is a map node node["seq"].push_back("first element"); // node["seq"] automatically becomes a sequence node["seq"].push_back("second element"); node["mirror"] = node["seq"][0]; // this creates an alias node["seq"][0] = "1st element"; // this also changes node["mirror"] node["mirror"] = "element #1"; // and this changes node["seq"][0] - they're really the "same" node node["self"] = node; // you can even create self-aliases node[node["mirror"]] = node["seq"]; // and strange loops 🙂 std::ofstream fout( "test7.yaml" ); fout &lt;&lt; node;&#125;//=============================================================================void test8()&#123; YAML::Node primes = YAML::Load("[2, 3, 5, 7, 11]"); for (std::size_t i=0;i&lt;primes.size();i++) &#123; std::cout &lt;&lt; primes[i].as&lt;int&gt;() &lt;&lt; "\n"; &#125; // or:// for (YAML::const_iterator it=primes.begin();it!=primes.end();++it) // &#123;// std::cout &lt;&lt; it-&gt;as&lt;int&gt;() &lt;&lt; "\n";// &#125; primes.push_back(13); assert(primes.size() == 6); std::ofstream fout("test8.yaml"); fout &lt;&lt; primes;&#125;//=============================================================================void test9()&#123; YAML::Node lineup = YAML::Load("&#123;1B: Prince Fielder, 2B: Rickie Weeks, LF: Ryan Braun&#125;"); for(YAML::const_iterator it = lineup.begin(); it != lineup.end(); ++it) &#123; std::cout &lt;&lt; "Playing at " &lt;&lt; it-&gt;first.as&lt;std::string&gt;() &lt;&lt; " is " &lt;&lt; it-&gt;second.as&lt;std::string&gt;() &lt;&lt; "\n"; &#125; lineup["RF"] = "Corey Hart"; lineup["C"] = "Jonathan Lucroy"; assert(lineup.size() == 2);//assert(表达式)保证满足特定条件，即表达式为真，否则整个程序退出并输出一条错误信息。 std::ofstream fout("test9.yaml"); fout &lt;&lt; lineup;&#125;//=============================================================================void test10()&#123; YAML::Node node = YAML::Load("&#123;name: Brewers, city: Milwaukee&#125;"); if (node["name"]) &#123; std::cout &lt;&lt; node["name"].as&lt;std::string&gt;() &lt;&lt; "\n"; &#125; if (node["mascot"]) &#123;//无输出 std::cout &lt;&lt; node["mascot"].as&lt;std::string&gt;() &lt;&lt; "\n"; &#125; assert(node.size() == 2); // the previous call didn't create a node&#125;//=============================================================================void test11()&#123; YAML::Node node1 = YAML::Load("[1, 2, 3]");//序列结构 assert(node1.Type() == YAML::NodeType::Sequence); assert(node1.IsSequence()); // a shortcut! YAML::Node node2 = YAML::Load("&#123;name: Brewers, city: Milwaukee&#125;");//散列/map结构 switch (node2.Type()) &#123; case YAML::NodeType::Null: cout&lt;&lt;"NodeType:NULL"&lt;&lt;endl; break; case YAML::NodeType::Scalar: cout&lt;&lt;"NodeType:Scalar"&lt;&lt;endl; break; case YAML::NodeType::Sequence: cout&lt;&lt;"NodeType:Sequence"&lt;&lt;endl; break; case YAML::NodeType::Map: cout&lt;&lt;"NodeType:Map"&lt;&lt;endl; break;//输出为Map case YAML::NodeType::Undefined: cout&lt;&lt;"NodeType:Undefined"&lt;&lt;endl; break; &#125;&#125;//=============================================================================int main(int argc ,char **argv)&#123; test11(); return 0; &#125; 测试输出文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#test1systemLog: destination: file path: c:\data\log\mongod.logstorage: dbPath: c:\data\dbsecurity: authorization: enablednet: bindIp: 127.0.0.1 port: 27017 #test2username: glimixpassword: 111222#test40: glimix1: 1232: 3.1415926char: - abool: - true #test5- 100- -45- 50#test6name: John Smithage: 37spouse: name: Jane Smith age: 25children: - name: Jimmy Smith age: 15 - name: Jenny Smith age: 12 - "&#123;name: Alex Smith, age: 14&#125;" - "name: Alex Smith, age: 14" - &#123;name: Alex Smith, age: 14&#125; - &#123;name: Alex Smith, age: 14&#125;#test7&amp;1key: valueseq: &amp;2 - &amp;3 "element #1" - second elementmirror: *3self: *1*3: *2#test8[2, 3, 5, 7, 11, 13]#test9&#123;1B: Prince Fielder, 2B: Rickie Weeks, LF: Ryan Braun, RF: Corey Hart, C: Jonathan Lucroy&#125; site.yaml文件（注意：冒号和其后内容之间有一空格）： 12345678910systemLog: destination: file path: c:\data\log\mongod.logstorage: dbPath: c:\data\dbsecurity: authorization: enablednet: bindIp: 127.0.0.1 port: 27017 CMakeLists.txt文件： 12345678910111213# 使用c++11编译add_compile_options(-std=c++11)cmake_minimum_required(VERSION 2.6)project(test)find_package(yaml-cpp REQUIRED)Message("YAML_INCLUDE:$&#123;YAML_CPP_INCLUDE_DIR&#125;")include_directories($&#123;PROJECT_SOURCE_DIR&#125; $&#123;YAML_CPP_INCLUDE_DIR&#125;)set(LIBS $&#123;YAML_CPP_LIBRARIES&#125;)add_executable(test main.cpp)target_link_libraries(test $&#123;LIBS&#125;) 结合ROS+OpenCV的使用在许多ROS项目中使用YAML文件作为配置文件可以简化程序、提高代码的可读性和可修改性，在结合OpenCV的一些API，可以很方便地对配置信息进行修改，而不需要修改程序。下面是结合ROS+OpenCV读取YAML文件的一小段示例代码（未运行）。 12345678910111213141516171819#include &lt;ros/ros.h&gt;#include &lt;ros/package.h&gt; //ros::packgae#include &lt;opencv2/opencv.hpp&gt; //cv::FileStorageint main(int argc, char** argv)&#123; ros::init(argc, argv, "node_name"); string packagePath = ros::package::getPath("package_name"); //使用ROS包名读取包的地址 string configPath = packagePath + "//config//config_name.yaml"; //YAML文件 cv::FileStorage fsSettings(configPath, cv::FileStorage::READ); //cv::FileStorage适用于XML/YAML/JSON文件 if(!fsSettings.isOpened()) &#123; cerr &lt;&lt; "ERROR: Wrong path to settings" &lt;&lt; endl; return -1; &#125; string config_variable_name = fsSettings["config_variable_name"]; //... return 0;&#125; YAML文件： 123config_variable_name: valuestr_name: "value"... 参考资料https://github.com/jbeder/yaml-cpp/wiki/Tutorial http://www.glimix.com/archives/1570 http://yaml.org/]]></content>
      <categories>
        <category>语言</category>
        <category>YAML</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>ROS</tag>
        <tag>YAML</tag>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习之基础知识]]></title>
    <url>%2F2018%2F05%2F13%2FDocker%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[这篇文章是有关Docker基础知识的内容。 官方文档：https://docs.docker.com/install/linux/docker-ce/ubuntu/ 学习到这啦：Containers share your image：https://docs.docker.com/get-started/part2/#share-your-image Dockerfile：https://docs.docker.com/engine/reference/builder/#usage Docker在乌班图系统上支持overlay2和aufs两种存储驱动。并且默认使用overlay2，如果需要使用aufs，需要手动配置。 Docker是一个为开发者和系统管理员提供的平台，允许使用容器来开发、部署、运行应用。 Docker的文件系统分为两层：bootfs和rootfs。 Docker采用AUFS分层文件系统时，文件系统的改动都是发生在最上面的容器层。在容器的生命周期内，它是持续的，包括容器在被停止后。但容器被删除后，该数据层也随之被删除了。因此Docker使用volume（卷）的形式来向容器提供持久化存储。Docker使用UnionFS搭建的分层镜像： 概念 容器（container）：是一个镜像的实例，通过运行一个镜像启动。它是一个镜像的运行时实例，这时，执行的镜像位于内存中。可以使用docker ps命令查看运行中的容器清单。运行着的容器有一个可写层（writable或称为容器层container layer），位于底下的若干只读层之上，运行时的所有变化，包括对数据和文件的写和更新，都会保存在这个层中。因此，从同一个镜像运行的多个容器包含了不同的容器层。 镜像（image）：是一个可执行包，包括执行一个应用程序的所有代码、运行时、库、环境变量以及配置文件。镜像是轻便的，它由Dockerfile定义。镜像是文件系统数据的复制，只允许读，它包括一个或多个只读层（read-only layers），一旦被创建就无法修改。 仓库（repository）：镜像的集合，其中的代码已经编译完成。 registry：仓库的集合。默认使用Docker的公共registry，也可以自己设置私人registry。 Swarm：一些运行Docker并且加入到cluster中的机器的集合。 Docker命令12345678docker --version #查看Docker版本docker info #查看Docker安装有关的所有细节信息docker version #查看Docker安装有关的所有细节信息docker image ls #列出镜像清单docker container ls #列出容器清单（列出运行中的容器）docker container ls --all #列出容器清单（列出所有容器）docker container ls --aq #列出容器清单（列出所有容器，简单模式，只有容器ID）docker run hello-world #执行Docker镜像，镜像名字为hello-world 分层结构 Stack：一组有关联的服务的组合，可以编排在一起，一起管理。 Services：一个应用的不同部分。伸缩一个服务就是改变这一个服务的运行的容器的数量。 Container Flocker：容器的分布式存储平台原生的 Docker volume 不具备可移植性。于是，出现了Docker 的分布式卷解决方案 Flocker。Flocker的结构： 参考文章：http://www.cnblogs.com/sammyliu/p/5932996.html]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之工作空间、软件包的创建和编译]]></title>
    <url>%2F2018%2F05%2F10%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%88%9B%E5%BB%BA%E5%B7%A5%E4%BD%9C%E7%A9%BA%E9%97%B4%E3%80%81%E8%BD%AF%E4%BB%B6%E5%8C%85%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中工作空间、软件包创建以及编译的内容。 创建工作空间123456mkdir -p work_space/srccd work_spacecatkin_makesource devel/setup.bashecho $ROS_PACKAGE_PATH #查看环境变量包含内容：/home/.../work_space/src:/opt/ros/kinetic/share:/opt/ros/kinetic/stacks 创建程序包12cd work_space/srccatkin_create_pkg my_package std_msgs rospy roscpp 编译程序包1234cd work_spacesource /opt/ros/kinetic/setup.bashcatkin_makecatkin_make install #可选 提示：如果工作空间下包很多，每次都使用catkin_make的话效率十分低下，因为这种编译方法会编译工作空间下的所有的包，特别是在调试程序过程中会经常修改CMakeLists.txt文件里的内容，这样每次修改都要编译整个工作空间。所以可以使用ROS的catkin_make的功能编译一个或者多个包，具体的命令是： catkin_make -DCATKIN_WHITELIST_PACKAGES=&quot;package_name&quot;) 例如：catkin_make -DCATKIN_WHITELIST_PACKAGES=&quot;ros_slam&quot; 如果需要编译两个或者多个只需要中间加分号即可： catkin_make -DCATKIN_WHITELIST_PACKAGES=&quot;ros_slam;cv_bridge&quot;]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之智能指针]]></title>
    <url>%2F2018%2F05%2F09%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++智能指针的学习内容。 C++11之前auto_ptr C++11shared_ptrunique_ptrweak_ptr C++14make_sharedmake_unique]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之explicit关键字详解]]></title>
    <url>%2F2018%2F05%2F09%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8Bexplicit%E5%85%B3%E9%94%AE%E5%AD%97%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++ explicit学习的内容。 作用和使用方法explicit关键字可以阻止不应该允许的经过转换函数进行的隐式转换的发生，声明为explicit的构造函数不能在隐式转换中使用。 explicit关键字只能用于修饰只有一个参数的类构造函数，它的作用是表明该构造函数是显式的，而非隐式的，跟它相对应的另一个关键字是implicit，意思是隐藏的，类构造函数默认情况下即声明为implicit(隐式)。 注意：当类的声明和定义分别在两个文件中时，explicit只能写在在声明中，不能写在定义中。 理解由于在C++中， 一个参数的构造函数(或者除了第一个参数外其余参数都有默认值的多参构造函数)， 承担了两个角色。一 是构造；二是默认且隐含的类型转换操作符。所以，如果写下如AAA = XXX这样的代码， 且恰好XXX的类型正好是AAA单参数构造器（构造函数）的参数类型， 这时候编译器就自动调用这个构造器， 创建一个AAA的对象。这种隐式转换在某些情况下， 违背了程序员的本意。 这就要在该构造器前加上explicit修饰， 指定该构造器只能被明确的调用/使用， 不能作为类型转换操作符被隐含的使用。 示例程序： 12345678910111213141516171819202122232425#include &lt;iostream&gt; using namespace std; class Test1 &#123; public : Test1(int num):n(num)&#123;&#125; private: int n; &#125;; class Test2 &#123; public : explicit Test2(int num):n(num)&#123;&#125; private: int n; &#125;; int main() &#123; Test1 t1 = 12; //1 通过 Test2 t2(13); //2 通过 Test2 t3 = 14; //3 此行会编译错误，提示：无法从“int”转换为“Test2” return 0; &#125; 1处编译通过，是因为C++中，如果构造函数只有一个参数，在编译时就会有一个缺省的转换操作：将该构造函数对应数据类型的数据转换为该类对象。也就是说 Test1 t1 = 12;这段代码，编译器自动将整型转换为Test1类对象，实际上等同于下面的操作： 1Test1 t1(12); 或者： 12Test1 temp(12);Test1 t1 = temp; 3处编译不通过，就是因为Test3类的定义中，构造函数前面加了explicit关键字，阻止了类构造函数的隐式自动转换。 参考链接： 1、http://www.cnblogs.com/ymy124/p/3632634.html 2、https://blog.csdn.net/tianmingdyx/article/details/79823470]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMake编译调试]]></title>
    <url>%2F2018%2F04%2F29%2FCMake%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[这篇文章是有关CMake编译调试相关指令的内容。 CMake编译时如果需要选择c++编译器版本，需要添加：add_compile_options(-std=c++11)。 如果需要编译出共享库而不是静态库，执行cmake命令时使用cmake .. -DBUILD_SHARED_LIBS=ON。 使用CMake编译的程序，也可以使用gdb进行调试。方法如下。 CMakeList.txt文件前面添加内容：123SET(CMAKE_BUILD_TYPE "Debug") SET(CMAKE_CXX_FLAGS_DEBUG "$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb")SET(CMAKE_CXX_FLAGS_RELEASE "$ENV&#123;CXXFLAGS&#125; -O3 -Wall") 重新编译在build目录下执行编译命令： 12cmake .. -DCMAKE_BUILD_TYPE=Debugmake 启动调试1gdb exe #exe为可执行文件 调试程序具体的调试方法，参考https://blog.csdn.net/haoel/article/details/2879/]]></content>
      <categories>
        <category>工具</category>
        <category>CMake</category>
      </categories>
      <tags>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe的CPU模式安装]]></title>
    <url>%2F2018%2F04%2F21%2FCaffe%E7%9A%84CPU%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[这篇文章是有关Caffe CPU模式安装的内容。 在跑师兄的程序时，有一个场景检测模块用到Caffe深度学习框架库，可惜自己的台式机显卡太菜，装不了CUDA。只能尝试使用Caffe的CPU模式。Caffe在计算时有两种模式可以选择，CPU或GPU，使用GPU处理图像速度会更快。 检查是否有NVIDIA显卡1lspci | grep -i nvidia 如果没有显示内容，说明电脑没有nvidia显卡。如果输出相关显卡及版本信息，说明可以使用GPU模式。 安装依赖包12345678910sudo apt-get install libprotobuf-dev sudo apt-get install libleveldb-devsudo apt-get install libsnappy-dev sudo apt-get install libopencv-devsudo apt-get install libhdf5-serial-devsudo apt-get install protobuf-compilersudo apt-get install libgflags-devsudo apt-get install libgoogle-glog-devsudo apt-get install liblmdb-devsudo apt-get install libatlas-base-dev 下载Caffe安装git 1sudo apt-get install git 克隆Caffe 1git clone git://github.com/BVLC/caffe.git 编译Caffe进入Caffe目录1cd caffe/ 生成Makefile.config文件将caffe目录下自带的Makefile.config.example文件复制一份并更名为Makefile.config，命令如下： 1cp Makefile.config.example Makefile.config 修改Makefile.config文件 取消CPU_ONLY := 1行的注释，设置为CPU模式 配置引用文件路径 INCLUDE_DIRS新增内容：/usr/include/hdf5/serial LIBRARY_DIRS新增内容：/usr/lib/x86_64-linux-gnu/hdf5/serial 编译需要在caffe目录下新建build目录，命令如下： 123456mkdir buildcd buildcamke ..sudo make allsudo make testsudo make runtest 编译成功的话，就会显示若干个用例执行成功。 可以执行sudo make clean撤销执行。 本人开始并没有新建build目录，直接开始make的。但是出现了类似如下的错误： 123Error: 'make all' 'make test'.build_release/lib/libcaffe.so: undefined reference to cv::imread(cv::String const&amp;, int)' .build_release/lib/libcaffe.so: undefined reference tocv::imencode(cv::String const&amp;, cv::_InputArray const&amp;, std::vector &gt;&amp;, std::vector &gt; const&amp;)' 网上有很多解决方式，本人试过都没有效果，最后按照上述过程编译就不会有这个错误。 参考文章： https://blog.csdn.net/u010193446/article/details/53259294 https://www.cnblogs.com/empty16/p/4828476.html?utm_source=tuicool&amp;utm_medium=referral]]></content>
      <categories>
        <category>深度学习</category>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu一些方便的命令]]></title>
    <url>%2F2018%2F04%2F21%2Fubuntu%E4%B8%80%E4%BA%9B%E6%96%B9%E4%BE%BF%E7%9A%84%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ubuntu下一些常用命令行指令的内容。 查看opencv的版本1pkg-config --modversion opencv 查看显卡信息1lspci |grep VGA 显示设备信息1lspci 查看ubuntu系统版本1lsb_release -a 查看内核版本1uname -a 或 1cat /proc/version 查看nvidia显卡信息12nvidia-sminvidia-settings]]></content>
      <categories>
        <category>工具</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>cmake</tag>
        <tag>vscode</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之复制构造函数]]></title>
    <url>%2F2018%2F04%2F16%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%A4%8D%E5%88%B6%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++复制构造函数的学习内容。 构造函数类的每个对象之间的区别有：外在区别为对象的名称，内在区别是对象自身的属性值，即数据成员的值。在定义对象的同时进行的数据成员设置，称为对象的初始化。 构造函数的作用就是在对象被创建时利用特定的值构造对象，将对象初始化为一个特定的状态。构造函数也是一类成员函数，在对象被创建时将被自动调用。调用时无须提供参数的构造函数称为默认构造函数，类中没有写构造函数，编译器会自动生成一个隐含的默认构造函数，其参数列表和函数体皆为空。 构造函数可以直接访问类的所有数据成员，可以是内联函数（inline声明的函数），可以带有参数列表，可以带默认的形参值，可以重载。 例子： 1234567891011121314151617class Clock&#123; public: Clock(int newH, int newM, int newS);//构造函数 Clock()&#123; //重载构造函数 hour = 0; minute = 0; second = 0; &#125; void setTime(int newH, int newM, int newS); void showTime();&#125;//其他函数实现略int main()&#123; Clock c1(0, 0, 0);//调用有参数的构造函数 Clock c2; //调用无参数的构造函数 ...&#125; 复制构造函数复制构造函数是一种特殊的构造函数，具有一般构造函数的所有特性，其形参是本类对象的引用。其作用是使用一个已经存在的对象（由复制构造函数的参数指定），去初始化同类的一个新对象。 可以根据实际需要定义复制构造函数，以实现同类对象之间的数据成员的传递，如果不定义类的复制构造函数，系统会在必要时（需要使用复制构造函数时，如下三种情况）自动生成一个隐含的复制构造函数，其功能是将初始值对象的每个数据成员的值都复制到新建立的对象中。 复制构造函数被调用的情况： 当用类的一个对象去初始化该类的另一个对象时 当函数的形参是类的对象，调用函数时，进行形参和实参的结合时 注意：只有把对象用值传递时，才会调用复制构造函数，如果传递引用，则不会调用复制构造函数，所以传递比较大的对象时，传递引用会比传递值的效率高很多。 但是如果是值传递，在形参复制到实参会调用复制构造函数，如果允许复制构造函数传值，就会在复制构造函数内调用复制构造函数，形成永无休止的递归调用从而导致栈溢出。因此，C++的标准不允许复制构造函数传值参数，也就是只能传递引用（这样还可以避免无谓的消耗，提高代码的效率），在复制构造函数内部不会改变传入的对象的状态，所以可以使用常量引用。即ClassA(const ClassA&amp; other)。 当函数的返回值是类的对象，函数执行完成返回调用者时 注意：由于在被调用函数内部定义的变量只在被调用函数作用域起作用，所以调用函数完成后，要返回的对象就会消亡。所以，这种情况系统会在主函数中创建一个无名临时对象，其生存期只在函数调用所处的表达式中。函数返回时会自动调用复制构造函数将返回值的值传给临时对象，使用临时对象完成赋值或输出等操作。 例子： 1234567891011121314151617181920212223242526272829class Point&#123; public: Point(int xx=0,int yy=0)&#123; //构造函数 x=xx; y=yy; &#125; Point(const Point &amp;p)&#123; //复制构造函数，最好使用常量引用，不允许使用值传递的形式Point(Point p) x=p.x; y=p.y; &#125; int getX()&#123;return x;&#125; int getY()&#123;return y;&#125; private: int x,y; &#125;void fun(Point p)&#123; cout &lt;&lt; p.getX()&lt;&lt;&lt;endl;&#125;Point g()&#123; Point a(1,2); return a;&#125;int main()&#123; Point a(1,2); Point b(a); //1 Point c = a; //1 f(a); //2 Point x = g();//3&#125; 如果只是直接将原对象的数据成员值一一赋值给新对象的相应数据成员，其实没有必要再编写复制构造函数，只需使用隐含的默认构造函数足以。 但是，编写复制构造函数可以实现这样的操作，即在进行对象的复制时，只是有选择、有变化地进行复制。 另外，当类的数据成员中有指针类型时，默认复制构造函数实现的只能是浅复制，这会带来数据安全方面的隐患。要实现正确的复制，即深复制，必须编写复制构造函数。 浅复制：用一个对象初始化另一个对象时，只复制了成员，并没有复制资源，使两个对象同时指向了同一资源的复制方式称为浅复制。但是析构函数又在对象生命周期结束后释放资源，势必会两次返还资源，就会导致编译器报错。 深复制：即当一个对象创建时，分配了资源，这时必须显示定义复制构造函数，这种在用一个对象初始化另一个对象时，不仅复制了成员，也复制了资源的复制方式称为深复制。 复制构造函数又被叫做拷贝构造函数（copy constructor），复制初始化也叫做拷贝初始化。 拷贝构造函数和拷贝初始化不要混淆。拷贝初始化过程要调用拷贝构造函数，但拷贝构造函数也可以用在直接初始化过程。 详情参见：explicit关键字、构造函数与对象初始化]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之STL sort排序]]></title>
    <url>%2F2018%2F04%2F12%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8BSTL%20sort%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++ STL sort的学习内容。 std::sort函数是C++ STL中自带的排序函数，该函数对给定区间所有元素进行排序。函数原型为： 123456//default (1) 使用了函数模板template &lt;class RandomAccessIterator&gt; void sort (RandomAccessIterator first, RandomAccessIterator last);//custom (2) template &lt;class RandomAccessIterator, class Compare&gt; void sort (RandomAccessIterator first, RandomAccessIterator last, Compare comp); 用法 需引用#include &lt;algorithm&gt;、using namespace std； 使用类似快速排序的方法，时间复杂度为n*logn； 默认的sort函数有两个参数，也可以使用三个参数： 参数first是要排序的数组的起始地址 参数last是数组结束的地址（最后一位要排序的地址），排序范围：[first, last) 参数comp是排序的方法，返回值为bool类型，可以升序或降序。可省略，此时默认的排序方法是升序。 举例123456789101112131415161718192021222324252627282930313233343536/ sort algorithm example#include &lt;iostream&gt; // std::cout#include &lt;algorithm&gt; // std::sort#include &lt;vector&gt; // std::vectorbool myfunction (int i,int j) &#123; return (i&lt;j); &#125; //升序bool myfunction１ (int i,int j) &#123; return (i&gt;j); &#125;//降序struct myclass &#123; bool operator() (int i,int j) &#123; return (i&lt;j);&#125;//运算符重载????&#125; myobject;int main () &#123; int myints[] = &#123;32,71,12,45,26,80,53,33&#125;; std::vector&lt;int&gt; myvector (myints, myints+8); // 32 71 12 45 26 80 53 33 // using default comparison (operator &lt;): std::sort (myvector.begin(), myvector.begin()+4); //(12 32 45 71)26 80 53 33 // using function as comp std::sort (myvector.begin()+4, myvector.end(), myfunction); // 12 32 45 71(26 33 53 80) // using function as comp std::sort (myvector.begin()+4, myvector.end(), myfunction1); // 12 32 45 71(80 53 33 26) // using object as comp std::sort (myvector.begin(), myvector.end(), myobject); //(12 26 32 33 45 53 71 80) // print out content: std::cout &lt;&lt; "myvector contains:"; for (std::vector&lt;int&gt;::iterator it=myvector.begin(); it!=myvector.end(); ++it) std::cout &lt;&lt; ' ' &lt;&lt; *it; std::cout &lt;&lt; '\n'; return 0;&#125; 另一个例子12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;struct Edge&#123; int vertexStart; //边连接的一个顶点编号 int vertexEnd; //边连接另一个顶点编号 int vertexWeight; //边的权值 friend bool operator&lt;(const Edge&amp; E1, const Edge&amp; E2)//友元函数 &#123; return E1.vertexWeight &lt; E2.vertexWeight; &#125;&#125;;// bool operator&lt;(const Algorithm::Edge&amp; E1, const Algorithm::Edge&amp; E2)// &#123;// return E1.vertexWeight &lt; E2.vertexWeight;// &#125;// // bool compare_edge(const Algorithm::Edge&amp; E1, const Algorithm::Edge&amp; E2)// &#123;// return E1&lt;E2;// &#125;int main(int argc ,char **argv)&#123; vector&lt;Edge&gt; edge; edge.assign(10, Edge()); for (int i = 0; i &lt; 10; i++) edge[i].vertexStart = edge[i].vertexEnd = edge[i].vertexWeight = i; sort(edge.begin(), edge.end()); //sort(edge.begin(), edge.end(), compare_edge); return 1; &#125; 可以使用在结构体（或者是类）声明的友元函数运算符重载函数，这时调用两个参数的sort函数即可； 也可以在结构体定义外面单独声明运算符重载函数、比较函数，要使用三个参数的sort函数，第三个参数传入比较函数。]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之基础概念]]></title>
    <url>%2F2018%2F04%2F11%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++基础概念的记录内容。 重写 子类重写父类的方法，覆盖原有方法 重载 分为方法重载和运算符重载 重用 代码服用/软件复用 重构 调整程序代码改善软件质量和性能 继承 抽象/接口类（具体类） 虚函数 纯虚函数 多态性 动态绑定 模板 函数模板 类模板]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之tf]]></title>
    <url>%2F2018%2F04%2F04%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Btf%2F</url>
    <content type="text"><![CDATA[这篇文章是ROS tf有关的学习内容。 概述一个机器人系统有很多三维的坐标系，随时都在发生变化。tf可以对所有的坐标系进行实时监控，以供查询坐标系以及坐标系与坐标系之间的关系和关系的变化情况。rf可以在分布式系统中操作，意味着一个机器人的所有的坐标信息都能提供给系统中的任何主机上面的所有ROS组件。 tf没有转换信息的中央服务器。 tf默认坐标系都是右手坐标系，x轴向前、y轴向左、z轴向上。 任何用户使用tf基本上都会有两个任务，即监听变换和广播变换。 使用tf必须要监听变换 监听变换：接收并缓存系统中广播的所有坐标系，在坐标系中查找特定变换。 如果希望扩展机器人的能力，需要开启广播变换： 广播变换：发送坐标系的相关位姿到系统的其他部分。一个系统就可以有很多广播者，每一个都会提供机器人不同部位的信息。 translation：平移 rotation：旋转 Transform(ations)：变换 数据类型 Type tf Quaternion tf::Quaternion Vector tf::Vector3 Point tf::Point Pose tf::Pose Transform tf::Transform 其中四元数tf::Quaternion可以通过固定轴的Roll, Pitch and Yaw(滚动，俯仰和偏转)构造。 tf工具可以使用tf工具查看相关的信息。 view_framesrosrun tf view_frames：查看正在广播的坐标系的关系图，会生成frames.pdf，evince frames.pdf查看图。 rqt_tf_treerosrun rqt_tf_tree rqt_tf_treeorrqt &amp;：查看ROS中正在广播的坐标系的树形图。 tf_echorosrun tf tf_echo [reference_frame][target_frame]：查看ROS中广播的任意两个坐标系之间的变换关系。 参考资料 wiki官方文档 wiki官方Tutorials ROS与C++入门教程 坐标系统-古月居 关于TF转换信息(Transforms)的理解—推荐阅读]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之nodelet]]></title>
    <url>%2F2018%2F04%2F04%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Bnodelet%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中nodelet使用的学习内容。 nodelet包提供了一种可在同一进程中运行多个算法，并在算法之间进行零拷贝传输的方法。该包提供了实现nodelet所需的nodelet基类以及用于实例化nodelet的NodeletLoader类。 Threading Model一个nodelet管理器有一个线程池，它在管理器中运行的所有节点上共享。在nodelet中运行的代码中有两种可能的线程API。 默认线程模型对所有回调都有一个线程。 还有一个多线程API。Nodelet将在NodeletManager内运行。 nodelet管理器是一个c ++程序，它被设置为监听ROS服务，并且将成为nodelet动态加载的可执行文件。 在这种情况下，我们将运行独立管理器，但在很多情况下，这些管理器将嵌入正在运行的节点中。]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之pluginlib]]></title>
    <url>%2F2018%2F04%2F03%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Bpluginlib%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS pluginlib使用的学习内容。 ROS的pluginlib程序包提供了一种使用ROS构建基础结构编写和动态加载插件的工具。为了能够工作，这些工具需要插件提供者在他们的包的package.xml中注册他们的插件。 概述pluginlib是一个C ++库，用于从ROS包中加载和卸载插件。插件是从运行时库（即共享对象，动态链接库）加载的可动态 加载的类。使用pluginlib，不需要将应用程序明确地链接到包含类的库。相反，pluginlib可以在任何时候打开包含导出类的库，而无需事先知道库或包含类定义的头文件。 插件对于扩展/修改应用程序行为而不需要应用程序源代码是很有用的。 pluginlib利用了C++多态的特性，不同的插件只要使用统一的接口（抽象基类）便可以替换使用。这样用户通过调用在插件中实现的统一的接口函数，不需要更改程序，也不需要重新编译，更换插件即可实现功能修正。 利用pluginlib编写插件的方法大致包括如下四步： 创建插件基类，定义统一接口（如果为现有接口编写插件，则跳过该步） 编写插件类，继承插件基类，实现统一接口 导出插件，并编译为动态库 将插件加入ROS系统，使其可识别和管理 贴一张自己总结的pluginlib框架图，帮助自己理解： 例子首先，假设存在一个包含多边形基类的ROS程序包（“polygon_interface_package”）。系统支持两种不同的多边形：一个是位于“rectangle_plugin”包中的矩形和一个是位于“triangle_plugin”包中的三角形。rectangle_plugin和triangle_plugin包的实现都在其package.xml文件中包含特殊的导出行，告诉rosbuild系统它们可以为polygon_interface_package包中的polygon类提供插件。实际上这些导出行作用是在ROS构建/打包系统中注册这些类。这样使用者如果希望在系统中看到所有的多边形类，它就可以运行一个简单的rospack命令查询，得到可以使用的类的清单，在这里是三角形和矩形。 提供一个插件注册/导出插件一个可以被动态加载的类必须被标记为导出类，可以使用特殊的宏PLUGINLIB_EXPORT_CLASS实现这一点。该宏可以放入任何组成插件库的源（.cpp）文件中，但通常放在导出类的.cpp文件的末尾。对于上述例子，可以在example_pkg包中创建class_list.cpp文件，并编译该文件，加入librectangle库： 123456#include &lt;pluginlib/class_list_macros.h&gt;#include &lt;polygon_interface_package/polygon.h&gt;#include &lt;rectangle_package/rectangle.h&gt;//Declare the Rectangle as a Polygon classPLUGINLIB_EXPORT_CLASS(rectangle_namespace::Rectangle, polygon_namespace::Polygon) 插件描述文件插件描述文件是一个XML文件，它以机器可读的格式存储关于插件的所有信息，包括插件所在的库、插件的名字、插件的类型等等。上述例子的插件描述文件（例如，rectangle_plugin.xml）可以写为： 1234567&lt;library path="lib/librectangle"&gt; &lt;class type="rectangle_namespace::Rectangle" base_class_type="polygon_namespace::Polygon"&gt; &lt;description&gt; This is a rectangle plugin &lt;/description&gt; &lt;/class&gt;&lt;/library&gt; 关于插件描述文件的详细信息，查看这里。 为什么需要这个文件除了代码宏之外，我们还需要这个文件来允许ROS系统自动发现、加载和推理插件。 插件描述文件也包含重要的信息，如插件的描述，它不适合在宏中使用。 使用ROS Package System注册插件为了让pluginlib可以通过所有的ROS包查询系统上的所有可用插件，每个包必须明确指定它导出的插件以及哪些包库包含这些插件。插件提供程序必须在其导出标记块内的package.xml中指向其插件描述文件。 请注意，如果有其他出口，它们都必须放在同一个出口字段中。对于上述例子，相关的内容写为： 123&lt;export&gt; &lt;polygon_interface_package plugin="$&#123;prefix&#125;/rectangle_plugin.xml" /&gt;&lt;/export&gt; 关于导出一个插件的细节学习，参考这里。 提醒：为了使上述导出命令正常工作，提供的包必须直接依赖于包含插件接口的包。 例如，rectangle_plugin的catkin / package.xml中必须包含以下行： 12&lt;build_depend&gt;polygon_interface_package&lt;/build_depend&gt;&lt;run_depend&gt;polygon_interface_package&lt;/run_depend&gt; 查询ROS包系统中的可用插件用户可以使用rospack命令查看可用的插件，例如： 1rospack plugins --attrib=plugin nav_core 其中nav_cor为包名，该命令将返回所有从nav_core包中导出的插件。 使用一个插件pluginlib在class_loader.h头文件中提供了ClassLoader类，使得可以快速方便地使用它提供的类。关于该类细节的描述参考这里。下面是一个简单的例子，使用ClassLoader类在使用多边形的代码中创建一个rectangle的实例： 123456789101112131415161718#include &lt;pluginlib/class_loader.h&gt;#include &lt;polygon_interface_package/polygon.h&gt;//... some code ...pluginlib::ClassLoader&lt;polygon_namespace::Polygon&gt; poly_loader("polygon_interface_package", "polygon_namespace::Polygon");try&#123; boost::shared_ptr&lt;polygon_namespace::Polygon&gt; poly = poly_loader.createInstance("rectangle_namespace::Rectangle"); //... use the polygon, boost::shared_ptr will automatically delete memory when it goes out of scope&#125;catch(pluginlib::PluginlibException&amp; ex)&#123; //handle the class failing to load ROS_ERROR("The plugin failed to load for some reason. Error: %s", ex.what());&#125; 注意：在使用插件时，ClassLoader不能超出范围。 所以，如果要在类中加载插件对象，请确保类加载器是该类的成员变量。 手动创建并使用一个简单的插件准备工作安装pluginlib_tutorials pkg： 1apt-get install ros-kinetic-common-tutorials catkin_ws/src目录下创建程序包： 1catkin_create_pkg pluginlib_tutorials_ roscpp pluginlib 创建基类创建文件pluginlib_tutorials_/include/pluginlib_tutorials_/polygon_base.h： 1234567891011121314151617#ifndef PLUGINLIB_TUTORIALS__POLYGON_BASE_H_#define PLUGINLIB_TUTORIALS__POLYGON_BASE_H_namespace polygon_base&#123; class RegularPolygon &#123; public: virtual void initialize(double side_length) = 0; virtual double area() = 0; virtual ~RegularPolygon()&#123;&#125; protected: RegularPolygon()&#123;&#125; &#125;;&#125;;#endif 创建插件创建文件include/pluginlib_tutorials_/polygon_plugins.h： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#ifndef PLUGINLIB_TUTORIALS__POLYGON_PLUGINS_H_#define PLUGINLIB_TUTORIALS__POLYGON_PLUGINS_H_#include &lt;pluginlib_tutorials_/polygon_base.h&gt;#include &lt;cmath&gt;namespace polygon_plugins&#123; class Triangle : public polygon_base::RegularPolygon &#123; public: Triangle()&#123;&#125; void initialize(double side_length) &#123; side_length_ = side_length; &#125; double area() &#123; return 0.5 * side_length_ * getHeight(); &#125; double getHeight() &#123; return sqrt((side_length_ * side_length_) - ((side_length_ / 2) * (side_length_ / 2))); &#125; private: double side_length_; &#125;; class Square : public polygon_base::RegularPolygon &#123; public: Square()&#123;&#125; void initialize(double side_length) &#123; side_length_ = side_length; &#125; double area() &#123; return side_length_ * side_length_; &#125; private: double side_length_; &#125;;&#125;;#endif 注册插件创建文件src/polygon_plugins.cpp： 123456#include &lt;pluginlib/class_list_macros.h&gt;#include &lt;pluginlib_tutorials_/polygon_base.h&gt;#include &lt;pluginlib_tutorials_/polygon_plugins.h&gt;PLUGINLIB_EXPORT_CLASS(polygon_plugins::Triangle, polygon_base::RegularPolygon)PLUGINLIB_EXPORT_CLASS(polygon_plugins::Square, polygon_base::RegularPolygon) 构建插件CMakeList.txt文件中添加内容： 12include_directories(include)add_library(polygon_plugins src/polygon_plugins.cpp) 使得ROS Toolchain可访问插件创建XML文件在程序包顶层目录创建文件polygon_plugins.xml： 12345678&lt;library path="lib/libpolygon_plugins"&gt; &lt;class type="polygon_plugins::Triangle" base_class_type="polygon_base::RegularPolygon"&gt; &lt;description&gt;This is a triangle plugin.&lt;/description&gt; &lt;/class&gt; &lt;class type="polygon_plugins::Square" base_class_type="polygon_base::RegularPolygon"&gt; &lt;description&gt;This is a square plugin.&lt;/description&gt; &lt;/class&gt;&lt;/library&gt; 导入插件在package.xml文件添加信息： 123&lt;export&gt; &lt;pluginlib_tutorials_ plugin="$&#123;prefix&#125;/polygon_plugins.xml" /&gt;&lt;/export&gt; 测试插件执行catkin_make命令编译工作空间，并执行如下命令： 1rospack plugins --attrib=plugin pluginlib_tutorials_ 输出 polygon_plugins.xml文件的路径信息则正确。 使用插件创建文件src/polygon_loader.cpp： 12345678910111213141516171819202122232425#include &lt;pluginlib/class_loader.h&gt;#include &lt;pluginlib_tutorials_/polygon_base.h&gt;int main(int argc, char** argv)&#123; pluginlib::ClassLoader&lt;polygon_base::RegularPolygon&gt; poly_loader("pluginlib_tutorials_", "polygon_base::RegularPolygon"); try &#123; boost::shared_ptr&lt;polygon_base::RegularPolygon&gt; triangle = poly_loader.createInstance("polygon_plugins::Triangle"); triangle-&gt;initialize(10.0); boost::shared_ptr&lt;polygon_base::RegularPolygon&gt; square = poly_loader.createInstance("polygon_plugins::Square"); square-&gt;initialize(10.0); ROS_INFO("Triangle area: %.2f", triangle-&gt;area()); ROS_INFO("Square area: %.2f", square-&gt;area()); &#125; catch(pluginlib::PluginlibException&amp; ex) &#123; ROS_ERROR("The plugin failed to load for some reason. Error: %s", ex.what()); &#125; return 0;&#125; 运行节点在 CMakeLists.txt文件中添加信息： 12add_executable(polygon_loader src/polygon_loader.cpp)target_link_libraries(polygon_loader $&#123;catkin_LIBRARIES&#125;) 执行catkin_make命令。 方式一：命令行rosrun方式运行可执行文件节点： 1rosrun pluginlib_tutorials_ polygon_loader 输出如下信息： 12[ INFO] [WallTime: 1279658450.869089666]: Triangle area: 43.30[ INFO] [WallTime: 1279658450.869138007]: Square area: 100.00 方式二：启动文件方式在程序包中创建launch文件夹，并创建文件polygon_loader.launch，输入： 123&lt;launch&gt; &lt;node name="polygon_loader" pkg="pluginlib_tutorials_" type="polygon_loader" output="screen"/&gt; &lt;/launch&gt; 执行命令：roslaunch pluginlib_tutorials_ polygon_loader.launch 可以看到同样的输出信息。 启动文件（launch file）方式，是ROS提供的一个同时启动节点管理器（master）和多个节点的途径。任何包含两个或两个以上节点的系统都可以利用启动文件来指定和配置需要使用的节点。通常的命名方案是以.launch作为启动文件的后缀，启动文件是XML文件。一般把启动文件存储在取名为launch的目录中。每个XML文件都必须要包含一个根元素。根元素由一对launch标签定义：&lt;launch&gt; … &lt;launch&gt;元素都应该包含在这两个标签之内。 节点属性中节点元素的形式： &lt;node pkg=”package-name” type=”executable-name” name=”node-name”/&gt;]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之actionlib库（４）-实践之小乌龟画五边形]]></title>
    <url>%2F2018%2F03%2F31%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Bactionlib%E5%BA%93%EF%BC%88%EF%BC%94%EF%BC%89-%E5%AE%9E%E8%B7%B5%E4%B9%8B%E5%B0%8F%E4%B9%8C%E9%BE%9F%E7%94%BB%E4%BA%94%E8%BE%B9%E5%BD%A2%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中actionlib使用的学习内容。 本例程并没有创建.action文件生成消息，在安装ROS时，在opt/ros/kinetic/路径中已经包含了我们需要的文件，其实就是一个依赖库turtle_actionlib。这一点不同就需要在CMakeList.txt和package.xml文件中手动添加一些信息，后续的内容会提到。 创建行为服务器创建文件actionlib_tutorials/src/shape_server.cpp： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143#include &lt;ros/ros.h&gt;#include &lt;turtlesim/Pose.h&gt;#include &lt;actionlib/server/simple_action_server.h&gt;#include &lt;cmath&gt;#include &lt;math.h&gt;#include &lt;angles/angles.h&gt;#include &lt;geometry_msgs/Twist.h&gt;#include &lt;turtle_actionlib/ShapeAction.h&gt;// This class computes the command_velocities of the turtle to draw regular polygons class ShapeAction&#123;public: ShapeAction(std::string name) : as_(nh_, name, false), action_name_(name) &#123; //register the goal and feeback callbacks as_.registerGoalCallback(boost::bind(&amp;ShapeAction::goalCB, this)); as_.registerPreemptCallback(boost::bind(&amp;ShapeAction::preemptCB, this)); //subscribe to the data topic of interest sub_ = nh_.subscribe("/turtle1/pose", 1, &amp;ShapeAction::controlCB, this); pub_ = nh_.advertise&lt;geometry_msgs::Twist&gt;("/turtle1/cmd_vel", 1); as_.start(); &#125; ~ShapeAction(void) &#123; &#125; void goalCB() &#123; // accept the new goal turtle_actionlib::ShapeGoal goal = *as_.acceptNewGoal(); //save the goal as private variables edges_ = goal.edges; radius_ = goal.radius; // reset helper variables interior_angle_ = ((edges_-2)*M_PI)/edges_; apothem_ = radius_*cos(M_PI/edges_); //compute the side length of the polygon side_len_ = apothem_ * 2* tan( M_PI/edges_); //store the result values result_.apothem = apothem_; result_.interior_angle = interior_angle_; edge_progress_ =0; start_edge_ = true; &#125; void preemptCB() &#123; ROS_INFO("%s: Preempted", action_name_.c_str()); // set the action state to preempted as_.setPreempted(); &#125; void controlCB(const turtlesim::Pose::ConstPtr&amp; msg) &#123; // make sure that the action hasn't been canceled if (!as_.isActive()) return; if (edge_progress_ &lt; edges_) &#123; // scalar values for drive the turtle faster and straighter double l_scale = 6.0; double a_scale = 6.0; double error_tol = 0.00001; if (start_edge_) &#123; start_x_ = msg-&gt;x; start_y_ = msg-&gt;y; start_theta_ = msg-&gt;theta; start_edge_ = false; &#125; // compute the distance and theta error for the shape dis_error_ = side_len_ - fabs(sqrt((start_x_- msg-&gt;x)*(start_x_-msg-&gt;x) + (start_y_-msg-&gt;y)*(start_y_-msg-&gt;y))); theta_error_ = angles::normalize_angle_positive(M_PI - interior_angle_ - (msg-&gt;theta - start_theta_)); if (dis_error_ &gt; error_tol) &#123; command_.linear.x = l_scale*dis_error_; command_.angular.z = 0; &#125; else if (dis_error_ &lt; error_tol &amp;&amp; fabs(theta_error_)&gt; error_tol) &#123; command_.linear.x = 0; command_.angular.z = a_scale*theta_error_; &#125; else if (dis_error_ &lt; error_tol &amp;&amp; fabs(theta_error_)&lt; error_tol) &#123; command_.linear.x = 0; command_.angular.z = 0; start_edge_ = true; edge_progress_++; &#125; else &#123; command_.linear.x = l_scale*dis_error_; command_.angular.z = a_scale*theta_error_; &#125; // publish the velocity command pub_.publish(command_); &#125; else &#123; ROS_INFO("%s: Succeeded", action_name_.c_str()); // set the action state to succeeded as_.setSucceeded(result_); &#125; &#125;protected: ros::NodeHandle nh_; actionlib::SimpleActionServer&lt;turtle_actionlib::ShapeAction&gt; as_; std::string action_name_; double radius_, apothem_, interior_angle_, side_len_; double start_x_, start_y_, start_theta_; double dis_error_, theta_error_; int edges_ , edge_progress_; bool start_edge_; geometry_msgs::Twist command_; turtle_actionlib::ShapeResult result_; ros::Subscriber sub_; ros::Publisher pub_;&#125;;int main(int argc, char** argv)&#123; ros::init(argc, argv, "turtle_shape"); ShapeAction shape(ros::this_node::getName()); ros::spin(); return 0;&#125; 创建行为客户端创建行为客户端文件actionlib_tutorials/src/shape_client.cpp： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;ros/ros.h&gt;#include &lt;actionlib/client/simple_action_client.h&gt;#include &lt;actionlib/client/terminal_state.h&gt;#include &lt;turtle_actionlib/ShapeAction.h&gt;int main (int argc, char **argv)&#123; ros::init(argc, argv, "test_shape"); // create the action client // true causes the client to spin it's own thread actionlib::SimpleActionClient&lt;turtle_actionlib::ShapeAction&gt; ac("turtle_shape", true); ROS_INFO("Waiting for action server to start."); // wait for the action server to start ac.waitForServer(); //will wait for infinite time ROS_INFO("Action server started, sending goal."); // send a goal to the action turtle_actionlib::ShapeGoal goal; goal.edges = 5; goal.radius = 1.3; ac.sendGoal(goal); //wait for the action to return bool finished_before_timeout = ac.waitForResult(ros::Duration(40.0)); if (finished_before_timeout) &#123; actionlib::SimpleClientGoalState state = ac.getState(); ROS_INFO("Action finished: %s",state.toString().c_str()); &#125; else ROS_INFO("Action did not finish before the time out."); //exit return 0;&#125; 编译行为注意： 本例程并没有创建.action文件生成消息，而是使用安装ROS时就有的opt/ros/kinetic/share/turtle_actionlib目录下的库文件，所以在CMakeLists.txt文件中需要添加如下信息： 123catkin_package( CATKIN_DEPENDS actionlib_msgs turtle_actionlib #turtle_actionlib是新添加的依赖库) 在CMakeLists.txt文件末尾添加以下几行： 12345add_executable(shape_server src/shape_server.cpp)target_link_libraries(shape_server $&#123;catkin_LIBRARIES&#125;)add_executable(shape_client src/shape_client.cpp)target_link_libraries(shape_client $&#123;catkin_LIBRARIES&#125;) 完整的CMakeList.txt文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758cmake_minimum_required(VERSION 2.8.3)project(actionlib_tutorials)find_package(catkin REQUIRED COMPONENTS actionlib actionlib_msgs message_generation roscpp rospy std_msgs)add_action_files( FILES Fibonacci.action Averaging.action DoDishes.action)generate_messages( DEPENDENCIES actionlib_msgs std_msgs)catkin_package( CATKIN_DEPENDS actionlib_msgs turtle_actionlib)include_directories(# include $&#123;catkin_INCLUDE_DIRS&#125;)add_executable(fibonacci_server src/fibonacci_server.cpp)target_link_libraries(fibonacci_server $&#123;catkin_LIBRARIES&#125;)add_executable(fibonacci_client src/fibonacci_client.cpp)target_link_libraries(fibonacci_client $&#123;catkin_LIBRARIES&#125;)add_executable(averaging_server src/averaging_server.cpp)target_link_libraries(averaging_server $&#123;catkin_LIBRARIES&#125;)add_executable(averaging_client src/averaging_client.cpp)target_link_libraries(averaging_client $&#123;catkin_LIBRARIES&#125;)add_executable(do_dishes_server src/do_dishes_server.cpp)target_link_libraries(do_dishes_server $&#123;catkin_LIBRARIES&#125;)add_executable(do_dishes_client src/do_dishes_client.cpp)target_link_libraries(do_dishes_client $&#123;catkin_LIBRARIES&#125;)add_executable(shape_server src/shape_server.cpp)target_link_libraries(shape_server $&#123;catkin_LIBRARIES&#125;)add_executable(shape_client src/shape_client.cpp)target_link_libraries(shape_client $&#123;catkin_LIBRARIES&#125;) 接着记得在package.xml文件中加入如下信息： 1&lt;build_depend&gt;turtle_actionlib&lt;/build_depend&gt;&lt;build_export_depend&gt;turtle_actionlib&lt;/build_export_depend&gt;&lt;exec_depend&gt;turtle_actionlib&lt;/exec_depend&gt; 工作空间下执行catkin_make命令。 运行行为终端启动ROS： 1roscore 运行小乌龟： 1rosrun turtlesim turtlesim_node 运行行为客户端： 1rosrun actionlib_tutorials shape_client 运行行为服务器： 1rosrun actionlib_tutorials shape_server 执行命令查看反馈信息： 1rostopic echo /turtle_shape/feedback 程序执行完并没有反馈信息输出，因为没有涉及到反馈信息。 执行命令查看结果信息： 1rostopic echo /turtle_shape/result 小乌龟画完五边形，会有信息输出： 123456789101112131415161718header: seq: 0 stamp: secs: 1522547343 nsecs: 139139044 frame_id: &apos;&apos;status: goal_id: stamp: secs: 1522547324 nsecs: 173274728 id: &quot;/test_shape-1-1522547324.173274728&quot; status: 3 text: &apos;&apos;result: interior_angle: 1.88495564461 apothem: 1.05172204971--- 小乌龟画图的效果： 执行命令rqt_graph查看节点图如下所示： 理解分析： 服务器是作为ROS中的一个节点存在的，该节点名称为/turtle_shape，它会发布消息到话题/turtle_shape/feedback、 /turtle_shape/result、/turtle_shape/status，客户端通过订阅这些话题获取到服务器执行客户端赋予的任务的完成进度信息；服务器还发布消息到/turtle1/cmd_vel话题，/turtlesim订阅了该话题，以此来控制小乌龟运动。同时，服务器订阅了turtle_shape/goal、/turtle_shape/cancel、/turtle/pose三个话题，服务器通过话题turtle_shape/goal获取到客户端发布的目标信息，通过turtle_shape/cancel话题获取到客户端发布的中断消息。 客户端也是一个节点test_shape，该节点订阅了/turtle_shape/feedback、 /turtle_shape/result、/turtle_shape/status三个话题，并发布消息到/turtle_shape/goal和/turtle_shape/cancel话题，前者使的服务器获取到客户端发布的任务目标，后者是客户端告知服务器中断任务停止执行的途径。 根据上面的节点图，笔者总结了一下actionlib SimpleAction的简单交互图，帮助自己理解。笔者理解的是feedback、result、status、goal、cancel，这些话题由于actionlib机制的存在会自动创建，并且ActionServer和ActionClient会自动发布或者订阅这些话题，这也许就是actionlib的作用了。仔细想想，其实和简单的消息发布器、接收器的核心思想是一样的。（这一点理解在官网的介绍中得到验证，“action客户端和服务端通过预定义的ROS Action协议通信，该通信机制基于ROS消息”） 如果要画其他多边形，将客户端文件中的goal对象的edges值设为相应的边数即可。]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之actionlib库（３）-使用目标回调方法编写一个简单的行为服务器]]></title>
    <url>%2F2018%2F03%2F30%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Bactionlib%E5%BA%93%EF%BC%88%EF%BC%93%EF%BC%89-%E4%BD%BF%E7%94%A8%E7%9B%AE%E6%A0%87%E5%9B%9E%E8%B0%83%E6%96%B9%E6%B3%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E8%A1%8C%E4%B8%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中actionlib使用的学习内容。 这一节接着上一节的内容，默认已经创建好程序包（和上一节公用一个程序包，即actionlib_tutorials），这里将讲述如何使用目标回调方法(Goal Callback Method)编写一个简单的行为服务器。 创建行为消息在程序包中创建文件actionlib_tutorials/action/Averaging.action： 123456789101112#goal definitionint32 samples---#result definitionfloat32 meanfloat32 std_dev---#feedbackint32 samplefloat32 datafloat32 meanfloat32 std_dev 生成消息文件添加一些内容到CMakeList.txt文件： 12345678910111213141516171819202122232425find_package(catkin REQUIRED COMPONENTS actionlib actionlib_msgs message_generation roscpp rospy std_msgs)add_action_files( FILES Fibonacci.action Averaging.action DoDishes.action)generate_messages( DEPENDENCIES actionlib_msgs std_msgs)catkin_package( CATKIN_DEPENDS actionlib_msgs) 运行： 1catkin_make # 工作空间下运行 创建行为服务器创建文件actionlib_tutorials/src/averaging_server.cpp： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include &lt;ros/ros.h&gt;#include &lt;std_msgs/Float32.h&gt;// 行为库#include &lt;actionlib/server/simple_action_server.h&gt;// 包含从Averaging.action文件中生成的消息#include &lt;actionlib_tutorials/AveragingAction.h&gt;class AveragingAction&#123;public: AveragingAction(std::string name) : as_(nh_, name, false), action_name_(name) &#123; //注册目标和反馈回调函数 as_.registerGoalCallback(boost::bind(&amp;AveragingAction::goalCB, this)); as_.registerPreemptCallback(boost::bind(&amp;AveragingAction::preemptCB, this)); //订阅感兴趣的话题数据 建立一个数据回调，该回调会处理行为 sub_ = nh_.subscribe("/random_number", 1, &amp;AveragingAction::analysisCB, this); as_.start();//行为服务器开启 &#125; ~AveragingAction(void) &#123; &#125; void goalCB() &#123; // 重置帮助变量 data_count_ = 0; sum_ = 0; sum_sq_ = 0; // 接收新目标 goal_ = as_.acceptNewGoal()-&gt;samples; &#125; void preemptCB() &#123; ROS_INFO("%s: Preempted", action_name_.c_str()); // 设置行为状态为抢占(preempted) as_.setPreempted(); &#125; void analysisCB(const std_msgs::Float32::ConstPtr&amp; msg) &#123; // 确保行为还没有被取消 if (!as_.isActive()) return; data_count_++; feedback_.sample = data_count_; feedback_.data = msg-&gt;data; //处理std_dev和数据含义 sum_ += msg-&gt;data; feedback_.mean = sum_ / data_count_; sum_sq_ += pow(msg-&gt;data, 2); feedback_.std_dev = sqrt(fabs((sum_sq_/data_count_) - pow(feedback_.mean, 2))); as_.publishFeedback(feedback_); if(data_count_ &gt; goal_) &#123; result_.mean = feedback_.mean; result_.std_dev = feedback_.std_dev; if(result_.mean &lt; 5.0) &#123; ROS_INFO("%s: Aborted", action_name_.c_str()); //设置行为状态为崩溃(aborted) as_.setAborted(result_); &#125; else &#123; ROS_INFO("%s: Succeeded", action_name_.c_str()); // 设置行为状态为成功(succeeded) as_.setSucceeded(result_); &#125; &#125; &#125;protected: ros::NodeHandle nh_; actionlib::SimpleActionServer&lt;actionlib_tutorials::AveragingAction&gt; as_; std::string action_name_; int data_count_, goal_; float sum_, sum_sq_; actionlib_tutorials::AveragingFeedback feedback_; actionlib_tutorials::AveragingResult result_; ros::Subscriber sub_;&#125;;int main(int argc, char** argv)&#123; ros::init(argc, argv, "averaging"); AveragingAction averaging(ros::this_node::getName()); ros::spin(); return 0;&#125; 创建行为客户端创建行为客户端文件actionlib_tutorials/src/averaging_client.cpp： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;ros/ros.h&gt;#include &lt;actionlib/client/simple_action_client.h&gt;#include &lt;actionlib/client/terminal_state.h&gt;#include &lt;actionlib_tutorials/AveragingAction.h&gt;#include &lt;boost/thread.hpp&gt;void spinThread()&#123; ros::spin();&#125;int main (int argc, char **argv)&#123; ros::init(argc, argv, "test_averaging"); // 创建一个行为客户端 actionlib::SimpleActionClient&lt;actionlib_tutorials::AveragingAction&gt; ac("averaging"); boost::thread spin_thread(&amp;spinThread); ROS_INFO("Waiting for action server to start."); ac.waitForServer(); ROS_INFO("Action server started, sending goal."); // 发送目标到行为 actionlib_tutorials::AveragingGoal goal; goal.samples = 100; ac.sendGoal(goal); //等待行为返回 bool finished_before_timeout = ac.waitForResult(ros::Duration(30.0)); if (finished_before_timeout) &#123; actionlib::SimpleClientGoalState state = ac.getState(); ROS_INFO("Action finished: %s",state.toString().c_str()); &#125; else ROS_INFO("Action did not finish before the time out."); // 关闭节点，在退出前加入线程 ros::shutdown(); spin_thread.join(); //exit return 0;&#125; 编译行为在CMakeLists.txt文件末尾添加以下几行： 12345add_executable(averaging_server src/averaging_server.cpp)target_link_libraries(averaging_server $&#123;catkin_LIBRARIES&#125;)add_executable(averaging_client src/averaging_client.cpp)target_link_libraries(averaging_client $&#123;catkin_LIBRARIES&#125;) 完整的CMakeList.txt文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142cmake_minimum_required(VERSION 2.8.3)project(actionlib_tutorials)find_package(catkin REQUIRED COMPONENTS actionlib actionlib_msgs message_generation roscpp rospy std_msgs)add_action_files( FILES Fibonacci.action Averaging.action DoDishes.action)generate_messages( DEPENDENCIES actionlib_msgs std_msgs)catkin_package( CATKIN_DEPENDS actionlib_msgs)include_directories(include $&#123;catkin_INCLUDE_DIRS&#125; $&#123;Boost_INCLUDE_DIRS&#125;)add_executable(fibonacci_server src/fibonacci_server.cpp)target_link_libraries(fibonacci_server $&#123;catkin_LIBRARIES&#125;)add_executable(fibonacci_client src/fibonacci_client.cpp)target_link_libraries( fibonacci_client $&#123;catkin_LIBRARIES&#125;)add_executable(averaging_server src/averaging_server.cpp)target_link_libraries(averaging_server $&#123;catkin_LIBRARIES&#125;)add_executable(averaging_client src/averaging_client.cpp)target_link_libraries(averaging_client $&#123;catkin_LIBRARIES&#125;) 工作空间下执行catkin_make命令。 运行行为－使用其他节点连接服务器和客户端编写数据节点创建文件actionlib_tutorials/scripts/gen_numbers.py： 12345678910111213141516171819#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Float32import randomdef gen_number(): pub = rospy.Publisher('random_number', Float32) rospy.init_node('random_number_generator', log_level=rospy.INFO) rospy.loginfo("Generating random numbers") while not rospy.is_shutdown(): pub.publish(Float32(random.normalvariate(5, 1))) rospy.sleep(0.05)if __name__ == '__main__': try: gen_number() except Exception, e: print "done" 该文件使用一个正态分布生成随机5个数字，并且标准差为1，然后发布数据到/random_number话题。编译该文件节点可运行：chmod +x gen_numbers.py。 运行行为 终端启动ROS： 1roscore 运行数据节点： 1rosrun actionlib_tutorials gen_numbers.py 查看行为反馈： 1rostopic echo /averaging/feedback 执行过程会有一系列的信息输出，最后一条消息是： 1234567891011121314151617181920header: seq: 100 stamp: secs: 1522553011 nsecs: 185810695 frame_id: ''status: goal_id: stamp: secs: 1522553006 nsecs: 117426116 id: "/test_averaging-1-1522553006.117426116" status: 1 text: "This goal has been accepted by the simple action server"feedback: sample: 101 data: 6.57791948318 mean: 4.95768070221 std_dev: 1.06043183804--- 查看行为结果： 1rostopic echo /averaging/result 执行完成输出信息： 12345678910111213141516171819eric@eric:~$ rostopic echo /averaging/resultheader: seq: 0 stamp: secs: 1522553011 nsecs: 186018651 frame_id: ''status: goal_id: stamp: secs: 1522553006 nsecs: 117426116 id: "/test_averaging-1-1522553006.117426116" status: 4 text: ''result: mean: 4.95768070221 std_dev: 1.06043183804--- 运行行为客户端： 1rosrun actionlib_tutorials averaging_client 执行完输出信息： 1234eric@eric:~$ rosrun actionlib_tutorials averaging_client [ INFO] [1522553004.895895005]: Waiting for action server to start.[ INFO] [1522553006.117383219]: Action server started, sending goal.[ INFO] [1522553011.186401349]: Action finished: ABORTED 运行行为服务器： 1rosrun actionlib_tutorials averaging_server 查看发布的话题列表（检查行为运行是否正常）： 1rostopic list -v 输出信息： 1234567891011121314151617181920212223eric@eric:~$ rostopic list -vPublished topics: * /turtle1/color_sensor [turtlesim/Color] 1 publisher * /averaging/status [actionlib_msgs/GoalStatusArray] 1 publisher * /random_number [std_msgs/Float32] 1 publisher * /averaging/result [actionlib_tutorials/AveragingActionResult] 1 publisher * /rosout [rosgraph_msgs/Log] 6 publishers * /averaging/feedback [actionlib_tutorials/AveragingActionFeedback] 1 publisher * /rosout_agg [rosgraph_msgs/Log] 1 publisher * /averaging/goal [actionlib_tutorials/AveragingActionGoal] 1 publisher * /averaging/cancel [actionlib_msgs/GoalID] 1 publisher * /turtle1/pose [turtlesim/Pose] 1 publisherSubscribed topics: * /averaging/cancel [actionlib_msgs/GoalID] 1 subscriber * /averaging/status [actionlib_msgs/GoalStatusArray] 1 subscriber * /random_number [std_msgs/Float32] 1 subscriber * /averaging/result [actionlib_tutorials/AveragingActionResult] 2 subscribers * /rosout [rosgraph_msgs/Log] 1 subscriber * /averaging/goal [actionlib_tutorials/AveragingActionGoal] 1 subscriber * /averaging/feedback [actionlib_tutorials/AveragingActionFeedback] 2 subscribers * /turtle1/cmd_vel [geometry_msgs/Twist] 1 subscriber 查看行为节点图： 1rqt_graph 显式如下节点图：]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之actionlib库（２）-使用Execute Callback编写一个简单的行为服务器]]></title>
    <url>%2F2018%2F03%2F30%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Bactionlib%E5%BA%93%EF%BC%88%EF%BC%92%EF%BC%89-%E4%BD%BF%E7%94%A8Execute%20Callback%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E8%A1%8C%E4%B8%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中actionlib使用的学习内容。 创建行为消息行为消息自动从.action文件生成，该文件放置在程序包的action目录下，它定义行为消息的目标、结果和行为反馈话题的类型和格式。下面是一个例子。 在程序包中创建文件actionlib_tutorials/action/Fibonacci.action： 12345678#goal definitionint32 order---#result definitionint32[] sequence---#feedbackint32[] sequence 生成消息文件使用编辑好的.action文件生成.msg消息文件，有两种方式，笔者认为手动生成方式其实不是必须的，只是提供了一种生成消息文件的方式而已，一般实践过程中只需要在CMakeList.txt文件添加必要的信息，自动生成消息文件。 通过设置CMakeList.txt文件在编译过程中自动生成，生成的.msg消息文件会自动放在 工作空间/devel/share/程序包名/msg/ 路径下； 使用generation.py脚本手动生成，生成的.msg消息文件可以自定义放置的目录，例如可以放在工作空间/src/程序包名/msg/ 路径下，这时可以在当前程序包下执行命令rosrun actionlib_msgs genaction.py -o msg/ action/Fibonacci.action。 手动生成12roscd actionlib_tutorialsrosrun actionlib_msgs genaction.py -o msg/ action/Fibonacci.action genaction.py文件位于/opt/ros/kinetic/lib/actionlib_msgs/目录下。 会出现如下提示信息： 1Generating for action Fibonacci 自动生成在编译过程中自动生成消息需要添加一些内容到CMakeList.txt文件。 123456789101112131415161718192021222324find_package(catkin REQUIRED COMPONENTS actionlib actionlib_msgs message_generation roscpp rospy std_msgs)add_action_files( FILES Fibonacci.action DoDishes.action)generate_messages( DEPENDENCIES actionlib_msgs std_msgs)catkin_package( CATKIN_DEPENDS actionlib_msgs) 运行： 1catkin_make # 工作空间下运行 使用如下命令就可以看到自动生成的.msg和.h文件： 12ls devel/share/actionlib_tutorials/msg/ls devel/include/actionlib_tutorials/ 创建行为服务器创建文件actionlib_tutorials/src/fibonacci_server.cpp： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;ros/ros.h&gt;#include &lt;actionlib/server/simple_action_server.h&gt;#include &lt;actionlib_tutorials/FibonacciAction.h&gt;class FibonacciAction&#123;protected: ros::NodeHandle nh_;// 将会传递到行为服务器中 actionlib::SimpleActionServer&lt;actionlib_tutorials::FibonacciAction&gt; as_; // NodeHandle instance must be created before this line. Otherwise strange error occurs. std::string action_name_; // create messages that are used to published feedback/result actionlib_tutorials::FibonacciFeedback feedback_;// 反馈消息 actionlib_tutorials::FibonacciResult result_;// 结果消息public: FibonacciAction(std::string name) : as_(nh_, name, boost::bind(&amp;FibonacciAction::executeCB, this, _1), false), action_name_(name) &#123; as_.start(); &#125;// 行为构造函数构造行为服务器as_,它会得到一个节点句柄（node handle）、行为名称和选择一个运行回调函数（executeCB）参数。 ~FibonacciAction(void) &#123; &#125; void executeCB(const actionlib_tutorials::FibonacciGoalConstPtr &amp;goal)// 传递一个指向目标消息的指针，它是一个boost共享指针 &#123; // helper variables ros::Rate r(1); bool success = true; // push_back the seeds for the fibonacci sequence feedback_.sequence.clear(); feedback_.sequence.push_back(0); feedback_.sequence.push_back(1); // publish info to the console for the user ROS_INFO("%s: Executing, creating fibonacci sequence of order %i with seeds %i, %i", action_name_.c_str(), goal-&gt;order, feedback_.sequence[0], feedback_.sequence[1]); // 开始执行行为服务器 for(int i=1; i&lt;=goal-&gt;order; i++) &#123; // 检测一个客户端请求是否抢占当前目标。体现行为服务器的一个重要组成部分：允许行为客户端请求取消当前目标执行 if (as_.isPreemptRequested() || !ros::ok()) &#123; ROS_INFO("%s: Preempted", action_name_.c_str()); // set the action state to preempted as_.setPreempted();//发出该行为已经被用户请求抢占信号 success = false; break; &#125; feedback_.sequence.push_back(feedback_.sequence[i] + feedback_.sequence[i-1]);//设置检查抢占请求服务器的等级到服务器系统 // 发布反馈：Fibonacci序列赋值给feedback变量，然后通过行为服务器提供的反馈频道发布出去 as_.publishFeedback(feedback_); // this sleep is not necessary, the sequence is computed at 1 Hz for demonstration purposes r.sleep(); &#125; if(success) &#123; result_.sequence = feedback_.sequence; ROS_INFO("%s: Succeeded", action_name_.c_str()); // set the action state to succeeded as_.setSucceeded(result_);// 一旦行为完成计算Fibonacci序列，通知行为客户端操作设置成功 &#125; &#125;&#125;;int main(int argc, char** argv)&#123; ros::init(argc, argv, "fibonacci"); FibonacciAction fibonacci("fibonacci");//创建行为 ros::spin();//spin节点，行为会运行并等待接收目标 return 0;&#125; 创建行为客户端创建行为客户端文件actionlib_tutorials/src/fibonacci_client.cpp： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;ros/ros.h&gt;#include &lt;actionlib/client/simple_action_client.h&gt;#include &lt;actionlib/client/terminal_state.h&gt;#include &lt;actionlib_tutorials/FibonacciAction.h&gt;int main (int argc, char **argv)&#123; ros::init(argc, argv, "test_fibonacci"); // 创建行为客户端 // 成功会开启客户端创建自己的线程 actionlib::SimpleActionClient&lt;actionlib_tutorials::FibonacciAction&gt; ac("fibonacci", true); ROS_INFO("Waiting for action server to start."); //等待行为服务器开启 ac.waitForServer(); //will wait for infinite time ROS_INFO("Action server started, sending goal."); // 发送目标到行为服务器 actionlib_tutorials::FibonacciGoal goal; goal.order = 20; ac.sendGoal(goal); //等待行为返回 bool finished_before_timeout = ac.waitForResult(ros::Duration(30.0)); if (finished_before_timeout) &#123; actionlib::SimpleClientGoalState state = ac.getState(); ROS_INFO("Action finished: %s",state.toString().c_str()); &#125; else ROS_INFO("Action did not finish before the time out."); //exit return 0;&#125; 编译行为在CMakeLists.txt文件末尾添加以下几行： 12345add_executable(fibonacci_server src/fibonacci_server.cpp)target_link_libraries(fibonacci_server $&#123;catkin_LIBRARIES&#125;)add_executable(fibonacci_client src/fibonacci_client.cpp)target_link_libraries(fibonacci_client $&#123;catkin_LIBRARIES&#125;) 完整的CMakeList.txt文件如下： 1234567891011121314151617181920212223242526272829303132333435cmake_minimum_required(VERSION 2.8.3)project(actionlib_tutorials)find_package(catkin REQUIRED COMPONENTS actionlib actionlib_msgs message_generation roscpp rospy std_msgs)add_action_files( FILES Fibonacci.action DoDishes.action)generate_messages( DEPENDENCIES actionlib_msgs std_msgs)catkin_package( CATKIN_DEPENDS actionlib_msgs)include_directories(include $&#123;catkin_INCLUDE_DIRS&#125; $&#123;Boost_INCLUDE_DIRS&#125;)add_executable(fibonacci_server src/fibonacci_server.cpp)target_link_libraries(fibonacci_server $&#123;catkin_LIBRARIES&#125;)add_executable(fibonacci_client src/fibonacci_client.cpp)target_link_libraries( fibonacci_client $&#123;catkin_LIBRARIES&#125;) 工作空间下执行catkin_make命令。 其实还需要在package.xml中添加如下信息，只不过在最开始创建程序包的时候catkin_create_pkg命令添加了对actionlib和actionlib_msgs的依赖，生成的程序包文件package.xml中已经自动添加了这些信息，不需要手动添加了。完美～ 123456&lt;build_depend&gt;actionlib&lt;/build_depend&gt;&lt;build_depend&gt;actionlib_msgs&lt;/build_depend&gt;&lt;build_export_depend&gt;actionlib&lt;/build_export_depend&gt;&lt;build_export_depend&gt;actionlib_msgs&lt;/build_export_depend&gt;&lt;exec_depend&gt;actionlib&lt;/exec_depend&gt;&lt;exec_depend&gt;actionlib_msgs&lt;/exec_depend&gt; 运行行为－连接服务器和客户端 终端启动ROS： 1roscore 查看行为反馈： 1rostopic echo /fibonacci/feedback 会有一系列的输出信息，最后的一条信息为： 1234567891011121314151617---header: seq: 19 stamp: secs: 1522552545 nsecs: 242077849 frame_id: ''status: goal_id: stamp: secs: 1522552526 nsecs: 241284287 id: "/test_fibonacci-1-1522552526.241284287" status: 1 text: "This goal has been accepted by the simple action server"feedback: sequence: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946] 查看行为结果： 1rostopic echo /fibonacci/result 执行完成输出信息： 12345678910111213141516171819eric@eric:~$ rostopic echo /fibonacci/resultWARNING: topic [/fibonacci/result] does not appear to be published yetheader: seq: 0 stamp: secs: 1522552546 nsecs: 242312501 frame_id: ''status: goal_id: stamp: secs: 1522552526 nsecs: 241284287 id: "/test_fibonacci-1-1522552526.241284287" status: 3 text: ''result: sequence: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946]--- 运行行为客户端： 1rosrun actionlib_tutorials fibonacci_client 执行完成的输出信息： 1234eric@eric:~$ rosrun actionlib_tutorials fibonacci_client[ INFO] [1522552522.129016184]: Waiting for action server to start.[ INFO] [1522552526.241189187]: Action server started, sending goal.[ INFO] [1522552546.242949052]: Action finished: SUCCEEDED 运行行为服务器： 1rosrun actionlib_tutorials fibonacci_server 执行完成的输出信息： 123eric@eric:~rosrun actionlib_tutorials fibonacci_server[ INFO] [1522552526.242112000]: fibonacci: Executing, creating fibonacci sequence of order 20 with seeds 0, 1[ INFO] [1522552546.242205514]: fibonacci: Succeeded 查看发布的话题列表（检查行为运行是否正常）： 1rostopic list -v 得到如下信息： 123456789101112131415161718192021eric@eric:~$ rostopic list -vPublished topics: * /turtle1/color_sensor [turtlesim/Color] 1 publisher * /fibonacci/feedback [actionlib_tutorials/FibonacciActionFeedback] 1 publisher * /fibonacci/cancel [actionlib_msgs/GoalID] 1 publisher * /rosout [rosgraph_msgs/Log] 5 publishers * /fibonacci/goal [actionlib_tutorials/FibonacciActionGoal] 1 publisher * /rosout_agg [rosgraph_msgs/Log] 1 publisher * /fibonacci/status [actionlib_msgs/GoalStatusArray] 1 publisher * /fibonacci/result [actionlib_tutorials/FibonacciActionResult] 1 publisher * /turtle1/pose [turtlesim/Pose] 1 publisherSubscribed topics: * /fibonacci/feedback [actionlib_tutorials/FibonacciActionFeedback] 2 subscribers * /rosout [rosgraph_msgs/Log] 1 subscriber * /fibonacci/cancel [actionlib_msgs/GoalID] 1 subscriber * /fibonacci/goal [actionlib_tutorials/FibonacciActionGoal] 1 subscriber * /fibonacci/status [actionlib_msgs/GoalStatusArray] 1 subscriber * /fibonacci/result [actionlib_tutorials/FibonacciActionResult] 2 subscribers * /turtle1/cmd_vel [geometry_msgs/Twist] 1 subscriber 查看行为节点图： 1rqt_graph 显式如下节点图：]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之actionlib库（１）-actionlib库的介绍]]></title>
    <url>%2F2018%2F03%2F29%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Bactionlib%E5%BA%93%EF%BC%88%EF%BC%91%EF%BC%89-actionlib%E5%BA%93%E7%9A%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中actionlib使用的学习内容。 介绍actionlib软件包为ROS中的可抢占任务提供了一个基于话题的通用接口。在任何一个比较大的基于ROS的系统，都会有这样的情况，向某个节点发送请求执行某一个任务，并返回相应的执行结果，这种请求-响应式的使用场景通常使用ROS提供的服务（services）机制完成。然而，有一些情况服务执行的时间很长，在执行中想要定期获得任务处理的进度，或可能取消执行任务（或请求），例如如果执行各种运动的动作，像控制机械手臂去抓取一个杯子，这个过程可能复杂而漫长，执行过程中还可能强制中断或反馈信息，service机制就无法满足需求，而actionlib就能实现这样的功能。它是ROS中一个很重要的功能包集合（库），可以实现一些简单的状态机功能，算的上是SMACH的一个弱化版。 扩展： SMACH是一个用于快速创建复杂机器人行为的任务级体系结构。SMACH的核心是独立于ROS的Python库，用于构建分层状态机。SMACH是一个新的库，它利用非常古老的概念来快速创建具有可维护和模块化代码的强大机器人行为。可以使用SMACH建立一个有限状态机，但SMACH能做的更多。SMACH是一个任务级的执行和协调库，并提供集中“状态容器”。一个这样的容器是一个有限状态机，但是这个容器也可以是另一个容器中的状态。更多内容参考wiki。 细节描述actionlib堆栈提供了一个标准化的接口同可抢占任务进行交互，这方面的例子包括将底座移动到目标位置、执行激光扫描并返回产生的点云、检测门的手柄等等。 介绍下面将描述动作客户端和服务器相互交互的底层机制，如果只是简单的使用actionlib就没必要深入学习了。 高级客户端/服务器交互服务器描述服务器状态机goal是在ActionClient端启动的（因为client会发送sendgoal嘛），一旦ActionServer接收到goal请求，它就会为这个goal创建一个状态机，来追踪goal的状态转换。注意，状态机跟踪的是goal！而不是不是跟踪ActionServer本身！所以系统中对于每一个goal都会有一个状态机。状态转换图如下所示： 服务器转换状态这些状态的转换大多是服务的实施者（其实就是服务的程序）触发的，用小一串命令： setAccepted - 检查到有goal之后，决定开始处理它 setRejected - 检察到goal后，决定不去处理它，因为它是个无效请求（溢出，资源不可用，无效等） setSucceeded - 告知goal被正确执行 setAborted - 告知goal在处理时遇到了问题不得不被终止了 setCanceled - 告知因cancle请求，goal不再被执行了 action client也能异步触发状态转换： CancelRequest: 客户端通知action server它想要server停止处理这个goal服务端状态 服务端状态 中间状态 （前面说了，simple的状态有三个，就是等待执行挂起） Pending - goal还没有被ActionServer处理 Active - goal正在被AS处理 Recalling - goal没有被处理并且从客户端已发送取消它的命令，但AS还不确定goal已经被取消了（时差导致的？） Preempting - goal正被处理呢，从AC端收到了取消请求，但AS还不确定goal已经被取消 终点状态 Rejected - AC没有发cancle请求，goal被AS不处理直接拒绝了The goal was rejected by the action server without being processed and without a request from the action client to cancel Succeeded - goal被AS成功实现 was achieved successfully by the action server Aborted - goal被AS终止没有AC的cancle请求 Recalled - 在AS开始执行之前这个goal被另一个goal或者cancle请求取消了 Preempted - 处理中的goal被另一个goal或者AC的取消请求给取消了 客户端描述客户端状态机客户端状态Action接口和传输层（协议）数据与信息goal话题cancel话题status话题feedback话题result话题协议简单的行为客户端客户端状态歧义多目标策略线程模型简单的行为服务器目标通知线程模型Client-Server交互如下图所示，actionlib的框架实际是一种特殊的客户-服务的模式。除了服务请求的功能外，还可以实时获取服务器执行任务的进度状态，以及强制中断服务的功能。action客户端和服务端通过预定义的ROS Action协议通信，该通信机制基于ROS消息。action客户端和服务端通过函数调用和回调的方式，向用户提供用于请求目标（在客户端发生）或执行目标（服务器端发生）的接口。 Action清单：Goal,Feedback,Result为了使得客户端和服务器之间进行通信，需要定义一些用于二者之间通信的消息，这就是action清单。该清单定义客户端和服务器之间通信的Goal、Feedback、Result信息。 Goal：为了使用action来完成任务，引入可以由ActionClient发送到服务器的Goal概念。对于移动底座的情况，Goal将是PoseStamped消息，其中包含关于机器人应该在世界坐标系移动到何处的信息。对于控制倾斜激光扫描仪的情况， Goal应该包含扫描参数（最小角、最大角、速度等）。 Feedback：Feedback为服务器实施者提供了一种方法，告知ActionClient目标的增量变化情况。对于移动底座的情况，它可能是机器人沿着路径运动时当前的姿势；对于控制倾斜激光扫描仪的情况，它可能是扫描完成之前剩下的时间。 Result：完成目标后，结果会从ActionServer发送到ActionClient。Result同Feedback不同，因为它只发送一次，当行动的目标是提供某种信息时，这一点就非常有用。对于移动底座的情况，Result不是非常重要，但它可以是机器人的最终位姿；对于控制倾斜激光扫描仪的情况，结果可能包含根据请求的扫描生成的点云。 .action文件在介绍.action文件之前，先创建一个程序包，用于后续内容的学习，actinlib库相关的文件都放在该程序包下： 1catkin_create_pkg actionlib_tutorials actionlib message_generation roscpp rospy std_msgs actionlib_msgs ROS中使用一个.action文件定义action清单，该文件包含goal、result、feedback的定义，使用—-分隔开，它一般会被放置在程序包的action目录下。以洗碟子为例，描述该过程的action清单如下所示： actionlib_tutorials/action/DoDishes.action 12345678# Define the goaluint32 dishwasher_id # Specify which dishwasher we want to use---# Define the resultuint32 total_dishes_cleaned---# Define a feedback messagefloat32 percent_complete 基于.action文件会产生６个消息用于客户端和服务器的通信，这一过程在catkin_make编译过程自动触发完成。 Catkin在当前程序包CMakeList.txt文件中catkin_package()之前添加： 1234567891011121314add_action_files( FILES DoDishes.action)generate_messages( DEPENDENCIES actionlib_msgs std_msgs)catkin_package( CATKIN_DEPENDS actionlib_msgs) 同时需要在package.xml文件中包含如下依赖： 1234&lt;build_depend&gt;actionlib&lt;/build_depend&gt;&lt;build_depend&gt;actionlib_msgs&lt;/build_depend&gt;&lt;run_depend&gt;actionlib&lt;/run_depend&gt;&lt;run_depend&gt;actionlib_msgs&lt;/run_depend&gt; Results执行命令： 12roscd actionlib_tutorialsrosrun actionlib_msgs genaction.py -o msg/ action/Fibonacci.action genaction.py文件位于/opt/ros/kinetic/lib/actionlib_msgs/目录下。 会出现如下提示信息： 1Generating for action Fibonacci 通过手动执行generation.py文件，我们就使用DoDishes.action生成了以下消息，并保存在了程序包的msg/目录下。这些消息文件将被actionlib内部用于ActionClient和ActionServer之间的通信。 DoDishesAction.msg DoDishesActionGoal.msg DoDishesActionResult.msg DoDishesActionFeedback.msg DoDishesGoal.msg DoDishesResult.msg DoDishesFeedback.msg 注意：其实我们可以完全不用手动执行如上操作，手动生成消息文件。我们可以在工作空间目录下执行catkin_make命令，就会自动生成的.msg和.h文件，并分别保存在工作空间/devel/share/actionlib_tutorials/msg和工作空间/devel/include/actionlib_tutorials目录下。 ActionClient的使用-C++ SimpleActionClient以下程序实现了如何将goal发送到名为do_dishes的DoDishes ActionServer。创建文件actionlib_tutorials/src/do_dishes_client.cpp： 12345678910111213141516171819#include &lt;actionlib_tutorials/DoDishesAction.h&gt; // Note: "Action" is appended#include &lt;actionlib/client/simple_action_client.h&gt;typedef actionlib::SimpleActionClient&lt;actionlib_tutorials::DoDishesAction&gt; Client;int main(int argc, char** argv)&#123; ros::init(argc, argv, "do_dishes_client"); Client client("do_dishes", true); // true -&gt; don't need ros::spin() client.waitForServer(); actionlib_tutorials::DoDishesGoal goal; // Fill in goal here client.sendGoal(goal); client.waitForResult(ros::Duration(5.0)); if (client.getState() == actionlib::SimpleClientGoalState::SUCCEEDED) printf("Yay! The dishes are now clean"); printf("Current State: %s\n", client.getState().toString().c_str()); return 0;&#125; 注意：对于C++SimpleActionClient，在一个单独的线程正在服务客户端的回调队列时，waitForServer方法才会工作。这需要传递给客户端构造函数的spin_thread选项，使用多线程微调器运行，或者使用您自己的线程为ROS回调队列提供服务。 ActionServer的使用-C++ SimpleActionServer以下片段显示了如何编写一个名为“do_dishes”的DoDishes ActionServer。创建文件actionlib_tutorials/src/do_dishes_server.cpp： 1234567891011121314151617181920#include &lt;actionlib_tutorials/DoDishesAction.h&gt; // Note: "Action" is appended#include &lt;actionlib/server/simple_action_server.h&gt;typedef actionlib::SimpleActionServer&lt;actionlib_tutorials::DoDishesAction&gt; Server;void execute(const actionlib_tutorials::DoDishesGoalConstPtr&amp; goal, Server* as) // Note: "Action" is not appended to DoDishes here&#123; // Do lots of awesome groundbreaking robot stuff here as-&gt;setSucceeded();&#125;int main(int argc, char** argv)&#123; ros::init(argc, argv, "do_dishes_server"); ros::NodeHandle n; Server server(n, "do_dishes", boost::bind(&amp;execute, _1, &amp;server), false); server.start(); ros::spin(); return 0;&#125; 测试Action工作空间目录下执行： 1catkin_make 执行完会自动生成可执行文件do_dishes_client、do_dishes_server，保存在工作空间/devel/lib/actionlib_tutorials目录下。 终端启动ROS： 1roscore 运行行为客户端： 1rosrun actionlib_tutorials do_dishes_client 运行行为服务器： 1rosrun actionlib_tutorials do_dishes_server 执行完成客户端会有如下输出信息： 1Yay! The dishes are now cleanCurrent State: SUCCEEDED 执行rqt_graph命令查看节点图：]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之不常用的重要关键字]]></title>
    <url>%2F2018%2F03%2F29%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E9%87%8D%E8%A6%81%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++一些不常用关键字的学习内容。 explicit关键字explicit关键字只能用于修饰只有一个参数的类构造函数, 它的作用是表明该构造函数是显式的，声明为explicit的构造函数不能在隐式转换中使用。参见博客另一篇文章。]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之Timer类]]></title>
    <url>%2F2018%2F03%2F29%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8BTimer%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS 定时器类Timer类的内容。 测试程序： 1234567891011121314151617181920#include "ros/ros.h"void callback1(const ros::TimerEvent&amp;)&#123; ROS_INFO("Callback 1 triggered");&#125;void callback2(const ros::TimerEvent&amp;)&#123; ROS_INFO("Callback 2 triggered");&#125;int main(int argc, char **argv)&#123; ros::init(argc, argv, "talker"); ros::NodeHandle n; ros::Timer timer1 = n.createTimer(ros::Duration(0.1), callback1);//每隔0.1秒执行一次回调函数 ros::Timer timer2 = n.createTimer(ros::Duration(1.0), callback2);//每隔1秒执行一次回调函数 ros::spin(); return 0;&#125;]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之编写简单的服务器和客户端]]></title>
    <url>%2F2018%2F03%2F28%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BC%96%E5%86%99%E7%AE%80%E5%8D%95%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中服务器和客户端的学习内容。 写在篇头： ROS程序包中一般包含msg、src、srv、scripts目录，分别存放msg消息文件、C++源文件（.cpp）、srv服务文件、Python源文件（.py），执行catkin_make命令编译完成后，.msg文件、srv文件都会转换为ROS所支持的源代码，并生成C++可执行文件。 C++ .msg、.srv文件生成的C++头文件将放在~/工作空间/devel/include/程序包名/下 生成的可执行文件放在~/工作空间/devel/lib/&lt;package name&gt; 下 Python .msg文件生成的Python.py脚本文件放在 ~/工作空间/devel/lib/python2.7/dist-packages/程序包名/msg下 .srv文件生成的Python.py脚本文件放在 ~/工作空间/devel/lib/python2.7/dist-packages/程序包名/srv下 使用chmod +x scripts/xxx.py命令，使节点文件具有执行属性 消息(msg)和服务(srv)介绍 消息(msg): msg文件就是一个描述ROS中所使用消息类型的简单文本，被存放在package的msg目录下。该文件会被用来生成不同语言的源代码，一般是C++、Python。 msg文件实际上就是每行声明一个数据类型和变量名，可以使用的数据类型如下： int8, int16, int32, int64 (plus uint*) float32, float64 string time, duration other msg files variable-length array[] and fixed-length array[C] 服务(srv): 一个srv文件描述一项服务， srv文件被存放在srv目录下。 srv文件分为请求和响应两部分，由’—-‘分隔。下面是srv的一个样例： 1234int64 Aint64 B---int64 Sum 其中 A 和 B 是请求, 而Sum 是响应。 创建完成该任务的程序包123cd ~/catkin_ws/src #工作空间catkin_wscatkin_create_pkg beginner_tutorials std_msgs rospy roscpp #创建程序包mkdir -p beginner_tutorials/src #放置所有源代码 创建msg 创建msg消息 12mkdir msg #在新创建的程序包目录下echo "int64 num" &gt; msg/Num.msg 配置文件 package.xml中添加： 12&lt;build_depend&gt;message_generation&lt;/build_depend&lt;run_depend&gt;message_runtime&lt;/run_depend&gt; ​ CMakeList.txt添加信息的部分： 1234567891011121314151617find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation)add_message_files( FILES Num.msg)generate_messages( DEPENDENCIES std_msgs)catkin_package( CATKIN_DEPENDS message_runtime) 注意：执行catkin_make编译程序包后，某个程序包中的.msg文件都会转换为ROS所支持的源代码，生成的C++头文件将会放置在~/工作空间/devel/include/程序包名/下，本程序生成Num.h。Python脚本语言会在 ~/工作空间/devel/lib/python2.7/dist-packages/程序包名/msg 目录下创建。 创建srv 创建srv文件 12mkdir msg #在新创建的程序包目录下roscp rospy_tutorials AddTwoInts.srv srv/AddTwoInts.srv #从其他程序包中复制一个服务文件 配置文件 CMakeList.txt添加信息的部分： 1234add_service_files( FILES AddTwoInts.srv) 注意：执行catkin_make编译程序包后，某个程序包中的.srv文件都会转换为ROS所支持的源代码，生成的C++头文件将会放置在~/工作空间/devel/include/程序包名/下，这里生成的是AddTwoInts.h。Python脚本语言会在 ~/工作空间/devel/lib/python2.7/dist-packages/程序包名/srv 目录下创建。 创建Service节点创建一个简单的service节点add_two_ints_server，该节点将接收到两个整形数字，并返回它们的和。 在程序包创建src/add_two_ints_server.cpp文件： 1234567891011121314151617181920212223242526#include "ros/ros.h"//编译系统自动根据先前创建的srv文件生成的对应该srv文件的头文件#include "beginner_tutorials/AddTwoInts.h"//提供两个int值的求和服务，int值从request中获取，返回数据装入response，这些数据类型都定义在srv文件内部bool add(beginner_tutorials::AddTwoInts::Request &amp;req, beginner_tutorials::AddTwoInts::Response &amp;res)&#123; res.sum = req.a + req.b; ROS_INFO("request: x=%ld, y=%ld", (long int)req.a, (long int)req.b); ROS_INFO("sending back response: [%ld]", (long int)res.sum); return true;&#125;int main(int argc, char **argv)&#123; ros::init(argc, argv, "add_two_ints_server"); ros::NodeHandle n; //service已经建立起来，并在ROS内发布出来，参数add_two_ints与客户端创建client时的参数一致 ros::ServiceServer service = n.advertiseService("add_two_ints", add); ROS_INFO("Ready to add two ints."); ros::spin(); return 0;&#125; 创建Clinet节点在程序包创建src/add_two_ints_client.cpp文件： 12345678910111213141516171819202122232425262728293031323334#include "ros/ros.h"#include "beginner_tutorials/AddTwoInts.h"#include &lt;cstdlib&gt;int main(int argc, char **argv)&#123; ros::init(argc, argv, "add_two_ints_client"); if (argc != 3) &#123; ROS_INFO("usage: add_two_ints_client X Y"); return 1; &#125; ros::NodeHandle n; //为add_two_ints service创建一个client。ros::ServiceClient 对象会用来调用service ros::ServiceClient client = n.serviceClient&lt;beginner_tutorials::AddTwoInts&gt;("add_two_ints"); beginner_tutorials::AddTwoInts srv; srv.request.a = atoll(argv[1]);//long long int atoll ( const char * str ); srv.request.b = atoll(argv[2]); //调用service，该过程是模态过程（调用的时候占用进程阻止其他代码的执行），一旦调用完成，将返回调用结果。如果service调用成功，call()函数将返回true，srv.response里面的值将是合法的值。如果调用失败，call()函数将返回false，srv.response里面的值将是非法的。 if (client.call(srv)) &#123; ROS_INFO("Sum: %ld", (long int)srv.response.sum); &#125; else &#123; ROS_ERROR("Failed to call service add_two_ints"); return 1; &#125; return 0;&#125; 编译节点在CMakeList.txt文件末尾添加： 1234567add_executable(add_two_ints_server src/add_two_ints_server.cpp)target_link_libraries(add_two_ints_server $&#123;catkin_LIBRARIES&#125;)add_dependencies(add_two_ints_server beginner_tutorials_gencpp) #不加这句也可以add_executable(add_two_ints_client src/add_two_ints_client.cpp)target_link_libraries(add_two_ints_client $&#123;catkin_LIBRARIES&#125;)add_dependencies(add_two_ints_client beginner_tutorials_gencpp) #不加这句也可以 工作空间下执行命令： 1catkin_make 执行结束将生成两个可执行程序add_two_ints_server和add_two_ints_client，默认放在devel space下的包目录下，即~/工作空间/devel/lib/&lt;package name&gt;。可以直接调用可执行程序，或者使用rosrun命令去调用。 运行节点 启动ROS 1roscore 启动服务端 1rosrun beginner_tutorials add_two_ints_server 启动客户端（带参数） 1rosrun beginner_tutorials add_two_ints_client 1 3 这样在服务端终端和客户端终端就会看到相应的输出信息。]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>catkin</tag>
        <tag>ROS服务器</tag>
        <tag>ROS客户端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之编写简单的消息发布器和订阅器]]></title>
    <url>%2F2018%2F03%2F28%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BC%96%E5%86%99%E7%AE%80%E5%8D%95%E7%9A%84%E6%B6%88%E6%81%AF%E5%8F%91%E5%B8%83%E5%99%A8%E5%92%8C%E8%AE%A2%E9%98%85%E5%99%A8%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中消息发布器和订阅器的学习内容。 创建完成该任务的程序包123cd ~/catkin_ws/src #工作空间catkin_wscatkin_create_pkg beginner_tutorials std_msgs rospy roscpp #创建程序包mkdir -p beginner_tutorials/src #放置所有源代码 创建消息发布器节点talker消息发布器节点talker将不断在ROS网络中广播消息。 在beginner_tutorials/src文件夹下创建talker.cpp文件： 1234567891011121314151617181920212223242526272829303132333435363738394041//ros.h引用了 ROS 系统中大部分常用的头文件#include "ros/ros.h"//引用std_msgs/String 消息, 它存放在 std_msgs package 里#include "std_msgs/String.h"#include &lt;sstream&gt;int main(int argc, char **argv)&#123; //初始化ros，定义节点名字"talker" ros::init(argc, argv, "talker"); //为该进程的节点创建句柄，与ROS系统通信的主要接入点。第一个创建的 NodeHandle 会为节点进行初始化，最后一个销毁的 NodeHandle 则会释放该节点所占用的所有资源。 ros::NodeHandle n; //告知master，在话题"chatter"上发布&lt;std_msgs::String&gt;类型的数据，chatter_pub用于发布消息 ros::Publisher chatter_pub = n.advertise&lt;std_msgs::String&gt;("chatter", 1000); //指定自循环的频率，设为10hz ros::Rate loop_rate(10); //发送数据的次数计数 int count = 0; //SIGINT 被触发 (Ctrl-C) 时返回false while (ros::ok())&#123; //消息对象 std_msgs::String msg; std::stringstream ss; ss &lt;&lt; "hello world " &lt;&lt; count; msg.data = ss.str(); //终端输出消息 ROS_INFO("%s", msg.data.c_str()); //发布消息 chatter_pub.publish(msg); //这里不是必须的，当有回调时需要使用该函数，否则回调函数就不会被调用 ros::spinOnce(); //调用 ros::Rate 对象来休眠一段时间以使得发布频率为 10Hz loop_rate.sleep(); ++count; &#125; return 0;&#125; 总结流程： 初始化 ROS 系统 在 ROS 网络内广播我们将要在 chatter 话题上发布std_msgs/String类型的消息 以每秒 10 次的频率在 chatter 上发布消息 创建消息订阅器节点在beginner_tutorials/src文件夹下创建listener.cpp文件： 12345678910111213141516171819#include "ros/ros.h"#include "std_msgs/String.h"//回调函数，接收到chatter话题时就会被调用void chatterCallback(const std_msgs::String::ConstPtr&amp; msg)&#123; ROS_INFO("I heard: [%s]", msg-&gt;data.c_str());&#125;int main(int argc, char **argv)&#123; ros::init(argc, argv, "listener"); ros::NodeHandle n; //告知master订阅chatter话题的消息，当有消息发布到这个话题时，ROS 就会调用回调函数 ros::Subscriber sub = n.subscribe("chatter", 1000, chatterCallback); //进入循环，等待回调，所有的回调函数都会被调用，摁下Ctrl-C或者被master关闭时退出 ros::spin(); return 0;&#125; 流程总结： 初始化ROS系统 订阅 chatter 话题 进入自循环，等待消息的到达 当消息到达，调用 chatterCallback() 函数 编译节点在CMakeList.txt文件中添加： 123456789include_directories(include $&#123;catkin_INCLUDE_DIRS&#125;)add_executable(talker src/talker.cpp)target_link_libraries(talker $&#123;catkin_LIBRARIES&#125;)add_dependencies(talker beginner_tutorials_gencpp) #不加这句也可以add_executable(listener src/listener.cpp)target_link_libraries(listener $&#123;catkin_LIBRARIES&#125;)add_dependencies(listener beginner_tutorials_gencpp) #不加这句也可以 在工作空间下执行： 1catkin_make 执行完编译命令就会生成两个可执行文件, talker 和 listener, 默认存储到 devel space 目录下，具体是在~/工作空间/devel/lib/&lt;package name&gt; 中。 运行节点 启动ROS 1roscore 启动发布器 123cd ~/catkin_ws #catkin_ws为工作空间source ./devel/setup.bashrosrun beginner_tutorials talker 启动订阅器 1rosrun beginner_tutorials listener 这样在发布器终端和订阅器终端就会看到相应的输出信息。 推荐阅读https://blog.csdn.net/weixin_28900531/article/details/79431155 https://blog.csdn.net/u013453604/article/details/49102957]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>ROS</tag>
        <tag>catkin</tag>
        <tag>ROS消息发布器</tag>
        <tag>ROS消息订阅器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之成员变量的初始化顺序]]></title>
    <url>%2F2018%2F03%2F28%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%88%90%E5%91%98%E5%8F%98%E9%87%8F%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++成员变量初始化顺序的学习内容。 我现在是研一，最近研二的学长学姐们都在准备找实习，各种面试笔试，工作不好找，形势好严峻。所以听从学长的建议从现在开始积累编程语言（C++、Python）、算法、数据结构方面的基础知识，为今后面试找工作做好准备。在读剑指offer书时遇到成员变量初始化顺序这种问题，打算查资料深入学习下。 首先是下面的测试代码： 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;using namespace std;class A&#123;private: int n1; int n2;public: //成员变量初始化方式一：构造函数初始化列表初始化 A():n2(0),n1(n2+2)&#123;&#125; //成员变量初始化方式二：构造函数内初始化 //A()&#123; // n2 = 0; // n1 = n2 + 2; //&#125; void Print()&#123; cout &lt;&lt; "n1:" &lt;&lt; n1 &lt;&lt; ", n2: " &lt;&lt; n2 &lt;&lt;endl; &#125;&#125;;int main(int argc, char **argv) &#123; A a; a.Print(); return 1;&#125; 方式一：类的成员变量通过构造函数初始化列表初始化时，这段代码的输出结果：n1:32766, n2:0，当然多次执行时，n1的值并不相同。 方式二：在构造函数内部初始化成员变量时，输出结果：n1:2, n2:0。 分析1、成员变量在使用初始化列表初始化时，与构造函数中初始化成员列表的顺序无关，只与定义成员变量的顺序有关。因为成员变量的初始化次序是由变量在内存中的次序决定的，而内存中的排列顺序早在编译期就根据变量的定义次序决定了。这点在EffectiveC++中有详细介绍。 2、如果不使用初始化列表初始化，在构造函数内初始化时，此时与成员变量在构造函数中的位置有关。 3、注意：类的成员变量在定义时，不能初始化。因为此时类并没有进行实例化（创建对象），因此并没有分配内存。 4、注意：类中const成员常量必须在构造函数初始化列表中初始化。 5、注意：类中static成员变量，必须在类外初始化。 6、静态变量进行初始化顺序是基类的静态变量先初始化，然后是其派生类，直到所有的静态变量都被初始化。这里需要注意全局变量和静态变量的初始化是不分次序的。这也不难理解，其实静态变量和全局变量都被放在公共内存区。可以把静态变量理解为带有”作用域“的全局变量。在一切初始化工作结束后，main函数会被调用，如果某个类的构造函数被执行，那么首先基类的成员变量会被初始化。 总结 类的成员变量的初始化 非static非const成员变量，一般在构造函数中进行初始化 static成员变量，必须在类的外面进行初始化（在类的实现时，在类的外部进行初始化） const成员变量，必须在类的构造函数的初始化列表中初始化 static const成员变量，可以在类的内部声明时初始化 类的成员变量初始化顺序 基类的静态变量或全局变量 派生类的静态变量或全局变量 基类的成员变量 派生类的成员变量 必须在构造函数的初始化列表中的情况 类的const常量 类的引用类型成员 没有默认构造函数的类类型成员 如果类存在继承关系，派生类必须在其初始化列表中调用基类的构造函数]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之基于arbotix和rviz进行简单的机器人仿真]]></title>
    <url>%2F2018%2F03%2F27%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9F%BA%E4%BA%8Earbotix%E5%92%8Crviz%E8%BF%9B%E8%A1%8C%E7%AE%80%E5%8D%95%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%BB%BF%E7%9C%9F%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中基于arbotix和rviz进行简单的机器人仿真的内容。 在学习古月居大神关于ROS的博客时，参考他安装模拟器arbotix的方式总是连接超时，找到了如下源码安装方式。 本人的环境ubuntu16.04+ROS(kinetic) 首先安装rbx1rbx1是 ROS by Example 一书中的实例代码，该书是国外关于ROS出版的第一本书，主要针对Electric和Fuerte版本，使用机器人主要是TurtleBot。书中详细讲解了关于机器人的基本仿真、导航、路径规划、图像处理、语音识别等等，相关的代码基本都包含在rbx１中。rbx1源码安装方式： 123456cd ~/catkin_ws/srcgit clone https://github.com/pirobot/rbx1.git cd ..catkin_makesource ./devel/setup.bashrospack profile 安装arbotix模拟器12345cd ~/catkin_ws/srcgit clone https://github.com/vanadiumlabs/arbotix_ros.git// 在arbotix_ros文件夹下新建文件夹src,将arbotix_ros目录下的所有文件剪切放到src文件夹下;cd ..catkin_make 后续测试模拟器等操作可以参考： https://blog.csdn.net/lizilpl/article/details/46757683 和http://www.guyuehome.com/237 rviz说明显示的类型： Axes 显示坐标轴 Effort 显示一个物体的边缘化 camera 提供一个窗口显示图像 grid 显示2D或者3D的一个栅格 Alpha 该参数表示透明度 Ｌine Style 该参数表示线性（Billboards、Lines） grid cells 在一个网格中绘制八叉树地图 iamge 用图像创造一个窗口 interactiveMake 允许用箭头来控制 Laser Scan 使用激光雷达进行扫描 Map 显示地图信息 Maker 允许程序员用topic 来控制 path 显示导航的路径 point 绘制出一些小的球 pose 绘制出位姿 ploygon 绘制多边形的轮廓线 Odometry 视觉里程计 range 显示视觉里程及声呐的测距范围 tf 显示tf变换的层次结构 RobotModel 显示机器人 动态修改参数1rosrun rqt_reconfigure rqt_reconfigure 该命令打开GUI中可以的动态调整ROS节点的参数，无需重新启动节点。]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>rviz</tag>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu学习之update和upgrade]]></title>
    <url>%2F2018%2F03%2F27%2Fubuntu%E5%AD%A6%E4%B9%A0%E4%B9%8Bupdate%E5%92%8Cupgrade%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ubuntu下update和upgrade命令使用的内容。 每个Linux的发行版，比如Ubuntu，都会维护一个自己的软件仓库，我们常用的几乎所有软件都在这里面。这里面的软件绝对安全，而且绝对的能正常安装。 安装软件的方式：在Ubuntu下维护一个源列表，源列表里面都是一些网址信息，这每一条网址就是一个源，这个地址指向的数据标识着这台源服务器上有哪些软件可以安装使用。 编辑源命令：sudo gedit /etc/apt/sources.list在这个文件里加入或者注释（加#）掉一些源后，保存。这时候，我们的源列表里指向的软件就会增加或减少一部分。 接一下要做的就是：sudo apt-get update执行该命令，会访问源列表里的每个网址，并读取软件列表，然后保存在本地电脑。我们在新立得软件包管理器里看到的软件列表，都是通过update命令更新的。 update后，可能需要upgrade一下：sudo apt-get upgrade该命令会把本地已安装的软件，与刚下载的软件列表里对应软件进行对比，如果发现已安装的软件版本太低，就会提示更新。如果软件都是最新版本，会提示：升级了 0 个软件包，新安装了 0 个软件包，要卸载 0 个软件包，有 0 个软件包未被升级。 总而言之，update是更新软件列表，upgrade是更新软件。 需要注意，在源列表里面会有系统更新，执行upgrade命令可能会产生系统升级，如果将系统按照这种方式升级，升级前安装过的应用软件可能会有无法使用的问题。]]></content>
      <categories>
        <category>系统</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之const关键字]]></title>
    <url>%2F2018%2F03%2F23%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8Bconst%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++ const关键字的学习内容。 const介绍与分类const是constant的简写，只要一个变量前面用const来修饰，就意味着该变量里的数据可以被访问，不能被修改。也就是说const意味着“只读”readonly。 采用符号常量写出的代码更容易维护；指针常常是边读边移动，而不是边写边移动；许多函数参数是只读不写的。const最常见用途是作为数组的界和switch分情况标号(也可以用枚举符代替)，const的使用分类如下： 常变量： const 类型说明符 变量名 常引用： const 类型说明符 &amp;引用名 常对象： 类名 const 对象名 常成员函数： 类名::fun(形参) const 常数组： 类型说明符 const 数组名[大小] 常指针： const 类型说明符* 指针名 (指针称为指针常量，指针指向是一个常量) 类型说明符* const 指针名 (指针称为常量指针，指针本身是一个常量) 注意：在常变量、常引用、常对象、常数组，const 与 “类型说明符”或“类名”（其实类名是一种自定义的类型说明符）的位置可以互换，但在常指针中不能互换，这一点在下面的内容也会提到。 const的作用 如果想阻止一个变量被改变，可以使用const关键字。在定义该const变量时，通常需要对它进行初始化，因为以后就没有机会再去改变它了；例如const int a = 3; const int &amp; b = a;必须向上面的方式写进行初始化，但const int * p; int a = 3; p = &amp;a;，这样也是可以的，当然最好将指针常量初始化为Null。 对指针来说，可以指定指针本身为const，也可以指定指针所指的数据为const，或二者同时指定为const； 例如： 12345int b = 500;const int * a = &amp;b; //1int const * a = &amp;b; //2int * const a = &amp;b; //3const int * const a = &amp;b; //4 分析： const位于*左侧，const修饰指针所指向的变量，即指针指向常量，不能修改指针指向的内容，p称为指针常量； const位于*右侧，const修饰指针本身，即指针本身是常量，不能对指针本身进行更改操作，p是常量指针。 因此，1和2的情况相同，都是指针所指向的内容为常量（const放在变量声明符的位置无关），这种情况下不允许对内容进行更改操作，如不能*a = 3 ； 3表示指针本身是常量，而指针所指向的内容不是常量，这种情况下不能对指针本身进行更改操作，如a++是错误的； 4为指针本身和指向的内容均为常量。 在一个函数声明中，const可以修饰形参，表明它是一个输入参数，在函数内部不能改变其值； 例如： 12void fun0(const A * a );void fun1(const A &amp; a); 调用函数的时候，用相应的变量初始化const常量，则在函数体中，按照const所修饰的部分进行常量化，如形参为｀const A* a，则不能对传递进来的指针的内容进行改变，保护了原指针所指向的内容；如形参为const A&amp; a ，则不能对传递进来的引用对象进行改变，保护了原对象的属性。 注意：参数const 通常用于参数为指针或引用的情况。 ​ 对于类的成员函数，若指定其为const类型，则表明其是一个常函数，不能修改类的成员变量；const 一般放在函数体后，形如：void fun() const;。 如果一个成员函数的不会修改数据成员，最好将其声明为const，因为const成员函数中不允许对数据成员进行修改，如果修改，编译器将报错，这大大提高了程序的健壮性。 知识扩展：如果有在const函数中修改成员变量值的需求，可以把成员变量声明为mutable类型，声明为mutable类型的成员变量可以在常函数中被修改。mutable是为了突破const的限制而设置的，被mutable修饰的变量，将永远处于可变的状态，即使在一个const函数中。可以参考博客 理解。 对于类的成员函数，有时候必须指定其返回值为const类型，以使得其返回值不为”左值”。 例如： const classA operator*(const classA &amp; a1, const classA &amp; a2); operator*的返回结果必须是一个const对象。需要注意，如果返回值不是const对象也不会编译出错。 12classA a, b, c;(a * b) = c; // 对a*b的结果赋值 操作(a * b) = c显然不符合编程者的初衷，也没有任何意义。 ​ 注意： const 返回类型只有在修饰指针或引用是才有用。 const对象只能调用类的const函数，因为常量对象的状态不允许被修改，调用常量对象的非常量函数会出错 在类中只有const函数时，非const对象可以调用cosnt函数，但有非const函数时，非const对象不能调用const函数。 非const函数、const函数同时存在时，非常对象将调用非常函数，常对象调用常函数。 例如： 1234567891011121314151617181920212223#include &lt;iostream&gt;using namespace std;class A&#123; public: A(int v): val(v) &#123;&#125; void print_val() &#123; cout &lt;&lt; "not const:" &lt;&lt; val &lt;&lt; endl;&#125; void print_val() const &#123; val++; cout &lt;&lt; "const print_val:" &lt;&lt; val &lt;&lt; endl;&#125; private: mutable int val;//注意体会mutable的作用&#125;;int main(int argc ,char **argv)&#123; A b(45); b.print_val(); const A a(12); a.print_val();&#125; 输出结果为： 12not const:45const print_val:13 类的成员变量为const类型时，类的const成员变量的初始化，必须在构造函数的初始化列表中进行。初始化列表是先于构造函数的函数体执行，并且成员初始化列表中的变量初始化顺序与成员在类中的声明顺序相同。 注意： const修饰的局部变量在栈上分配内存空间，const修饰的全局变量在只读存储区分配内储存空间。 类的static const成员变量可以在类的内部声明时初始化 const的初始化 非指针const常量初始化 12classA b;const classA a = b; 指针(引用)const常量初始化 指针 12classA * d = new classA();const classA * c = d; 或 1const classA * c = new classA(); 引用 12classA f;const classA &amp; e = f; //这种方式，e只能访问声明为const的成员函数，不能访问一般的成员函数 类的const成员变量初始化 12345678class A &#123; public: A():Size(0)&#123;&#125;//必须在构造函数的初始化列表初始化 private: const int Size;&#125; 思考题 以下的这种赋值方法正确吗？ 12const classA * c = new classA();classA * e = c; 以下的这种赋值方法正确吗？ 12classA * const c = new classA();classA * b = c; 这样定义赋值操作符重载函数可以吗？ 1const classA &amp; operator = (const classA &amp; a); [思考题答案] 不正确。因为声明指针的目的是为了对其指向的内容进行改变，而声明的指针e指向的是一个常量，所以不正确。 正确。因为声明指针所指向的内容可变。 不正确。在const A::operator=(const A&amp; a)中，参数列表中的const的用法正确，而当这样连续赋值的时侯，问题就出现了： 12A a,b,c:(a = b) = c; 因为a.operator = (b)的返回值是对a的const引用，不能再将c赋值给const常量。]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习之字符和字符串]]></title>
    <url>%2F2018%2F03%2F23%2FC%2B%2B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%AD%97%E7%AC%A6%E5%92%8C%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[这篇文章是有关C++字符和字符串的学习内容。 上午给大一的C++课学生答疑，一个学弟在做“福尔摩斯的密码”问题，程序里面字符数组的定义出现了错误，他的定义是：char b[7][4] = {&#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;}，在VS2018里面编译只是有警告大概是”int常量转换为cha类型“，放到网站测试代码就会报错，错误提示是一样的。学弟的本意是在二维字符数组的每一行存三个字母。但是这么初始化字符数组肯定是错误的，因为单引号是字符，只能有单个字符，学弟的写法应该改成双引号。这也激起我对字符和字符串的细致的学习，从这篇文章开始，我会逐渐积累C++的知识点，希望自己不断地进步。 先附上C++的API链接，cplusplus，好多细节的知识可以从这里学习到。 在C/C++里，单个字符和字符串是有区别的，而这又取决于使用的是单引号或双引号。 表达式&#39;A&#39;代表一个单个字符。编译期间，C++将表达式替换为字符“A”的ACSII编码，该编码的十进制值是65。 而“A”代表一个长度为1的字符串，C++编译器会把以下两个字节放到数据区里： 字母&quot;A&quot;的ASCII代码 一个零值（字符串结束标记） C++编译器随后会把表达式“A”替换为这两个字节数组的地址。 &#39;A&#39;和&quot;A &quot;是不同的，前者将被转换为一个整数值，后者被转换为一个地址。 下面来具体看一下C和C++中的字符和字符串。 C中的字符和字符串 C中的字符使用单引号‘ ’，引号内只能有一个字符。 C中并没有字符串数据类型，有两种方式表示字符串。 一种方式是使用字符数组来保存字符串，c字符串实际上是一个以‘null’（‘\0’）字符结尾的字符数组，’null‘字符表示字符串的结束。例如：char str[10] = &quot;woshilee&quot;; 注意：只有以null字符结尾的字符数组才是C字符串，否则只是一般的C字符数组。 另一种方式是使用字符指针来访问一个字符串，通过字符指针指向存放字符串数组的首元素地址来进行访问。 例如：char* str = &quot;12345&quot;; 其实两种方式中的变量str意义是一样的，都是指向字符串数组的首地址。 C字符串定义时可以使用=进行初始化，但不能使用=对C字符串进行赋值，对C字符串的操作需要使用string文件中定义的字符串处理函数。 字符串处理函数（头文件&lt;string.h&gt;或&lt;cstring&gt;）： strlen(const char *str)：返回字符串的长度，但不包含字符串结尾的’\0’。 例子：char mystr[100]=&quot;test string&quot;; 注意：sizeof(mystr)值为100（sizeof返回占用的字节数，如果是char arr[]=&quot;string&quot;，sizeof(arr)=7，sizeof包括‘\0’），strlen(mystr)值为 11。 strcpy(char *destination, const char *source)：复制source指针指向的字符串到des，包括’null‘结束字符。des空间要足够，避免内存溢出，两者空间不能重叠。 strncpy(char *destination, const char *source, size_t num)：从source复制前num个字符到des；如果source的字符数少于num个，在des中会自动补0凑够num个；如果source的字符数大于num，des结尾就不会有隐式的结束符，要手动添加‘0’，否则读des会溢出。参数size_t是无符号整型。 strcat(char *destination, const char *source)：拼接两个字符串，des的结尾字符被覆盖，拼接后的字符串结尾自动添加’null‘结束字符。des空间要足够，以避免内存溢出；des和source空间不能重叠。 strncat(char *destination, const char *source, size_t num)：拼接source的前num个字符串到des，结尾自动添加’null‘结束字符；source的字符数小于num时，只有到结束字符的内容被复制。 strcmp(const char *str1, const char *str2)：从第一个字符开始，两两比较两个字符串是否相等，相等返回0，否则返回非0（&lt;0或&gt;0，取决于第一个不同的字符的值的大小）。 strncmp(const char *str1, const char *str2, size_t num)：比较两个字符串的前num个字符是否相等，相等返回0，否则返回非0（&lt;0或&gt;0，取决于第一个不同的字符的值的大小）。 C中字符串的输入 scanf和scanf_s函数（头文件&lt;cstdio&gt;或&lt;stdio.h&gt;） 例如： 1234char str[10] = &#123; 0 &#125;;scanf("%s", str);char str[10] = &#123; 0 &#125;;scanf_s("%s", str, 10); 注意： 使用scanf函数时，若输入的字符数大于定义的字符数组长度就会出现缓冲区溢出； 当输入的字符串中包含空格时，scanf 和scanf_s 函数只会接收空格前的字符串。例如：hello world则只会接收到：hello。 scanf_s函数最后一个参数代表缓冲区的大小，示例中缓冲区大小为10，但最多能放入9个字符，因为最后一个需要放’\0’。 gets和gets_s函数（头文件&lt;cstdio&gt;或&lt;stdio.h&gt;） gets函数从标准输入流stdin中读取字符，并存储在字符串指针所指的内存空间，直到读取到换行符或文件结束符，换行符不会读入字符串；结束符号‘null’会自动添加到结尾。 123char str[10] = &#123; 0 &#125;;gets(str);gets_s(str, 10); 注意： gets函数解决了scanf和 scanf_s不能输入空格的问题，但是没有解决缓冲区溢出的问题。 gets函数由于也不安全所以被 gets_s函数代替，该函数的后一个参数代表缓冲区大小。 但是，get_s在linux中用不了，会提示没有定义，因为该函数是微软自创的，要在windows下vs中使用才行，可以使用fgets 函数替代。 fgets 函数（头文件&lt;cstdio&gt;或&lt;stdio.h&gt;） 函数原型： 1char * fgets (char *str, int num, FILE *stream); fgets接受一个流参数，将读取到的字符保存在C字符串中，直到读取到num-1个字符或读到换行符或文件结束符；与gets函数不同的是它会将换行符读入字符串，作为字符串的一部分；结束符号‘null’会自动添加到结尾。其中参数num包含结束符号‘null&#39;。 总结一下fgets和gets的区别： gets函数从标准输入流stdin读取字符，换行符不会加入字符串；fgets接受一个流参数，换行符会加入字符串； fgets函数限制了接收字符的个数，改进了gets函数缓冲区溢出的问题，是安全函数； fgets是为读取文件设计的，读取键盘时没有gets函数方便。 C中字符串的输出 printf和printf_s函数（头文件&lt;cstdio&gt;或&lt;stdio.h&gt;） printf函数输出字符串到标准输出流stdout。 puts函数（头文件&lt;cstdio&gt;或&lt;stdio.h&gt;） 将字符串数据写到标准输出流stdout，直到遇到结束符号&#39;\0&#39;(不输出结束符号)，并在末尾自动添加一个换行符’\n&#39;。 例如： 12char *str="hello world";puts(str); fputs函数（头文件&lt;cstdio&gt;或&lt;stdio.h&gt;） 将字符串数据写到流，一个指向标识输出流的FILE对象的指针，直到遇到结束符号&#39;\0&#39;(不输出结束符号)。 函数原型： 1int fputs(const char *str, FILE *stream); 例如： 12char *str="hello world";fputs(str, stdout); C++中的字符和字符串不同于C中的使用字符数组或字符指针表示字符串，C++中定义了字符串类String类，引用头文件&lt;string&gt;可以直接定义string类的对象，即字符串对象，使用该变量对字符串进行操作。string是C++标准库的一个重要的部分，主要用于字符串处理。可以使用输入输出流方式直接进行操作，也可以通过文件等手段进行操作。同时C++的算法库对string也有着很好的支持，而且string还和C语言的字符串之间有着良好的接口。当然也存在一些弊端。 C++字符串的输入 使用输入操作符填充一个字符串变量 例如： 12string str;cin&gt;&gt;str; 注意：读取过程会忽略最初的空白字符(空格、制表符和换行符)，同时输入会在下一个空格或者换行符处停止。例如：hello world则只会接收到：hello。 使用预定义函数getline()获取整行输入（包括空格） 从输入流中提取字符。 函数原型： 1istream&amp; getline (istream&amp; is, string&amp; str);//其中之一 getline()的两个参数： is：输入流 str：用于接收输入的字符串变量 例如： 12string str;getline(cin, str); 注意：getline()函数在遇到行结束（换行符或‘\n’）时停止接收字符。 string类型的变量可以使用+、=等运算操作符，直接对字符串变量进行操作。 字符串中的小细节 string对象和C字符串直接的转换 可以将C字符串存储在string类型的变量中 例如： 123char str[] = "hhhha";string s;s = str; string对象不能自动的转换为C字符串，需要进行显式的类型转换，用到string类的成员函数c_str() 例如： 1strcpy(str, s.c_str()); 字符串到数字的转换 可以使用atoi函数获取一个字符串参数，该函数返回字符串对应的int值。如果参数不与一个int值对应，atoi就会返回0。atoi函数在文件为cstdlib的库中。如果数字太大，不能转换成int类型的值，可以使用atol将字符串转换为long类型的值。如果想转换为浮点数，可以使用atof函数。 例如： 123456atoi("1234"); //返回整数1234atoi("#123"); //返回0char* str1 = "29.3547";float f = atof(str1);//给f赋值为29.354700char* str2 = "31465666";long l = atof(str2);//给l赋值为31465666 C++对于单引号“字符串”的处理 以‘ABCD’为例，在C++中将单引号引用的部分看作一个字符，它表示一个整形常量。 1234567int flag = 'ab'; cout&lt;&lt; flag &lt;&lt;endl;printf("%X\n", flag); char * array = (char *)&amp;flag; char buff[5] = &#123;0&#125;; strncpy(buff, array, 4); cout &lt;&lt; buff &lt;&lt;endl; 输出： 123249306162ba C++在处理单引号引出的多个字符时，用4个字节大小的整数来表示，‘ab’的每个字符逐个赋给了flag这个变量的每个字节，Intel的CPU基于x86的架构，X86采用的是小端模式，即将整形的高位放在了内存的低地址处，所以a看成高位，b看成低位。如果使用printf(&quot;%X\n&quot;, flag)会看到输出为6162。在C++内部的运算：a的ACSII是97，b的ACSII是98，经过运算97*256+98=24930或97 &lt;&lt; 8 + 98 = 24930（左移操作符相当于扩大为2^4倍）。值得注意的是，如果大小超过4字节，就会溢出。 关于字符和字符串的知识先总结到这里，在今后的学习中会继续学习积累，希望能不断进步。加油～～:smile:]]></content>
      <categories>
        <category>语言</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>面试</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之基本概念和命令]]></title>
    <url>%2F2018%2F03%2F22%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中基本概念和命令的学习内容。 ROS命令**source /devel/setup.bash：刷新setup.bash文件，这个自动生成的脚本文件设置了若干环境变量，从而使ROS能够找到创建的功能包和新生成的可执行文件，类似与下面所述的全局脚本文件，但该文件是专门为自己的工作区量身定做的。 source /opt/ros/kinetic/setup.bash：全局的脚本文件 ros··· -h：查看ros命令格式 roscore：运行所有ROS程序前首先要运行的命令 roswtf：进行全面深入的检测，包括检测环境变量、安装的文件以及运行的节点 rosnode list：列出活跃的节点 rosnode info /[node_name]：返回关于一个特定节点的信息 rosrun [package_name] [node_name]：允许使用包名直接运行一个包内的节点 rosrun rqt_graph rqt_graph：以图的形式显示正在运行的节点和话题之间的消息 roscp [package_name] [file_to_copy_path] [copy_path]：将文件从一个package复制到另一个package rospack：允许用户获取软件包的有关信息，用法：rospack find [package_name] roscd：切换工作目录到某个软件包或软件包集中 rosls：直接按软件包的名称而不是绝对路径执行ls命令，罗列命令。 catkin_create_pkg &lt;package_name&gt; [depend1] [depend2] [depend3]：创建程序包 rospack depends1 &lt;package_name&gt;：一级依赖 rospcak depends &lt;package_name&gt;：间接依赖 catkin_make [make_targets] [-DCMAKE_VARIABLES=...]：编译程序包 rosnode list：列出正在运行的活跃的节点 rosnode info [node_name]：返回关于一个特定节点的信息 rosnode kill [node_name]：终止节点运行 rosnode cleanup：将节点从rosnode列表中删除 rosrun [package_name][node_name]：允许使用包名直接运行一个包内的节点 rostopic echo [topic]：显示在某个话题上发布的数据，本质是生成rostopic echo节点，订阅该话题以接收该话题上发布的消息并显示。 rostopic hz [topic]：订阅指定的话题，显示该话题的数据发布速率，每秒发布的消息数量 rostopic bw [topic]：订阅指定的话题，显示话题使用的宽带，即每秒发布消息所占的字节量 rostopic list：列出所有当前订阅和发布的话题，运行rostopic list -h可查看其子命令。 rostopic pub -r rate-in-hz [topic] [msg_type] [args]：向当前某个正在广播的话题重复地按照指定频率发布指定的消息，使用rostopic pub -h查看该命令参数，例子： rostopic pub –r 1 /turtle1/cmd_vel geometry_msgs/Twist ’[2,0,0]’ ’[0,0,0]’ 子参数说明： -r：指定话题以频率模式发布消息，即以一定的时间周期发布消息 -1(数字1)：一次性发布模式 -l(小写L)：默认的模式，即特别的锁存模式，也是发布一次消息，但会确保该话题的新订阅者也会收到消息 -f：从文件中读取消息或从标准的输入中读取 rostopic type [topic]：显示所发布话题的消息类型 rostopic info [topic]：获取关于话题的信息（消息类型、发布者、订阅者） rosmsg show [message type]：查看消息的详细情况，即消息类型的基本数据类型组成 rosmsg users：Find files that use message rosmsg md5：Display message md5sum rosmsg package：List messages in a package rosmsg packages：List packages that contain messages rosservice命令可以使用ROS客户端/服务器框架提供的服务，其子命令如下： rosservice list：输出可用服务的信息 rosservice call [service] [args]：调用带参数的服务 rosservice type [service]：输出服务类型 rosservice find：依据类型寻找服务 rosservice uri：输出服务的ROSRPC uri rosparam set [param_name]：设置参数 rosparam get [param_name]：获取参数 rosparam load：从文件读取参数 rosparam dump：向文件中写入参数 rosparam delete：删除参数 rosparam list：列出参数名 rosdep install [package]：下载并安装ROS package所需要的系统依赖项 Roslaunch xml文件标签说明：http://wiki.ros.org/roslaunch/XML Urdf xml 文件标签说明：http://wiki.ros.org/urdf/XML Roscpp api 文档：http://docs.ros.org/jade/api/roscpp/html/ Rospy api 文档：http://docs.ros.org/jade/api/rospy/html/ ROS概念ROS文件系统文件系统层概念主要指在硬盘里能看到的ROS目录和文件，包括： Packages：软件包，ROS应用程序代码的组织单元，每个软件包都可包含程序库、可执行文件、脚本或其他手动创建的文件。 Manifest（package.xml）：清单是对于“软件包”相关信息的描述，用于定义软件包相关元信息之间的依赖关系，这些信息包括版本、维护者和许可协议等。 Message (msg) types: 存储在my_package/msg/MyMessageType.msg的Message文件，主要定义了ROS系统的messages传输的数据结构。 Service (srv) types: 存储在 my_package/srv/MyServiceType.srv的服务services文件，定义了ROS的服务通信时的请求（request ）和响应（response ）相关的数据结构。 ROS计算图层计算图是ROS在点对点网络里整合并处理数据的过程。基本计算图概念是 节点, 主机, 参数服务器, 消息, 服务, 话题, and 数据包，它们通过不同的方式提供数据给图层。 这些概念是在ros_comm库里实现的。 Nodes: 节点主要执行计算处理 。ROS被设计为细粒度的模块化的系统：一个机器人控制系统通常有很多节点组成 。例如，一个节点控制激光测距仪，一个节点控制轮电机，一个节点执行定位，一个节点执行路径规划，一个节点提供系统图形界面，等等。一个ROS节点通过ROS客户端库 client library编写，例如 roscpp o或rospy 。 Master: The ROS Master provides name registration and lookup to the rest of the Computation Graph. Without the Master, nodes would not be able to find each other, exchange messages, or invoke services. Parameter Server: The Parameter Server allows data to be stored by key in a central location. It is currently part of the Master. Messages: 节点之间使用messages信息互相通信。 一个消息就是一个由类型域组成的简单的数据结构，支持标准的原始数据类型（integer, floating point, boolean等等）和数组 。消息可以包含任意嵌套的结构和数组（很像C结构）。 Topics: Messages are routed via a transport system with publish / subscribe semantics. A node sends out a message by publishing it to a given topic. The topic is a name that is used to identify the content of the message. A node that is interested in a certain kind of data will subscribe to the appropriate topic. There may be multiple concurrent publishers and subscribers for a single topic, and a single node may publish and/or subscribe to multiple topics. In general, publishers and subscribers are not aware of each others’ existence. The idea is to decouple the production of information from its consumption. Logically, one can think of a topic as a strongly typed message bus. Each bus has a name, and anyone can connect to the bus to send or receive messages as long as they are the right type. Services: The publish / subscribe model is a very flexible communication paradigm, but its many-to-many, one-way transport is not appropriate for request / reply interactions, which are often required in a distributed system. Request / reply is done via services, which are defined by a pair of message structures: one for the request and one for the reply. A providing node offers a service under a name and a client uses the service by sending the request message and awaiting the reply. ROS client libraries generally present this interaction to the programmer as if it were a remote procedure call. Bags: Bags are a format for saving and playing back ROS message data. Bags are an important mechanism for storing data, such as sensor data, that can be difficult to collect but is necessary for developing and testing algorithms. 工作空间结构 build：build space默认的所在位置，同时也是cmake 和 make被调用来配置并编译程序包的地方 devel：devel space默认的所在位置，也是安装程序包之前存放可执行文件和库文件的地方 src：存放软件包的位置 文件系统工具rospack：允许用户获取软件包的有关信息，用法：rospack find [package_name] roscd：是rosbash命令集中的一部分，允许用户切换工作目录到某个软件包或软件包集中；和ROS中其他工具一样，只能切换到那些路径已经包含在ROS_PACKAGE_PATH环境变量中的软件包，可以使用echo $ROS_PACKAGE_PATH查看其中包含的路径。ROS_PACKAGE_PATH环境变量应该包含那些保存有ROS软件包的路径，并且每个路径之间用冒号分隔开。 rosls：rosbash命令集中的一部分，允许用户直接按软件包的名称而不是绝对路径执行ls命令，罗列命令。 ROS catkin程序包组成：package.xml文件+CMakeLists.txt文件。 每个目录下只能有一个程序包，同一目录下不能有嵌套的或多个程序包存在。 创建程序包的命令catkin_create_pkg &lt;package_name&gt; [depend1] [depend2] [depend3] 该命令需要在工作空间/src目录下执行，&lt;package_name&gt;是要创建的软件包的名字，depend1..3是创建的程序包依赖的其他程序包,执行完该命令后,就会在src目录下生成一个文件夹，包含package.xml和CMakeLists.txt文件。 程序包依赖关系查看命令: 一级依赖：rospack depends1 &lt;package_name&gt; 间接依赖：rospcak depends &lt;package_name&gt; 编译程序包catkin_make [make_targets] [-DCMAKE_VARIABLES=...] catkin_make install # (可选) 编译工作空间下的某个软件包： 1catkin_make -DCATKIN_WHITELIST_PACKAGES="package1;package2" 在工作空间下执行上述命令，会编译src文件夹下的所有catkin工程。 ROS图概念Nodes：节点，ROS网络中的可执行文件，可通过ROS客户库与其他节点通信,节点可以发布或接收一个话题，节点也可以提供或使用某种服务。 Messages：消息，一种ROS数据类型，用于订阅或发布到一个话题 Topics：话题，节点可以发布消息到话题，也可以订阅话题以接受消息 Master：节点管理器，ROS名称服务（如帮助节点找到彼此） ROS客户端库允许使用不同编程语言编写的节点之间互相通信： rospy = python客户端 roscpp = c++ 客户端 rosout：ROS中相当于stdout/stderr，用于收集和记录节点调试输出信息，它总是运行的。 roscore：主机+rosout+参数服务器；运行所有ROS程序前首先要运行的命令;启动节点管理器（The Master） rosnode list：列出活跃的节点 rosnode info /[node_name]：返回关于一个特定节点的信息 rosrun [package_name] [node_name]：允许使用包名直接运行一个包内的节点（不需要知道包的路径） ROS话题(topic)节点和节点之间是通过一个ROS话题来互相通信的，某一个节点在一个话题上发布特定的消息，其他节点可以订阅该话题以接收该消息。 rostopic命令工具可以获取有关ROS话题的信息，运行rostopic -h可以查看所有的rostopic子命令。 rostopic bw：显示话题使用的宽带。 rostopic echo [topic]：显示在某个话题上发布的数据，本质是生成rostopic echo节点，订阅该话题以接收该话题上发布的消息并显示。 rostopic hz [topic]：显示话题的数据发布速率 rostopic list：列出所有当前订阅和发布的话题，运行rostopic list -h可查看其子命令。 rostopic pub [topic] [msg_type] [args]：向当前某个正在广播的话题发布数据，使用rostopic pub -h查看该命令参数 rostopic type [topic]：显示所发布话题的消息类型，可以根据显示的话题类型，再继续执行rosmsg show [message type]：查看消息的详细情况 ROS消息(msg)话题之间的通信是通过节点之间发送ROS消息实现的，发布器和订阅器之间的通信，必须发送和接收相同类型的消息，意味着话题的类型是由发布在它上面的消息类型决定的。 msg文件存放在package的msg目录下，它是一个描述ROS中所使用消息类型的简单文本，实际是每行声明一个数据类型和变量名，会被用于生成不同语言的源代码。 rostopic type [topic]：用来显示所发布话题的消息类型 rosmsg show [message type]：查看消息的详细情况，即消息类型的基本数据类型组成 rosmsg users：Find files that use message rosmsg md5：Display message md5sum rosmsg package：List messages in a package rosmsg packages：List packages that contain messages ROS服务(srv)服务（services）是节点之间通信的另一种方式，服务允许节点发送请求（request）并获得一个响应（response）。srv文件存放在srv目录下，一个srv文件描述一项服务，包含请求和响应两个部分，在srv文件中由’—-‘分隔。 rosservice命令可以使用ROS客户端/服务器框架提供的服务，其子命令如下： rosservice list：输出可用服务的信息 rosservice call [service] [args]：调用带参数的服务 rosservice type [service]：输出服务类型 rosservice find：依据类型寻找服务 rosservice uri：输出服务的ROSRPC uri ROS参数rosparam使得能够存储并操作ROS参数服务器（Parameter Server）上的数据，参数服务器能够存储整型、浮点、布尔、字符串、字典和列表等数据类型，使用YAML标记语言的语法，其子命令如下： rosparam set [param_name]：设置参数 rosparam get [param_name]：获取参数 rosparam load：从文件读取参数 rosparam dump：向文件中写入参数 rosparam delete：删除参数 rosparam list：列出参数名 消息发布器（节点）创建过程： 初始化 ROS 系统 在 ROS 网络内广播将要在话题上发布的某一类型的消息 以某一频率在话题上发布消息 消息订阅器（节点）创建过程： 初始化ROS系统 订阅话题 进入自循环，等待消息的到达 当消息到达，调用回调函数 服务器（Service）节点、客户端（Client）节点录制与回放数据只有消息已经发布了才可以被录制。 使用rostopic list -v命令查看当前系统中发布的所有话题。 在一个保存录制的目录下运行rosbag record -a命令，附加的-a选项表示将当前发布的所有话题数据都录制保存到一个bag文件中，该文件会自动以年份、日期和时间命名并以.bag作为后缀，它包含了rosbag record运行期间所有节点发布的话题。 bag文件可以使用rosbag info检查其内容，使用rosbag play命令回放出来。 常见错误执行roscd命令时，出现no such packag的情况，解决方案执行： 1echo "export ROS_PACKAGE_PATH"=~/catkin_ws:"$ROS_PACKAGE_PATH " &gt;&gt; ~/.bashrc 在新的终端执行：roscd ...成功。]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04 Hexo+github+Typora搭建博客]]></title>
    <url>%2F2018%2F03%2F21%2Fubuntu16.04%20Hexo%2Bgithub%2BTypora%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[这篇文章是Hexo博客搭建有关的内容。 预备知识Hexo是一个基于Node.js的静态博客程序，可以方便的生成静态网页托管在github、gitcafe和Heroku上。博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，hexo所做的就是将这些md文件都放在本地，更新博文目录和相关链接信息，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 Hexo依赖Node.js和Git。nvm（node version manager）是nodejs版本管理工具，管理nodejs和npm的版本；npm是随同nodeJs一起安装的包管理工具，npm管理对应nodeJs的第三方插件；nvm管理构建nodejs和对应的npm，npm管理对应的nodejs的第三方插件。 本地搭建安装Git1sudo apt-get install git-core 安装Node.js最好的方式是使用NVM（Node Version Manager）安装，在终端安装nvm执行命令： cURL： 1curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash Wget： 1wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash 重启终端安装Node.js： 1nvm install stable 安装Hexo1npm install -g hexo-cli 初始化Hexo12345mkdir gitcd githexo init hexo #自定义的文件夹cd hexonpm install 设置Hexo执行命令： 12hexo g/generate #生成静态网页hexo s/server #运行本地服务器 如果出现提示： 12INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 说明安装成功，在浏览器地址栏输入http://localhost:4000 就可以看到默认主题的博客界面了。 博客主题更改安装主题next或yillia，在～/git/hexo/目录下执行命令： 12hexo cleangit clone https://github.com/iissnan/hexo-theme-next themes/next 或 1git clone git@github.com:litten/hexo-theme-yilia.git themes/yilia 更新主题，修改hexo目录下的_config.yml，将theme属性设置为next或yilia，默认是landscape。 执行命令查看本地效果： 12hexo ghexo s 到此为止已经完成了Hexo博客的本地安装和查看，下一步是将博客部署到github上面，这样就可以通过网络远程访问自己的博客了。 目前使用的是next主题，完整的安装配置过程可以参考这里。 next主题优化推荐文章：https://www.jianshu.com/p/1f8107a8778c 部署到github 首先到github上面注册自己的账号。 配置github 命令行输入命令： 12git config --global user.name "username" #ruoxiangligit config --global user.eamil "email@example.com" #981968690@qq.com 其中yourname是输入你自己的用户名，email@example.com输入你自己的注册邮箱。 这里可以使用git config --list命令查看配置好的内容（保存在home/.gitconfig文件中），如果需要修改用户名或邮箱，执行如下命令（也可以直接修改文件）： 12git config --global --replace-all user.name “username”git config --global --replace-all user.email “email@example.com” 创建公钥，命令行输入命令： 1ssh-keygen -C 'you email address@gmail.com' -t rsa 说明：C必须大写，改为自己的注册邮箱，然后一直回车，直到出现“The key’s randomart image is：”的提示。 之后用户目录~/.ssh/下建立了相应的密钥文件id_rsa.pub，打开该文件。 添加公钥：github首页右上角点击头像，选择Settings，再选择New SSH KEY，把上一步id_rsa.pub文件的秘钥复制进去生成公钥。 创建项目仓库：github首页点击右上角的+，选择New repository。在页面里输入username.github.io，必须这么写。填完后点击Create repository。 部署博客：修改hexo目录下的_config.yml文件，最后面修改为： 1234deploy: type: git repository: git@github.com:username/username.github.io.git branch: master 安装hexo的插件： 1npm install hexo-deployer-git --save 然后： 123hexo cleanhexo generatehexo deploy #可以使用hexo g -d命令代替上面两个命令 在浏览器输入yourname.github.io就可以访问的自己的博客啦。 绑定自己的域名绑定域名分2种情况：带www和不带www的。 域名配置最常见有2种方式，CNAME和A记录，CNAME填写域名，A记录填写IP，由于不带www方式只能采用A记录，所以必须先ping一下yourname.github.io的IP，然后到域名DNS设置页，将A记录指向ping出来的IP，将CNAME指向username.github.io，这样可以保证无论是否添加www都可以访问，如下： 然后到github项目根目录新建一个名为CNAME的文件（无后缀），里面填写自己的域名。在绑定了新域名之后，原来的username.github.io并没有失效，还是会自动跳转到新域名。 编辑.md文件工具推荐笔者使用的是Typora，官网，网页最下面是下载入口，根据自己的系统选择，ubuntu的方式如下： 1234567# optional, but recommendedsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE# add Typora's repositorysudo add-apt-repository 'deb http://typora.io linux/'sudo apt-get update# install typorasudo apt-get install typora Typora的Markdown语法的学习可以参考博客。 配置Hexo渲染MathJax数学公式，推荐文章：https://www.jianshu.com/p/7ab21c7f0674 多机更新方案一（不好用）使用坚果云同步hexo文件夹文件。坚果云同步hexo中文件时，有些文件会一直处于分析状态，上传不上去，影响其他文件的上传，so放弃坚果云。 方案二使用GitHub进行同步 旧设备操作 假设已经按照前面的步骤在旧设备上搭建好了Hexo并部署到了GitHub。 在旧设备部署博客到Github以后，我们可以在Github仓库的master分支上看到上传的博客文件。但是这个博客文件不包含hexo配置文件，所以需要新建分支，使用git指令将带hexo的配置文件上传到新建的分支上。在本地博客根目录下使用git指令上传项目到GitHub，按如下进行操作： 123456789101112// git初始化 git init // 添加仓库地址 git remote add origin https://github.com/用户名/仓库名.git // 新建分支hexo并切换到新建的分支 git checkout -b hexo // 添加所有本地文件到gitgit add . // git提交 git commit -m &quot;...&quot; // 文件推送到hexo分支 git push origin hexo 至此，旧设备上需要进行的操作完成。 新设备操作 Github上新建的分支的文件git clone到本地 1git clone -b hexo https://github.com/用户名/仓库名.git 安装Git、Node.js 安装依赖库 1234567#hexo-renderer-kramed markdown渲染引擎npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save#rssnpm install --save hexo-generator-feed#hexo-word countnpm install hexo-wordcount --save 安装过程中会提示没有/home/eric/package.json文件的提示（但应该已经安装完成了），不过对后面的过程没有影响。 安装hexo（到git clone的目录下操作命令） 123npm install -g hexo-clinpm installnpm install hexo-deployer-git 这个过程应该会在当前目录下产生package.json文件，执行hexo clean，应该可以顺利执行，说明配置成功。 添加SSH key 命令行输入ssh-keygen -t rsa -C “邮箱地址” 按三次回车（密码为空），生成密匙。 在home/username/.ssh目录下找到id_rsa.pub，打开复制内容到GitHub添加新的SSH key。 终端输入ssh - T git@github.com回车，提示认证成功即可。 在终端输入命令(和旧设备中的相同)： 12git config --global user.name &quot;username&quot; #ruoxiangligit config --global user.eamil &quot;email@example.com&quot; #981968690@qq.com 执行hexo g -d，顺利执行则说明配置成功。 配置Hexo渲染MathJax数学公式 新旧设备的日常维护注意：在当前设备上进行所有操作之前，一定要现将本地的配置文件（包括添加的新博文、修改内容样式等等）进行更新，因为在此之前另一台设备可能向GitHub推送了更新，但是本地的内容还是旧版，若不更新进行操作，之后提交的会是旧版的内容修改后的效果。 为了保证本地内容为最新，所有操作前的操作：git pull origin hexo 本地对博客进行修改（添加新博文、修改样式等等）后，通过下面的流程进行管理： 配置文件的更新：依次执行git add .、git commit -m “…”、git push origin hexo指令将改动推送到GitHub（此时当前分支应为hexo） 补充：如果不想每次push都输入用户名和密码。查看到传输协议，终端执行： 1git remote -v 可以看到： 12origin https://github.com/ruoxiangli/ruoxiangli.github.io.git (fetch)origin https://github.com/ruoxiangli/ruoxiangli.github.io.git (push) 重新设置成ssh的方式： 123git remote rm origingit remote add origin git@github.com:username/repository.gitgit push -u origin master 再查看当前传输协议： 12origin git@github.com:ruoxiangli/ruoxiangli.github.io.git (fetch)origin git@github.com:ruoxiangli/ruoxiangli.github.io.git (push) 到此操作成功。 静态网页的更新：执行hexo g -d发布网站到master分支 参考文章：https://www.jianshu.com/p/6fb0b287f950 https://blog.csdn.net/crazy_scott/article/details/79342303 遇到的错误解决方法发现执行时nvm install stable时出现未找到‘nvm’命令的错误提示，解决方式，分别执行下面两行指令： 12export NVM_DIR="$HOME/.nvm"[ -s "NVM_DIR/nvm.sh" ] &amp;&amp; \. "NVM_DIR/nvm.sh" # This loads nvm 执行hexo server后访问http://localhost:4000，出现Cannot Get /提示，打不开网页，可能是由于端口号4000被占用，可以使用其他端口号打开。解决方式：hexo server -p 5000]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>ubuntu16.04</tag>
        <tag>Hexo</tag>
        <tag>Github</tag>
        <tag>Typora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS学习之catkin CMakeList.txt介绍（译）]]></title>
    <url>%2F2018%2F03%2F21%2FROS%E5%AD%A6%E4%B9%A0%E4%B9%8Bcatkin%20CMakeList.txt%E4%BB%8B%E7%BB%8D%EF%BC%88%E8%AF%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇文章是有关ROS中catkin CMakeLists.txt使用的内容。 ​ 本文翻译自ROS官网关于catkin CMakeList.txt的介绍，官网原文链接，由于直接阅读英文文档感觉自己理解不透彻、收获不多，所以决定一边翻译一边学习。其中零星的加入了一些译者个人使用过程中的体会以及在阅读《机器人操作系统（ROS）浅析》（Jason M.O’Kane著 肖军浩译）一书时学习到的内容，帮助自己更好地理解catkin编译生成的过程，留作今后复习完善。 概况​ CMakeList.txt文件是CMake编译系统编译软件包过程的输入文件。任何CMake兼容包都包含一个或多个CMakeLists.txt文件，这些文件描述了如何编译代码以及将其安装到哪里。将CMakeLists.txt文件应用于一个catkin项目时，它就作为一个标准的附带一些限制条件的vanilla CMakeLists.txt文件。使用CMake编译程序时，cmake指令依据CMakeLists.txt 文件生成makefiles文件，make命令再依据makefiles文件编译链接生成可执行文件。 ​ catkin是ROS官方的一个编译构建系统，是原本的ROS的编译构建系统rosbuild的发展。catkin_make是将cmake与make的编译方式做了一个封装的指令工具，规范了工作路径与生成文件路径。 总体结构和顺序​ CMakeList.txt文件必须遵循如下的格式，不然就无法正确地编译（译者遇到一些编译ros软件包时提示“ros未定义的引用”的错误，原因就是CMakeList.txt文件中命令顺序不正确）。 必需的CMake版本：cmake_minimum_required() 软件包名：project() 查找编译依赖的其他CMake/Catkin包（声明依赖库）：find_package() 启动Python模块支持：catkin_python_package() 消息/服务/操作(Message/Service/Action)生成器：add_message_files(),add_service_files(),add_action_files() 调用消息/服务/操作生成：generate_messages() 指定包编译信息导出：catkin_package() 添加要编译的库和可执行文件：add_library()/add_executable()/target_link_libraries() 测试编译：catkin_add_gtest() 安装规则：install() CMake版本​ 每一个catkin CMakeList.txt文件必须以所需的CMake版本说明语句开始，Catkin需要2.8.3或者更高的版本 1cmake_minimum_required(VERSION 2.8.3) 软件包包名​ 软件包报名使用CMake的 project()函数指明，例如以robot_brain命名一个软件包： 1project(robot_brain) ​ CMake中，可以通过使用变量 ${PROJECT_NAME}在CMake脚本后面的任何位置引用项目名称。 查找编译依赖的CMake包​ 编译一个项目，需要使用CMake 的 find_package函数确定依赖的其他CMake包并找到它们，一般情况下至少会有一个catkin依赖： 1find_package(catkin REQUIRED) ​ 除此之外，项目依赖的其他软件包，都会自动成为catkin的组件（components）（就CMake而言）。因此可以将这些依赖包指定为catkin的组件，而不必再使用find_package，这样将会变得简单，例如依赖包nodelet： 1find_package(catkin REQUIRED COMPONENTS nodelet) ​ 注意：只能find_package那些想要编译标志的组件，不能添加运行时（runtime）依赖。 ​ 当然也可以写成下面的方式，但不方便: 12find_package(catkin REQUIRED)find_package(nodelet REQUIRED)- ? find_package()做了什么？​ 如果CMake通过 find_package()查找到一个软件包，它就会创建几个CMake环境变量，以提供有关已查找到的软件包的信息。这些环境变量可以在后面的CMake脚本中使用，它们表示软件包导出的头文件所在的位置、源文件所在的位置、软件包依赖的库以及这些库的查找路径，环境变量的名字遵循&lt;PACKAGENAME&gt;_&lt;PROPERTY&gt;，即包名-属性： &lt;NAME&gt;_FOUND：当库被查找到时置为true，否则为false &lt;NAME&gt;_INCLUDE_DIRS或&lt;NAME&gt;_INCLUDES：软件包导出的头文件路径 &lt;NAME&gt;_LIBRARIES或&lt;NAME&gt;_LIBS：软件包导出的库的路径 &lt;NAME&gt;_DEFINITIONS：？ 为何将Catkin软件包指定为组件？​ Catkin软件包严格意义上并不是catkin的组件，而且，CMake的功能组件功能被用于catkin的设计，以节省大量的打字时间。 ​ 对于catkin软件包，以catkin的组件的方式 find_package它们是有好处的，因为这个过程以catkin_prefix的形式创建了一组环境变量。例如，在程序中要使用nodelet软件包，推荐查找软件包的方式是： 1find_package(catkin REQUIRED COMPONENTS nodelet) ​ 这就意味着nodelet导出的头文件路径、库等都会附加到 catkin_variables上，比如，catkin_INCLUDE_DIRS不仅包含catkin的头文件路径，也包含了nodelet软件包的头文件路径，这在后面会派上用场。 ​ 如果单独的find_package nodelet： 1find_package(nodelet) ​ 这意味着nodelet的头文件路径、库及其他文件都不会包含在 catkin_variables中，对于nodelet_INCLUDE_DIRS,nodelet_LIBRARIES及其他变量也是如此。相同的变量也可以通过下面的方式创建： 1find_package(catkin REQUIRED COMPONENTS nodelet) Boost库​ 如果使用C++和Boost库，需要在Boost上调用 find_package()，并指定Boost中将要作为组件的那部分。例如，如果想要使用Boost的线程，可以用： 1find_package(Boost REQUIRED COMPONENTS thread) catkin_package()​ catkin_package()是一个由catkin提供的CMake宏。需要指定特定的catkin信息到编译系统，而这些信息又会被用于生成pkg-config和CMake文件。 ​ 该函数必须在使用 add_library()或add_executable()声明任何targets之前调用。其5个可选参数： INCLUDE_DIRS：软件包导出的头文件路径（例如cflags） LIBRARIES：项目导出的库 CATKIN_DEPENDS：当前项目依赖的其他catkin项目 DEPENDS：当前项目依赖的非catkin CMake项目，详细解释参见这里 CFG_EXTRAS：其他的配置选项 完整的宏文件参见[这里](#catkin-package)。 例子： 1234catkin_package( INCLUDE_DIRS include LIBRARIES $&#123;PROJECT_NAME&#125; CATKIN_DEPENDS roscpp nodelet DEPENDS eigen opencv) ​ 这里表明软件包文件夹中的include文件夹是导出头文件的位置，CMake环境变量 ${PROJECT_NAME}将会鉴定之前传递给project()函数的所有内容，在这种情况下它作为“robot_brain”。“roscpp”+“nodelet”是编译/运行此程序包需要存在的软件包，“eigen”+“opencv”是编译/运行此程序包时需要存在的系统依赖项（ROS packages有时会需要操作系统提供一些外部函数库，这些函数库就是所谓的“系统依赖项”）。 明确编译的目标​ 编译目标可以采取多种形式，但通常它们代表两种可能性之一： 可执行目标：可以运行的程序 库目标：在编译和/或运行时可以由可执行目标使用的库 目标命名​ 非常重要的一点是，不管编译/安装到哪个文件夹中，编译目标在catkin中的名称都必须是唯一的。这是CMake的一项要求，但目标唯一的名称又只是在CMake内部是必需的。可以使用set_target_properties()函数对目标重命名，例子： 123set_target_properties(rviz_image_view PROPERTIES OUTPUT_NAME image_view PREFIX "") ​ 这会在编译和安装输出中将目标 rviz_image_view的名称改为image_view。 自定义输出目录​ 可执行文件和库的默认输出目录通常设置为了合理的值，但在某些情况下必须进行自定义，例如，包含Python绑定的库必须放置在不同的文件夹中才能在Python中导入。 ​ 例子： 1set_target_properties(python_module_library PROPERTIES LIBRARY_OUTPUT_DIRECTORY &#123;CATKIN_DEVEL_PREFIX&#125;/&#123;CATKIN_PACKAGE_PYTHON_DESTINATION&#125;) 头文件和库路径​ 在指定目标之前，需要指定可以为所述目标找到资源的位置，特别是头文件和库： 头文件目录：将要编译的代码（C/C++）所需的头文件路径 库目录：可执行目标编译指向的库路径 include_directories(&lt;dir1&gt;, &lt;dir2&gt;, ..., &lt;dirN&gt;) link_directories(&lt;dir1&gt;, &lt;dir2&gt;, ..., &lt;dirN&gt;) include_directories()​ include_directories的参数应该是由调用find_package生成的* _INCLUDE_DIRS变量以及需要包含的任何其他目录。如果使用catkin和Boost，include_directories()的调用为： 1include_directories(include &#123;Boost_INCLUDE_DIRS&#125; &#123;catkin_INCLUDE_DIRS&#125;) ​ 第一个参数“include”表示包中的include/目录也是路径的一部分。 link_directories()​ CMake的 link_directories()函数可以添加其他的库目录，然而，并不推荐这么做。所有的catkin和CMake包在find_package时都会自动添加链接信息。只需链接到target_link_libraries()中的库。 ​ 例子： 1link_directories(~/my_libs) ​ 详细信息参加这里。 可执行目标​ 要指定必须编译的可执行目标，必须使用CMake函数 add_executable()。声明想要的可执行文件的文件名，以此生成此可执行文件所需的源文件列表，如果有多个源文件，用空格区分开。例如： 1add_executable(myProgram src/main.cpp src/some_file.cpp src/another_file.cpp) ​ 该命令会编译名为 myProgram的可执行文件，它是由后面的三个源文件共同编译生成的。 库目标​ CMake函数 add_library()指定用于编译的库文件，默认情况下，catkin编译共享库。 1add_library(&#123;PROJECT_NAME&#125; &#123;$&#123;PROJECT_NAME&#125;_SRCS&#125;) target_link_libraries​ 使用 target_link_libraries()函数指定可执行目标所要链接的库，即告诉CMake当链接此可执行文件时需要链接哪些库（这些库在上面的find_package中定义），通常在调用完add_executable()后被调用。如果出现ros未定义的引用错误，则添加${catkin_LIBRARIES}。 ​ 语法： 1target_link_libraries(&lt;executableTargetName&gt;, &lt;lib1&gt;, &lt;lib2&gt;, ... &lt;libN&gt;) ​ 例子: 123add_executable(foo src/foo.cpp)add_library(moo src/moo.cpp)target_link_libraries(foo moo) ​ 上面的例子将 foo与libmoo.so链接起来 ​ 注意，在大多数使用情况下，没有必要使用link_directories()，因为该信息通过find_package()已经自动提取到了。 消息、服务和操作目标​ 在被ROS软件包编译和使用之前，ROS中的消息（.msg）、服务（.srv）和操作（.action）文件需要特殊的预处理器编译步骤。这些宏的要点是生成编程语言特定的文件，以便可以在编程语言中使用消息、服务和操作。编译系统将使用所有可用的生成器（例如gencpp、genpy、genlisp）生成绑定。 ​ 提供了三个宏来分别处理消息，服务和操作： add_message_files add_service_files add_action_files 这些宏后面必须调用一个调用生成的宏： 1generate_messages() 重要的前提和限制 这些宏必须在调用catkin_package()之前被调用，以正确地完成生成工作。 123456find_package(catkin REQUIRED COMPONENTS ...) add_message_files(...) add_service_files(...) add_action_files(...) generate_messages(...) catkin_package(...) ... catkin_package()宏必须包含一个在message_runtime上的CATKIN_DEPENDS依赖。 123catkin_package( ... CATKIN_DEPENDS message_runtime ... ...) 必须对软件包message_generation使用find_package()，可单独或者作为catkin的组件使用： 1find_package(catkin REQUIRED COMPONENTS message_generation) package.xml文件必须包含一个在message_generation上的编译依赖和一个在message_runtime上的运行时依赖，如果从其他包中传递依赖关系，则这不是必需的。 如果有一个目标（甚至是过渡性的）依赖于需要建立消息/服务/动作的其他目标，需要在目标catkin_EXPORTED_TARGETS上添加显式依赖项，以使它们按照正确的顺序编译。这种情况几乎总是适用，除非你的软件包真的不使用ROS的任何部分。不幸的是，这种依赖不能自动传播。（some_target是由add_executable()设置的目标的名字） 1add_dependencies(some_target $&#123;catkin_EXPORTED_TARGETS&#125;) 如果有编译消息和/或服务的软件包以及使用这些软件的可执行文件，则需要在自动生成的消息目标上创建明确的依赖关系，以便它们按正确的顺序编译。（some_target是由add_executable()设置的目标的名字） 1add_dependencies(some_target $&#123;$&#123;PROJECT_NAME&#125;_EXPORTED_TARGETS&#125;) 如果软件包满足上述两个条件，则需要添加两个依赖项，即： 1add_dependencies(some_target &#123;$&#123;PROJECT_NAME&#125;_EXPORTED_TARGETS&#125; &#123;catkin_EXPORTED_TARGETS&#125;) 例子​ 如果在msg目录下有两个消息文件 MyMessage1.msg和MyMessage2.msg，并且这些消息依赖于std_msgs和sensor_msgs，另外在srv目录下有一个服务文件MyService.srv，就可以使用这些消息、服务定义可执行message_program，和可执行的程序does_not_use_local_messages_program，这个过程使用了ROS的某些部分，但不包含此包中定义的消息/服务。需要在CMakeList.txt文件中加上一下内容： 123456789# Get the information about this package's buildtime dependencies find_package(catkin REQUIRED COMPONENTS message_generation std_msgs sensor_msgs) # Declare the message files to be built add_message_files(FILES MyMessage1.msg MyMessage2.msg ) # Declare the service files to be built add_service_files(FILES MyService.srv ) # Actually generate the language-specific message and service files generate_messages(DEPENDENCIES std_msgs sensor_msgs) # Declare that this catkin package's runtime dependencies catkin_package( CATKIN_DEPENDS message_runtime std_msgs sensor_msgs ) # define executable using MyMessage1 etc. add_executable(message_program src/main.cpp) add_dependencies(message_program &#123;$&#123;PROJECT_NAME&#125;_EXPORTED_TARGETS&#125; &#123;catkin_EXPORTED_TARGETS&#125;) # define executable not using any messages/services provided by this package add_executable(does_not_use_local_messages_program src/main.cpp) add_dependencies(does_not_use_local_messages_program $&#123;catkin_EXPORTED_TARGETS&#125;) ​ 另外如果需要编译actionlib操作，并且在action目录下有一个名为MyAction.action的操作规范文件，就必须要添加actionlib_msgs到组件列表中，该组件列表就是find_package中catkin的组件，并在调用generate_messages()之前调用： 1add_action_files(FILES MyAction.action) ​ 此外，该包必须对 actionlib_msgs具有编译依赖关系。 启动Python模块支持​ 如果ROS软件包提供了一些Python模块，就要创建一个setup.py文件并调用： 1catkin_python_setup() ​ 该调用要在generate_message()和catkin_package()的调用之前。 单元测试​ 特定的catkin宏 catkin_add_gtest()用于处理基于gtest的单元测试： 1catkin_add_gtest(myUnitTest test/utest.cpp) 可选步骤：明确安装目标​ 编译完成后，目标被放入catkin工作空间下的devel目录。一般希望将目标安装到系统上，以使其他用户使用，或者安装到本地目录来测试系统级别的安装。也就是说，如果希望能够对代码进行make install，就需要明确目标结束的位置。 ​ 上述过程可以使用CMake的 install()函数实现，该函数的参数有： TARGETS：要安装的目标 ARCHIVE DESTINATION：静态库和动态链接库DLL(Windows).lib存根 LIBRARY DESTINATION：非DLL共享库和模块 RUNTIME DESTINATION：可执行目标和DLL(Windows)模式共享库 例子： 1234install(TARGETS $&#123;PROJECT_NAME&#125; ARCHIVE DESTINATION $&#123;CATKIN_PACKAGE_LIB_DESTINATION&#125; LIBRARY DESTINATION $&#123;CATKIN_PACKAGE_LIB_DESTINATION&#125; RUNTIME DESTINATION $&#123;CATKIN_PACKAGE_BIN_DESTINATION&#125;) ​ 除了这些标准的目标，还要安装一些文件到特定的目录下，即一个包含Python绑定的库必须要安装到另外的不同的目录下，这对Python是重要的： 123install(TARGETS python_module_library ARCHIVE DESTINATION $&#123;CATKIN_PACKAGE_PYTHON_DESTINATION&#125; LIBRARY DESTINATION $&#123;CATKIN_PACKAGE_PYTHON_DESTINATION&#125;) 安装Python可执行脚本​ Python代码的安装规则有些不同，它不需要使用 add_library()和add_executable()函数来告知CMake哪个文件是目标文件、目标文件是什么类型的。而是使用如下的CMakeList.txt文件： 12catkin_install_python(PROGRAMS scripts/myscript DESTINATION $&#123;CATKIN_PACKAGE_BIN_DESTINATION&#125;) ​ 如果只是安装了Python的脚本，不提供任何模块的话，就不用创建上文提到的 setup.py文件，也不用调用catkin_python_setup()。 安装头文件​ 头文件必须安装到include目录下，这通常通过安装整个文件夹的文件来完成（可以根据文件名模式进行过滤，并排除SVN子文件夹）。可以通过一下安装规则实现： 123install(DIRECTORY include/$&#123;PROJECT_NAME&#125;/ DESTINATION $&#123;CATKIN_PACKAGE_INCLUDE_DESTINATION&#125; PATTERN ".svn" EXCLUDE) ​ 或者如果include目录下的子文件夹无法和软件包名匹配时： 123install(DIRECTORY include/ DESTINATION $&#123;CATKIN_GLOBAL_INCLUDE_DESTINATION&#125; PATTERN ".svn" EXCLUDE) 安装roslaunch文件或其他源​ 其他像launchfiles的资源可以安装到 ${CATKIN_PACKAGE_SHARE_DESTINATION}： 123install(DIRECTORY launch/ DESTINATION $&#123;CATKIN_PACKAGE_SHARE_DESTINATION&#125;/launch PATTERN ".svn" EXCLUDE) CMakeLists.txt文件书写模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199cmake_minimum_required(VERSION 2.8.3)project(my_p)## Compile as C++11, supported in ROS Kinetic and newer# add_compile_options(-std=c++11)## Find catkin macros and libraries## if COMPONENTS list like find_package(catkin REQUIRED COMPONENTS xyz)## is used, also find other catkin packagesfind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs)## System dependencies are found with CMake's conventions# find_package(Boost REQUIRED COMPONENTS system)## Uncomment this if the package has a setup.py. This macro ensures## modules and global scripts declared therein get installed## See http://ros.org/doc/api/catkin/html/user_guide/setup_dot_py.html# catkin_python_setup()################################################## Declare ROS messages, services and actions #################################################### To declare and build messages, services or actions from within this## package, follow these steps:## * Let MSG_DEP_SET be the set of packages whose message types you use in## your messages/services/actions (e.g. std_msgs, actionlib_msgs, ...).## * In the file package.xml:## * add a build_depend tag for "message_generation"## * add a build_depend and a run_depend tag for each package in MSG_DEP_SET## * If MSG_DEP_SET isn't empty the following dependency has been pulled in## but can be declared for certainty nonetheless:## * add a run_depend tag for "message_runtime"## * In this file (CMakeLists.txt):## * add "message_generation" and every package in MSG_DEP_SET to## find_package(catkin REQUIRED COMPONENTS ...)## * add "message_runtime" and every package in MSG_DEP_SET to## catkin_package(CATKIN_DEPENDS ...)## * uncomment the add_*_files sections below as needed## and list every .msg/.srv/.action file to be processed## * uncomment the generate_messages entry below## * add every package in MSG_DEP_SET to generate_messages(DEPENDENCIES ...)## Generate messages in the 'msg' folder# add_message_files(# FILES# Message1.msg# Message2.msg# )## Generate services in the 'srv' folder# add_service_files(# FILES# Service1.srv# Service2.srv# )## Generate actions in the 'action' folder# add_action_files(# FILES# Action1.action# Action2.action# )## Generate added messages and services with any dependencies listed here# generate_messages(# DEPENDENCIES# std_msgs# )################################################## Declare ROS dynamic reconfigure parameters #################################################### To declare and build dynamic reconfigure parameters within this## package, follow these steps:## * In the file package.xml:## * add a build_depend and a run_depend tag for "dynamic_reconfigure"## * In this file (CMakeLists.txt):## * add "dynamic_reconfigure" to## find_package(catkin REQUIRED COMPONENTS ...)## * uncomment the "generate_dynamic_reconfigure_options" section below## and list every .cfg file to be processed## Generate dynamic reconfigure parameters in the 'cfg' folder# generate_dynamic_reconfigure_options(# cfg/DynReconf1.cfg# cfg/DynReconf2.cfg# )##################################### catkin specific configuration ####################################### The catkin_package macro generates cmake config files for your package## Declare things to be passed to dependent projects## INCLUDE_DIRS: uncomment this if your package contains header files## LIBRARIES: libraries you create in this project that dependent projects also need## CATKIN_DEPENDS: catkin_packages dependent projects also need## DEPENDS: system dependencies of this project that dependent projects also needcatkin_package(# INCLUDE_DIRS include# LIBRARIES my_p# CATKIN_DEPENDS roscpp rospy std_msgs# DEPENDS system_lib)############# Build ############### Specify additional locations of header files## Your package locations should be listed before other locationsinclude_directories(# include $&#123;catkin_INCLUDE_DIRS&#125;)## Declare a C++ library# add_library($&#123;PROJECT_NAME&#125;# src/$&#123;PROJECT_NAME&#125;/my_p.cpp# )## Add cmake target dependencies of the library## as an example, code may need to be generated before libraries## either from message generation or dynamic reconfigure# add_dependencies($&#123;PROJECT_NAME&#125; $&#123;$&#123;PROJECT_NAME&#125;_EXPORTED_TARGETS&#125; $&#123;catkin_EXPORTED_TARGETS&#125;)## Declare a C++ executable## With catkin_make all packages are built within a single CMake context## The recommended prefix ensures that target names across packages don't collide# add_executable($&#123;PROJECT_NAME&#125;_node src/my_p_node.cpp)## Rename C++ executable without prefix## The above recommended prefix causes long target names, the following renames the## target back to the shorter version for ease of user use## e.g. "rosrun someones_pkg node" instead of "rosrun someones_pkg someones_pkg_node"# set_target_properties($&#123;PROJECT_NAME&#125;_node PROPERTIES OUTPUT_NAME node PREFIX "")## Add cmake target dependencies of the executable## same as for the library above# add_dependencies($&#123;PROJECT_NAME&#125;_node $&#123;$&#123;PROJECT_NAME&#125;_EXPORTED_TARGETS&#125; $&#123;catkin_EXPORTED_TARGETS&#125;)## Specify libraries to link a library or executable target against# target_link_libraries($&#123;PROJECT_NAME&#125;_node# $&#123;catkin_LIBRARIES&#125;# )############### Install ################ all install targets should use catkin DESTINATION variables# See http://ros.org/doc/api/catkin/html/adv_user_guide/variables.html## Mark executable scripts (Python etc.) for installation## in contrast to setup.py, you can choose the destination# install(PROGRAMS# scripts/my_python_script# DESTINATION $&#123;CATKIN_PACKAGE_BIN_DESTINATION&#125;# )## Mark executables and/or libraries for installation# install(TARGETS $&#123;PROJECT_NAME&#125; $&#123;PROJECT_NAME&#125;_node# ARCHIVE DESTINATION $&#123;CATKIN_PACKAGE_LIB_DESTINATION&#125;# LIBRARY DESTINATION $&#123;CATKIN_PACKAGE_LIB_DESTINATION&#125;# RUNTIME DESTINATION $&#123;CATKIN_PACKAGE_BIN_DESTINATION&#125;# )## Mark cpp header files for installation# install(DIRECTORY include/$&#123;PROJECT_NAME&#125;/# DESTINATION $&#123;CATKIN_PACKAGE_INCLUDE_DESTINATION&#125;# FILES_MATCHING PATTERN "*.h"# PATTERN ".svn" EXCLUDE# )## Mark other files for installation (e.g. launch and bag files, etc.)# install(FILES# # myfile1# # myfile2# DESTINATION $&#123;CATKIN_PACKAGE_SHARE_DESTINATION&#125;# )############### Testing ################# Add gtest based cpp test target and link libraries# catkin_add_gtest($&#123;PROJECT_NAME&#125;-test test/test_my_p.cpp)# if(TARGET $&#123;PROJECT_NAME&#125;-test)# target_link_libraries($&#123;PROJECT_NAME&#125;-test $&#123;PROJECT_NAME&#125;)# endif()## Add folders to be run by python nosetests# catkin_add_nosetests(test)]]></content>
      <categories>
        <category>机器人</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>CMake</tag>
        <tag>ROS</tag>
      </tags>
  </entry>
</search>
